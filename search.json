[
  {
    "objectID": "clinic/github.html#what-is-github",
    "href": "clinic/github.html#what-is-github",
    "title": "",
    "section": "What is GitHub?",
    "text": "GitHub is a hosting service for Git repositories, enabling us to share code across teams in a web environment."
  },
  {
    "objectID": "clinic/github.html#why-do-i-need-a-github-account",
    "href": "clinic/github.html#why-do-i-need-a-github-account",
    "title": "",
    "section": "Why do I need a GitHub account?",
    "text": "There are three reasons you are required to have a GitHub account for the hackweek:\n\nYour GitHub accounts will give you access to the hackweek cloud computing resources\nAll hackweek tutorials will be shared on GitHub\nAll project teams will use GitHub to collaborate and work together on their code"
  },
  {
    "objectID": "clinic/github.html#creating-a-github-account",
    "href": "clinic/github.html#creating-a-github-account",
    "title": "",
    "section": "Creating a GitHub account",
    "text": "Go to GitHub.\n\n\n\ngithub-signup\n\n\nNext, enter your email address and click on the green ‘Sing up for GitHub’ button. You will need to answer a few required questions in the following dialogs. Be sure to save your password somewhere safe because you will need it later! The steps for doing this are also well documented on this GitHub help page.\n\n\n\nrepos-tab\n\n\nEach repository is a container for a specific subset of material for this event. For example, there is a repository for the public-facing website you used to register for this event {{website_url}}. We’ll also create new repositories for each project."
  },
  {
    "objectID": "clinic/index.html#welcome",
    "href": "clinic/index.html#welcome",
    "title": "Pre-Hackathon Clinic",
    "section": "Welcome!",
    "text": "Thanks for being here\n\nWho instructors & helpers are, how to ask for help.\nCode of Conduct reminder: Be respectful and value each other’s ideas, styles and viewpoints.\nIf you have issues, please direct them to: Julie - lowndes @ nceas.ucsb.edu.\nLive transcripts are available.\nWe are recording this session.\n\n\n\n\n\n\n\nText to paste into Zoom Chat\n\n\n\n\n\nWelcome to the Cloud Hackathon Clinic!\nPlease go to https://openscapes.2i2c.cloud/hub/ - log in with your GitHub Account, and select “Small”\nClinic materials that we’ll cover today are here: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/clinic/"
  },
  {
    "objectID": "clinic/index.html#agenda",
    "href": "clinic/index.html#agenda",
    "title": "Pre-Hackathon Clinic",
    "section": "Agenda",
    "text": "The Clinic will occur in 2 halves, with a 5 minute break in-between:\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nJulie Lowndes, Openscapes\n\n\n8:05 am\nJupyterHub, repos, environments\nLuis Lopez, NSIDC\n\n\n9:00 am\nBreak\n\n\n\n9:05 am\nNotebooks, python, syncing\nMakhan Virdi, ASDC\n\n\n10:00 am\nClosing"
  },
  {
    "objectID": "clinic/index.html#before-the-clinic",
    "href": "clinic/index.html#before-the-clinic",
    "title": "Pre-Hackathon Clinic",
    "section": "Before the Clinic",
    "text": "Please follow the set up prerequisites before the Clinic."
  },
  {
    "objectID": "clinic/jupyterhub.html#why-are-we-using-a-cloud-environment",
    "href": "clinic/jupyterhub.html#why-are-we-using-a-cloud-environment",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Why are we using a cloud environment?",
    "text": "“Anyone working with large-scale Earth System data today faces the same general problems:\n\nThe data we want to work with are huge (typical analyses involve several TB at least)\nThe data we need are produced and distributed by many different organizations (NASA, NOAA, ESGF, Copernicus, etc.)\nWe want to apply a wide range of different analysis methodologies to the data, from simple statistics to signal processing to machine learning.\n\nThe community is waking up to the idea that we can’t simply expect scientists to download all this data to their personal computers for processing.”\nRyan Abernathey, Pangeo Project.\n\n\n\nDownload-based workflow. From Abernathey, Ryan (2020): Data Access Modes in Science"
  },
  {
    "objectID": "clinic/jupyterhub.html#openscapes-hub-and-cloud-infrastructure",
    "href": "clinic/jupyterhub.html#openscapes-hub-and-cloud-infrastructure",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Openscapes Hub and Cloud Infrastructure",
    "text": "There is no cloud, it’s someone else’s computer\nGo to https://openscapes.2i2c.cloud/hub/. You will be asked to log in with your GitHub Account\n\n\n\nOpenscapes JupyterHub Login\n\n\nOnce we are logged with our Github account we need to select our server type. There are different hardware configurations for each profile, for the duration of the Hackweek we’ll use small instances, the option at the top.\n\n\n\nMachine Profiles\n\n\nAfter we select our server type and click on start, Jupyterhub will allocate our instance using Amazon Web Services (AWS). This may take several minutes. While we wait, we’ll get set up with GitHub and a brief overview.\n\n\n\nJupyterhub Spawning"
  },
  {
    "objectID": "clinic/jupyterhub.html#jupyter-ecosystem",
    "href": "clinic/jupyterhub.html#jupyter-ecosystem",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Jupyter Ecosystem",
    "text": "Source: Project Pythia"
  },
  {
    "objectID": "clinic/jupyterhub.html#pythonconda-environments",
    "href": "clinic/jupyterhub.html#pythonconda-environments",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Python/Conda environments",
    "text": "name: nsidc\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel\n  - awscli~=1.21.4\n  - requests\n  - pip\n\nHow do I get my code in and out of the Openscapes hub?\nWhen you start your own server you will have access to your own virtual drive space. No other users will be able to see or access your data files. You can easily upload files to your virtual drive space and save files from the hub back to another location, such as GitHub or your own local laptop drive.\nHere we’ll show you how to pull (copy) some files from GitHub into your virtual drive space using git. This will be a common task during the hackweek: at the start of each tutorial we’ll ask you to “fork” (create your own copy of in your GitHub account) and “clone” (make a copy of in a computing environment, such as your local computer or Openscapes instance) the GitHub repository corresponding to the specific tutorial being taught into your Openscapes drive space.\n\n\n\nterminal-button\n\n\nThis will open a new terminal tab in your JupyterLab interface:\n\n\n\nterminal-tab\n\n\nNow you can issue any Linux commands to manage your local file system.\nYou may also upload files from your local system using the upload button (up-pointing arrow) on the top left of the JupyterHub navigation panel. Similarly, you may download files to your local system by right-clicking the file and selecting download (down-pointing arrow).\nSimple, example GitHub/git/local-workspace workflows for getting a tutorial started in your Openscapes instance and working on a group project are provided. The getting started on a tutorial workflow briefly reviews much of the information in this preliminary exercise along with steps for making and saving notes or other changes as you work through the tutorial and keeping it updated with the original, master copy. The basic git workflow for a project serves as a reminder of the git workflow for working on a group project while minimizing code conflicts that could result from multiple people making changes to the same files simultaneously."
  },
  {
    "objectID": "clinic/jupyterhub.html#how-do-i-end-my-openscapes-session-will-i-lose-all-of-my-work",
    "href": "clinic/jupyterhub.html#how-do-i-end-my-openscapes-session-will-i-lose-all-of-my-work",
    "title": "NASA Openscapes Cloud Environment",
    "section": "How do I end my Openscapes session? Will I lose all of my work?",
    "text": "When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and just click “Log Out”!\n\n\n\nhub-control-panel-button\n\n\n!!! warning “logging out” Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.\nReferences:\n\nProject Pythia\nWhy Jupyter is data scientists’ computational notebook of choice\nClosed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics\nIntroduction to Geospatial Concepts\n2i2c user storage\nSnowEX Hackweek"
  },
  {
    "objectID": "clinic/notebooks.html#summary",
    "href": "clinic/notebooks.html#summary",
    "title": "Notebooks, Python, Git",
    "section": "Summary",
    "text": "In this session, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nVersion Control (code management using git)\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\n\nYou will need a working knowledge of git and terminal for this hackathon. We will provide an overview of these topics and also share resources for self-paced learning."
  },
  {
    "objectID": "clinic/notebooks.html#introduction-command-line-terminalshell",
    "href": "clinic/notebooks.html#introduction-command-line-terminalshell",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Command Line (Terminal/Shell)",
    "text": "Shell Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\n\n\n\nShell: More Details\nDetailed self-paced lesson on shell: Shell Lesson from Software Carpentry"
  },
  {
    "objectID": "clinic/notebooks.html#introduction-version-control-git-and-github",
    "href": "clinic/notebooks.html#introduction-version-control-git-and-github",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Version Control (Git and Github)",
    "text": "What is Version Control?\n\n\nConfigure git\nConfigure git with your name and email address\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n\n\nStore credentials\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nConfigure Git to store your credentials to avoid having to enter the credentials every time.\ngit config --global credential.helper store\ngit config -l\nUse a git command that requires you to enter credentials (e.g. pull, push)\ngit pull\nProvide your github “username” and “Github Token” when asked. Now your password is stored in “~/.git-credentials” and you will not be prompted again unless the Github token expires. You can check the presence of this file using Terminal\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output looks like this:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\n\n\nClone a Repository\nTo clone a repository from github, copy the link for the repository and use git clone to create a copy of the repository:\ngit clone https://github.com/NASA-Openscapes/check_github_setup\n\n\nTracking Changes\n\nstatus\nadd\ncommit\nlog\ndiff\n\n\n\nUsing Remotes in GitHub\n\nremote\npull\npush\n\n\n\nGit Commands: Summary\n\nCommonly used git commands (modified from source)\n\n\n\n\n\n\nGit Command\nSummary\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!"
  },
  {
    "objectID": "clinic/notebooks.html#introduction-programming-in-python",
    "href": "clinic/notebooks.html#introduction-programming-in-python",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Programming in Python",
    "text": "Switch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nPython Learning Resources\nSelf-paced lesson on Programming with Python from Software Carpentry"
  },
  {
    "objectID": "clinic/notebooks.html#introduction-geospatial-fundamentals-optional",
    "href": "clinic/notebooks.html#introduction-geospatial-fundamentals-optional",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Geospatial Fundamentals (Optional)",
    "text": "Detailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry"
  },
  {
    "objectID": "clinic/earthdata.html#overview",
    "href": "clinic/earthdata.html#overview",
    "title": "",
    "section": "Overview",
    "text": "NASA data are stored at one of several Distributed Active Archive Centers (DAACs). If you’re interested in available data for a given area and time of interest, the Earthdata Search portal provides a convenient web interface."
  },
  {
    "objectID": "clinic/earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "clinic/earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "",
    "section": "Why do I need an Earthdata login?",
    "text": "Each participant will need a login. We will be teaching you ways to programmatically access NASA data from within your Python scripts. You will need to enter your Earthdata username and password in order for this to work."
  },
  {
    "objectID": "clinic/earthdata.html#getting-an-earthdata-login",
    "href": "clinic/earthdata.html#getting-an-earthdata-login",
    "title": "",
    "section": "Getting an Earthdata login",
    "text": "If you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:\n\n\n\nearthdata-login"
  },
  {
    "objectID": "clinic/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "clinic/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "",
    "section": "Configure programmatic access to NASA servers",
    "text": "If you use web interfaces to retrieve nasa data such as Earthdata Search you are prompted to login. We will be using software to retrieve data from NASA Servers during the hackweek, so you must store your credentials on the JupyterHub as explained in this documentation. Run the following commands on the JupyterHub in a terminal replacing your Earthdata login username and password:\necho \"machine urs.earthdata.nasa.gov login EARTHDATA_LOGIN password EARTHDATA_PASSWORD\" > ~/.netrc\nchmod 0600 .netrc"
  },
  {
    "objectID": "logistics/another-thing.html",
    "href": "logistics/another-thing.html",
    "title": "",
    "section": "",
    "text": "write what you want"
  },
  {
    "objectID": "logistics/index.html#for-hackathon-participants",
    "href": "logistics/index.html#for-hackathon-participants",
    "title": "Logistics",
    "section": "For hackathon participants",
    "text": "Before the hackathon, please complete the prerequisites and be prepared for the daily setup and be familiar with the ways of getting help."
  },
  {
    "objectID": "logistics/index.html#for-hackathon-helpers",
    "href": "logistics/index.html#for-hackathon-helpers",
    "title": "Logistics",
    "section": "For hackathon helpers",
    "text": "Slack\nWhen you see a question you will reply to, please add the “eyes” emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help.\n\n\nBreakout rooms\nThe tech host will monitor Zoom Chat, moving participants into breakout rooms with Helpers as needed."
  },
  {
    "objectID": "logistics/prerequisites.html#prerequisites",
    "href": "logistics/prerequisites.html#prerequisites",
    "title": "Prerequisites, setup, & getting help",
    "section": "Prerequisites",
    "text": "Before the Hackathon, please do the following (20 minutes). All software is free. If you are attending the Clinic, please do this in advance of the Clinic.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nPlease provide your GitHub username here; this will allow us to add you to the cloud hackathon workspace.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the workshop!\n\nSlack\n\nJoin our Slack workspace (invite sent via email). In the 2021-nasacloudhack-projects channel, suggest a hackathon project idea/use case, and mention if you are looking for teammates to join. Read through other entries and comment on those of interest to you. On Day one of the hackathon we will hold a project pitchfest and finalize teams for the week.\nLearn more about how we’ll use Slack during the workshop\n\nCheck back soon for additional items\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You will be following along in Jupyter Hub on your own computer while also watching an instructor live-code over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links."
  },
  {
    "objectID": "logistics/prerequisites.html#daily-setup",
    "href": "logistics/prerequisites.html#daily-setup",
    "title": "Prerequisites, setup, & getting help",
    "section": "Daily setup",
    "text": "Zoom: Call into Zoom using the link provided in the Slack 2021-nasacloudhack-general Channel.\nJupyterHub: Log into 2i2c at https://openscapes.2i2c.cloud/hub/. This takes a few minutes so please start this as soon we reconvene each day"
  },
  {
    "objectID": "logistics/prerequisites.html#getting-help",
    "href": "logistics/prerequisites.html#getting-help",
    "title": "Prerequisites, setup, & getting help",
    "section": "Getting help",
    "text": "We will use Slack rather than Zoom Chat as our main channel for help, since the conversations are preserved beyond a single call and since it’s easier to reply and have threaded conversations and post screenshots.\n\nSlack\nYou will be invited to the Openscapes Slack organization, where there is a growing community of Openscapes Champions. (New to Slack? See this Quick Start Guide). We have three private Slack channels for the Hackathon:\n2021-nasacloudhack-general: General channel is for announcements and general questions and communications.\n2021-nasacloudhack-projects: Projects channel is for for participants to pitch project ideas and discuss projects with the whole hackathon group. As teams form you can have direct messages with your team (and we can create new channels as needed).\n2021-nasacloudhack-help: Help channel is the place to get troubleshooting help: please paste error messages and post screenshots and we will help you by replying to your post.\nTo create a screenshot:\n\nOn your Mac - Screenshot\nOn your PC - Snipping Tool\n\n\n\nZoom Breakout Rooms\n\nDuring Tutorials Session\nIf you’d like to talk to someone and live-screenshare about your issue, please write in Zoom Chat that you need help and we will move you into a breakout room with a helper.\n\n\nDuring Team Hacktime\nDuring the team project time, you will be placed in a Zoom breakout room with your respective teammates to collaborate more easily. If you have questions as you work, post your question(s) in the Slack 2021-nasacloudhack-help Channel and a helper will respond in that thread. If needed, a helper can also join your team’s Zoom breakour room for easy screensharing, troubleshooting or to further discuss a question."
  },
  {
    "objectID": "logistics/schedule.html#pre-hackathon-clinic-november-9",
    "href": "logistics/schedule.html#pre-hackathon-clinic-november-9",
    "title": "Schedule",
    "section": "Pre-Hackathon Clinic: November 9",
    "text": "This Clinic is optional and we will share a recording that participants can review ahead of time.\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nJulie Lowndes, Openscapes\n\n\n8:05 am\nJupyterHub, repos, environments\nLuis Lopez, NSIDC\n\n\n9:00 am\nBreak\n\n\n\n9:05 am\nNotebooks, python, syncing\nMakhan Virdi, ASDC\n\n\n10:00 am\nClosing"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-1-november-15",
    "href": "logistics/schedule.html#hackathon-day-1-november-15",
    "title": "Schedule",
    "section": "Hackathon Day 1: November 15",
    "text": "Time, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n9:00 am\nWelcome\nErin Robinson, Openscapes\n\n\n9:25 am\nCloud Paradigm Overview\nCatalina Oaida, PO.DAAC\n\n\n9:45 am\nTutorial 0: Getting set up and connected\nLuis Lopez, NSIDC\n\n\n10:45 am\nQ&A and Break\n\n\n\n11:00 am\nTutorial 1: Data discovery with CMR\nAndy Barrett, NSIDC\n\n\n11:30 am\nTutorial 2: Data discovery with CMR-STAC API\nAaron Friesz, LP DAAC\n\n\n12:00 pm\nQ&A and Break\n\n\n\n12:15 pm\nPitchfest\nCatalina Oaida, PO.DAAC\n\n\n12:55 pm\nClosing"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-2-november-16",
    "href": "logistics/schedule.html#hackathon-day-2-november-16",
    "title": "Schedule",
    "section": "Hackathon Day 2: November 16",
    "text": "Time, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC\n\n\n9:15 am\nDemo: the power of Earthdata Cloud and Q&A\nMarisol Garcia Reyes, Farallon Institute\n\n\n9:35 am\nTutorial 3: Introduction to Xarray\nAndy Barrett, NSIDC\n\n\n10:30 am\nQ&A and Break\n\n\n\n10:50 am\nTutorial 4: Authentication for NASA Earthdata\nAaron Friesz, LP DAAC\n\n\n11:05 am\nTutorial 5: Direct S3 Access\nAaron Friesz, LP DAAC\n\n\n11:35 am\nTeam Hack Time\nAll\n\n\n12:55 pm\nClosing"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-3-november-17",
    "href": "logistics/schedule.html#hackathon-day-3-november-17",
    "title": "Schedule",
    "section": "Hackathon Day 3: November 17",
    "text": "Time, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC\n\n\n9:15 am\nTutorial 6: TBD\nJack McNelis, PO.DAAC\n\n\n9:45 am\nQ&A and Break\n\n\n\n10:00 am\nTutorial 7: EDC and on-prem DAAC hybrid use case\nAmy Steiker, NSIDC\n\n\n10:40 am\nQ&A and Break\n\n\n\n11:00 am\nTeam Hack Time\nAll\n\n\n12:55 pm\nClosing"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-4-november-18",
    "href": "logistics/schedule.html#hackathon-day-4-november-18",
    "title": "Schedule",
    "section": "Hackathon Day 4: November 18",
    "text": "Time, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC\n\n\n9:15 am\nTutorial 8: Data Subsetting and Transformation Services in the Cloud\nAmy Steiker, NSIDC\n\n\n10:00 am\nQ&A and Break\n\n\n\n10:15 am\nTutorial 9: Access COF data vis Zarr EOSDIS Store\nPatrick Quinn, Element84\n\n\n11:00 am\nQ&A and Break\n\n\n\n11:20 am\nTeam Hack Time\nAll\n\n\n12:55 pm\nClosing"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-5-november-19",
    "href": "logistics/schedule.html#hackathon-day-5-november-19",
    "title": "Schedule",
    "section": "Hackathon Day 5: November 19",
    "text": "Time, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nTeam Hack Time\nAll\n\n\n10:00 am\nTeam Report-outs Part 1\nTeams 1-6\n\n\n11:00 am\nBreak\n\n\n\n11:15 am\nTeam Report-outs Part 2\nTeams 7-12\n\n\n12:15 pm\nSurvey\n\n\n\n12:30 pm\nWhat’s next\nJulie Lowndes, Openscapes\n\n\n12:55 pm\nClosing"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2021 Cloud Hackathon",
    "section": "Welcome",
    "text": "Welcome to Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud, co-hosted by the NASA EOSDIS Physical Oceanography Distributed Active Archive Center (PO.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Land Processes Distributed Active Archive Center (LP.DAAC), with support provided by ASDC DAAC, GES DISC and NASA Openscapes.\nThe Cloud Hackathon will take place virtually from November 15-19, 2021. The event is free to attend, but an application is required. The application period (September 21 - October 12, 2021) is now closed. Those who applied will be informed of the outcome on or around October 20th, 2021."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2021 Cloud Hackathon",
    "section": "About",
    "text": "The Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud is a virtual 5-day (4 hours per day) collaborative open science learning experience aimed at exploring, creating, and promoting effective cloud-based science and applications workflows using NASA Earthdata Cloud data, tools, and services (among others), in support of Earth science data processing and analysis in the era of big data. Its goals are to:\n\nIntroduce Earth science data users to NASA Earthdata cloud-based data products, tools and services in order to increase awareness and support transition to cloud-based science and applications workflows.\nEnable science and applications workflows in the cloud that leverage NASA Earth Observations and capabilities (services) from within the NASA Earthdata Cloud, hosted in Amazon Web Services (AWS) cloud, thus increasing NASA Earthdata data utility and meaningfulness for science and applications use cases.\nFoster community engagement utilizing Earthdata cloud tools and services in support of open science and open data.\n\nOutcome: Participants prototype their science and applications workflows (via hackathon projects) that leverage Earthdata Cloud data and services (focusing on, but not limited to, oceanography, cryosphere, hydrology and land data), which supports them in their transition to cloud-based or hybrid workflows for data processing and analysis.\nThis is an opportunity for researchers that might not yet have had the opportunity to work in the Cloud to explore, learn and prototype workflows with NASA Earthdata in the Cloud, but more intermediate or advanced cloud users interested in further exploring cloud workflows with Earthdata Cloud data and service are also welcome."
  },
  {
    "objectID": "index.html#application",
    "href": "index.html#application",
    "title": "2021 Cloud Hackathon",
    "section": "Application",
    "text": "Information for applicants\nThe Cloud Hackathon will be a virtual event held November 15-19, 2021, where participants will explore the intersection of Earth science data, cloud computing, and big data analysis through demonstration tutorials and hands-on “hacking” projects. To best benefit from the event, we recommend some familiarity or experience with:\n\nNASA Earthdata data (focusing on oceanography, cryosphere, hydrology, cryosphere and land data, including interdisciplinary applications); and\nProgramming skills using Python. We plan to accept participants with diverse skill levels and backgrounds in programming. However, to best benefit from and contribute to the program, participants are expected to have some experience with Python programming.\n\nNo cloud computing experience is required, but we encourage both beginner and more experienced participants with AWS cloud to apply.\nIf selected, participants will have the option to attend a Carpentries-style github, python, shell scripting clinic ahead of the Cloud Hachathon.\n\n\nApplication Form\nIn the application form, we encourage you to think about and provide a science use case that you would like to prototype in the cloud. At the beginning of the hackathon, participants will be able to pitch their use case to support the formation of “hack” projects - by which we mean collaboratively experiment working in with NASA Earthdata data and capabilities in the Cloud. During the hackathon, participants will get into teams of their choosing, around a common use case to “hack” in the cloud. The use cases provided in the application form will also help the organizers best prepare materials tailored to those use cases.\nThe application period has now closed. Thank you for your interest."
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "2021 Cloud Hackathon",
    "section": "What to expect",
    "text": "During the Cloud Hackathon, the selected participants will have access to cloud environments in AWS through a JupyterHub interface, provided through 2i2c.\nParticipants will be guided on how to log into the cloud environment, import needed data recipes and resources, and will have the opportunity to explore and develop science and applications workflows in a cloud environment (hosted in AWS) using example tutorials as building blocks.\nThe Cloud Hackathon is an open science event: all tutorials and examples are developed openly and will be publicly available during and following hackathon. Participants will strengthen their practice of open science, using open source code and “hacking” their projects openly to enable further discovery and contributions by the broader open community following the hackathon.\nThroughout the hackathon, participants will learn about NASA’s Earthdata move to the cloud and Earthdata APIs for data discovery, access, and transformations to enable faster, more efficient time to science.\n\nIn the two to three weeks leading up to the hackathon, participants are encouraged to review background resources that will facilitate a more effective hackathon experience. These resources will be shared here leading up to the Hackathon dates, and will be accessible to all data users, whether they attend the hackathon or not.\nThe following datasets are currently available from the NASA Earthdata Cloud. Participants can choose to prototype a cloud-based science workflow using a combination of these datasets, as well as other non-Earthdata Cloud data. If your preferred dataset is not yet available in the Earthdata Cloud, consider using a current cloud-based dataset as proxy to explore prototyping.\n\nhttps://search.earthdata.nasa.gov/search?ff=Available%20from%20AWS%20Cloud\n\nExample use cases to explore in the cloud (note these are for inspiration only, you are not limited to these workflows):\n\nUse the advanced wildcard search capabilities in Earthdata Search Client/Common Metadata Repository (CMR) to precisely search/select all cloud-archived Sentinel-6A granules\n\nfrom a specific cycle (i.e. a sequence orbits that together provide global spatial coverage), and/or\nfrom a specific pass(es) over multiple cycles (i.e. selected orbits over a series of cycles that together provide a time series coverage).\nThen, prepare the data for gridding or for local analysis at space/time scales which are appropriate for the target analysis (and limited by default given the length of S6A data record…)\n\nTime series analysis across multi-mission measurements spanning data housed both within and outside of NASA Earthdata Cloud, to develop a workflow that can accommodate different data locations, as data continue to migrate to the Cloud:\n\nProgrammatically search for a data variable (e.g. altimetry measurements) at a single point or area of interest across multiple datasets and identify whether the data are available in the Cloud\nAcquire the data based on archived location and combine in order to produce a homogenous time series\n\nExplore/leverage cloud-optimized formats (COFs) such as Zarr to compute global or regional climatology and anomalies for a large-volume dataset (e.g. 1-km MUR SST) without having to download data (in-cloud analysis).\nSubset Level 2 swath dataset of interest spatially and for specific variable and do some exploratory analysis and visualization from within the cloud.\nUse NASA’s CMR-STAC API to search and discover Harmonized Landsat Sentinel-2 (HLS) cloud assets based on cloud data products, area of interest, and date range query parameters.\nHarmonized Landsat Sentinel-2 (HLS) for land monitoring: access, explore, and visualize time series surface reflectance data in the cloud.\n\nThis event is motivated by the dawn of the era of Big Data. NASA’s Earth Observing System Data and Information System (EOSDIS) is in the process of moving EOSDIS data to the cloud, driven by a rapid rate of data ingest into the EOSDIS archive. NASA remote sensing data from both upcoming (e.g. SWOT) and existing (e.g. Terra, Aqua, ICESat-2) missions will be available in the Earthdata Cloud platform in the coming years. The paradigm shift from on-premise (local) to cloud-based data distribution, and that from “download and analyze” to “analysis in place” present opportunities and challenges. Guiding users through this transition is of the utmost importance."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "2021 Cloud Hackathon",
    "section": "Code of Conduct",
    "text": "The 2021 Cloud Hackathon is a safe learning space and all participants are required to abide by our Code of Conduct."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2021 Cloud Hackathon",
    "section": "Acknowledgements",
    "text": "Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud is co-hosted by NASA’s PO.DAAC, NSIDC DAAC, LP.DAAC, with support from ASDC DAAC, GES DISC and the NASA Openscapes Project, and cloud computing infrastructure by 2i2c.   \nOther resources that have helped inform our hackathon:\neScience Institute, University of Washington: - https://uwhackweek.github.io/hackweeks-as-a-service/intro.html - https://snowex-hackweek.github.io/website/intro.html - https://icesat-2hackweek.github.io/learning-resources/"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#summary",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Hello World"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#exercise",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#exercise",
    "title": "",
    "section": "Exercise",
    "text": "Import Required Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport subprocess\nimport requests\nimport boto3\nfrom pystac_client import Client\nfrom collections import defaultdict\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geopandas\nimport pyproj\nfrom pyproj import Proj\nfrom shapely.ops import transform\nimport geoviews as gv\nfrom cartopy import crs\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\nGet Temporary Credentials and Configure Local Environment\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME.\ndef get_temp_creds():\n    temp_creds_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n    return requests.get(temp_creds_url).json()\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\nInsert the credentials into our boto3 session and configure out rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session can then be used to pass those credentials and get S3 objects from applicable buckets.\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7fdb42409c10>\n\n\n\n\n\nRead In and Process STAC Asset Links\nIn the previous section, we used the NASA CMR-STAC API to discover HLS assets the intersect with our search criteria, i.e., ROI, Date range, and collections. The search results were filtered and saved as text files by individual bands for each tile. We will read in the text files for tile T13TGF for the RED (L30: B04 & S30: B04), NIR (L30: B05 & S30: B8A), and Fmask bands.\n\nList text files with HLS links\n\n[t for t in os.listdir('./data') if '.txt' in t]\n\n['HTTPS_T13TGF_B02_Links.txt',\n 'S3_T13TGF_B05_Links.txt',\n 'HTTPS_T13TGF_Fmask_Links.txt',\n 'S3_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B04_Links.txt',\n 'S3_T13TGF_B04_Links.txt',\n 'S3_T13TGF_Fmask_Links.txt',\n 'HTTPS_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B05_Links.txt',\n 'S3_T13TGF_B02_Links.txt']\n\n\n\n\nRead in our asset links for BO4 (RED)\n\nred_s3_links = open('./data/S3_T13TGF_B04_Links.txt').read().splitlines()\nred_s3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\n\nRead in and combine our asset links for BO5 (Landsat NIR) and B8A (Sentinel-2 NIR)\nThe near-infrared (NIR) band for Landsat is B05 while the NIR band for Sentinel-2 is B8A. In the next step we will read in and combine the lists into a single NIR list.\n\nnir_bands = ['B05', 'B8A']\nnir_link_text = [x for x in os.listdir('./data') if any(b in x for b in nir_bands) and 'S3' in x]\nnir_s3_links = []\nfor file in nir_link_text:\n    nir_s3_links.extend(open(f'./data/{file}').read().splitlines())\nnir_s3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B8A.tif']\n\n\n\n\nRead in our asset links for Fmask\nfmask_s3_links = open('./data/S3_T13TGF_Fmask_Links.txt').read().splitlines()\n#fmask_s3_links\nIn this example we will use the gdalbuildvrt.exe utility to create a time series virtual raster format (VRT) file. The utility, however, expects the links to be formated with the GDAL virtual file system (VSI) path, rather than the actual asset links. We will therefore use the VSI path to access our assets. The examples below show the VSI path substitution for S3 (vsis3) links.\n/vsis3/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2020191T172901.v1.5.B04.tif\nSee the GDAL Virtual File Systems for more information regarding GDAL VSI.\n\n\nWrite out a new text file containing the vsis3 path\nwith open('./data/S3_T13TGF_RED_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in red_s3_links]\n    for link in links_vsi:\n        f.write(link)\nwith open('./data/S3_T13TGF_NIR_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in nir_s3_links]\n    for link in links_vsi:\n        f.write(link)\nwith open('./data/S3_T13TGF_FMASK_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in fmask_s3_links]\n    for link in links_vsi:\n        f.write(link)\n\n\n\nRead in geoJSON for subsetting\nWe will use the input geoJSON file to clip the source data to our desired region of interest.\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]  \nTo clip the source data to our input feature boundary, we need to transform the feature boundary from its original WGS84 coordinate reference system to the projected reference system of the source HLS file (i.e., UTM Zone 13).\n\nfoa_url = red_s3_links[0]\nwith rio.open(foa_url) as src:\n    hls_proj = src.crs.to_string()\n\nhls_proj    \n\n'EPSG:32613'\n\n\n\nTransform geoJSON feature from WGS84 to UTM\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\nproject = pyproj.Transformer.from_proj(geo_CRS, hls_proj)                    # Set up the transformation\nfsUTM = transform(project.transform, fieldShape)\n\n\n\nDirect S3 Data Access\n\nStart up a dask client\n#from dask.distributed import Client\n#client = Client(n_workers=2)\n#client\nThere are multiple way to read COG data in as a time series. The subprocess package is used in this example to run GDAL’s build virtual raster file (gdalbuildvrt) executable outside our python session. First we’ll need to construct a string object with the command and it’s parameter parameters (including our temporary credentials). Then, we run the command using the subprocess.call() function.\n\n\nBuild GDAL VRT Files\n\nConstruct the GDAL VRT call\nbuild_red_vrt = f\"gdalbuildvrt ./data/red_stack.vrt -separate -input_file_list ./data/S3_T13TGF_RED_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\n#build_red_vrt    # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\nWe now have a fully configured gdalbuildvrt string that we can pass to Python’s subprocess module to run the gdalbuildvrt executable outside our Python environment.\n\n\n\nExecute gdalbuildvrt to construct a VRT on disk from the S3 links\n\n%%time\n\nsubprocess.call(build_red_vrt, shell=True)\n\nCPU times: user 2.1 ms, sys: 3.83 ms, total: 5.93 ms\nWall time: 3.74 s\n\n\n0\n\n\n0 means success! We’ll have some troubleshooting to do you get any other value. In this tutorial, the path for the output VRT file or the input file list are the first things to check.\nWhile we’re here, we’ll build the VRT files for the NIR layers and the Fmask layers.\n\nbuild_nir_vrt = f\"gdalbuildvrt ./data/nir_stack.vrt -separate -input_file_list ./data/S3_T13TGF_NIR_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\nsubprocess.call(build_nir_vrt, shell=True)\n\n0\n\n\n\nbuild_fmask_vrt = f\"gdalbuildvrt ./data/fmask_stack.vrt -separate -input_file_list ./data/S3_T13TGF_FMASK_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\nsubprocess.call(build_fmask_vrt, shell=True)\n\n0\n\n\n\n\n\nReading in an HLS time series\nWe can now read the VRT files into our Python session. A drawback of reading VRTs into Python is that the time coordinate variable needs to be contructed. Below we not only read in the VRT file using rioxarray, but we also repurpose the band variable, which is generated automatically, to hold out time information.\n\nRead the RED VRT in as xarray with Dask backing\n\n%%time\n\nchunks=dict(band=1, x=1024, y=1024)\n#chunks=dict(band=1, x=512, y=512)\nred = rioxarray.open_rasterio('./data/red_stack.vrt', chunks=chunks)                    # Read in VRT\nred = red.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nred['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]   # Extract the time information from the input file names and assign them to the time coordinate variable\nred = red.sortby('time')                                                                # Sort by the time coordinate variable\nred\n\nCPU times: user 219 ms, sys: 20.3 ms, total: 239 ms\nWall time: 246 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<getitem, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         485.45 MiB \n                         2.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         609 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\nAbove we use the parameter chunk in the rioxarray.open_rasterio() function to enable the Dask backing. What this allows is lazy reading of the data, which means the data is not actually read in into memory at this point. What we have is an object with some metadata and pointer to the source data. The data will be streamed to us when we call for it, but not stored in memory until with call the Dask compute() or persist() methods.\n\n\nPrint out the time coordinate\n\nred.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 19)>\narray(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0xarray.DataArray'time'time: 192021-05-13T17:24:06 2021-05-13T17:38:59 ... 2021-08-17T17:24:41array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')Coordinates: (2)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (0)\n\n\n\n\nClip out the ROI and persist the result in memory\nUp until now, we haven’t read any of the HLS data into memory. Now we will use the persist() method to load the data into memory.\n\nred_clip = red.rio.clip([fsUTM]).persist()\nred_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 56, x: 56)>\ndask.array<astype, shape=(19, 56, 56), dtype=int16, chunksize=(1, 56, 56), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    _FillValue:    -9999xarray.DataArraytime: 19y: 56x: 56dask.array<chunksize=(1, 56, 56), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         116.38 kiB \n                         6.12 kiB \n                    \n                    \n                    \n                         Shape \n                         (19, 56, 56) \n                         (1, 56, 56) \n                    \n                    \n                         Count \n                         19 Tasks \n                         19 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  56\n  56\n  19\n\n        \n    \nCoordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (3)scale_factor :0.0001add_offset :0.0_FillValue :-9999\n\n\nAbove, we persisted the clipped results to memory using the persist() method. This doesn’t necessarily need to be done, but it will substantially improve the performance of the visualization of the time series below.\n\n\nPlot red_clip with hvplot\n\nred_clip.hvplot.image(x='x', y='y', width=800, height=600, colorbar=True, cmap='Reds').opts(clim=(0.0, red_clip.values.max()))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nRead in the NIR and Fmask VRT files\n\n%%time\nchunks=dict(band=1, x=1024, y=1024)\nnir = rioxarray.open_rasterio('./data/nir_stack.vrt', chunks=chunks)                    # Read in VRT\nnir = nir.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nnir['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]   # Extract the time information from the input file names and assign them to the time coordinate variable\nnir = nir.sortby('time')                                                                # Sort by the time coordinate variable\nnir\n\nCPU times: user 69.9 ms, sys: 216 µs, total: 70.1 ms\nWall time: 81.9 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<getitem, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         485.45 MiB \n                         2.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         609 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n%%time\nchunks=dict(band=1, x=1024, y=1024)\nfmask = rioxarray.open_rasterio('./data/fmask_stack.vrt', chunks=chunks)                    # Read in VRT\nfmask = fmask.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nfmask['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]     # Extract the time information from the input file names and assign them to the time coordinate variable\nfmask = fmask.sortby('time')                                                                # Sort by the time coordinate variable\nfmask\n\nCPU times: user 64.6 ms, sys: 85 µs, total: 64.7 ms\nWall time: 74.8 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<getitem, shape=(19, 3660, 3660), dtype=uint8, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    255.0\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         242.73 MiB \n                         1.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         609 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     uint8 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :255.0scale_factor :1.0add_offset :0.0\n\n\n\n\nCreate an xarray dataset\nWe will now combine the RED, NIR, and Fmask arrays into a dataset and create/add a new NDVI variable.\n\nhls_ndvi = xr.Dataset({'red': red, 'nir': nir, 'fmask': fmask, 'ndvi': (nir - red) / (nir + red)})\nhls_ndvi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (time: 19, x: 3660, y: 3660)\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nData variables:\n    red          (time, y, x) int16 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    nir          (time, y, x) int16 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    fmask        (time, y, x) uint8 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    ndvi         (time, y, x) float64 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>xarray.DatasetDimensions:time: 19x: 3660y: 3660Coordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Data variables: (4)red(time, y, x)int16dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         485.45 MiB \n                         2.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         609 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nnir(time, y, x)int16dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         485.45 MiB \n                         2.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         609 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nfmask(time, y, x)uint8dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>_FillValue :255.0scale_factor :1.0add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         242.73 MiB \n                         1.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         609 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     uint8 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nndvi(time, y, x)float64dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         1.90 GiB \n                         8.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         2130 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nAttributes: (0)\n\n\nAbove, we created a new NDVI variable. Now, we will clip and plot our results.\n\nndvi_clip = hls_ndvi.ndvi.rio.clip([fsUTM]).persist()\nndvi_clip\n\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: divide by zero encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'ndvi' (time: 19, y: 56, x: 56)>\ndask.array<getitem, shape=(19, 56, 56), dtype=float64, chunksize=(1, 56, 56), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0xarray.DataArray'ndvi'time: 19y: 56x: 56dask.array<chunksize=(1, 56, 56), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         465.50 kiB \n                         24.50 kiB \n                    \n                    \n                    \n                         Shape \n                         (19, 56, 56) \n                         (1, 56, 56) \n                    \n                    \n                         Count \n                         19 Tasks \n                         19 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  56\n  56\n  19\n\n        \n    \nCoordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (0)\n\n\n\nPlot NDVI\n\nndvi_clip.hvplot.image(x='x', y='y', groupby='time', width=800, height=600, colorbar=True, cmap='YlGn').opts(clim=(0.0, 1.0))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nYou may have notices that some images for some of the time step are ‘blurrier’ than other. This is because they are contaminated in some way, be it clouds, cloud shadows, snow, ice.\n\n\n\nApply quality filter\nWe want to keep NDVI data values where Fmask equals 0 (no clouds, no cloud shadow, no snow/ice, no water.\n\nndvi_clip_filter = hls_ndvi.ndvi.where(fmask==0, np.nan).rio.clip([fsUTM]).persist()\n\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: divide by zero encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n\n\n\nndvi_clip_filter.hvplot.image(x='x', y='y', groupby='time', width=800, height=600, colorbar=True, cmap='YlGn').opts(clim=(0.0, 1.0))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nAggregate by month\nFinally, we will use xarray’s groupby operation to aggregate by month.\n\nndvi_clip_filter.groupby('time.month').mean('time').hvplot.image(x = 'x', y = 'y', crs = hls_proj, groupby='month', cmap='YlGn', width=800, height=600, colorbar=True).opts(clim=(0.0, 1.0))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nrio_env.__exit__()"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#references",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#references",
    "title": "",
    "section": "References",
    "text": "https://rasterio.readthedocs.io/en/latest/\nhttps://corteva.github.io/rioxarray/stable/index.html\nhttps://tutorial.dask.org/index.html\nhttps://examples.dask.org/applications/satellite-imagery-geotiff.html"
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#summary",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the harmonization of the ICESat-2 ATL03 data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, with Sea Surface Temperature variables available from PO.DAAC on the Earthdata Cloud.\n\nObjectives\n[TODO]\n\n\n\nImport packages\nimport requests\nimport netrc\nfrom pprint import pprint\nimport os\nfrom pathlib import Path\n\nimport s3fs\n\nimport xarray as xr\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n# packages below here are not used\n#from xml.etree import ElementTree as ET\n#import time\n#import zipfile\n#import io\n#import shutil\n#import json\n#from urllib import request\n\n\nDetermine storage location of datasets of interest\nFirst, let’s see whether our datasets of interest reside in the Earthdata Cloud or whether they reside on premise, or “on prem” at a local data center.\nBackground from CMR API [TODO: consider removing]: The cloud_hosted parameter can be set to “true” or “false”. When true, the results will be restricted to collections that have a DirectDistributionInformation element or have been tagged with gov.nasa.earthdatacloud.s3.\nWe are building off of the CMR introductory tutorial, beginning with a collection search.\ncmr_search_url = 'https://cmr.earthdata.nasa.gov/search'\nWe want to search by collection to inspect the access and service options that exist:\ncmr_collection_url = f'{cmr_search_url}/{\"collections\"}'\nIn the CMR introduction tutorial, we explored cloud-hosted collections from different DAAC providers, and identified the CMR concept-id for a given dataset id (also referred to as a short_name). Here we’ll start with two datasets that we want to explore over a coincident area and time:\nmodis_name = 'MODIS_A-JPL-L2P-v2019.0'\nicesat2_name = 'ATL03'\nLike in the intro tutorial, we’re going to first determine what concept-ids are returned for the MODIS dataset. First, retrieve collection results based on the MODIS short_name:\nresponse = requests.get(cmr_collection_url, \n                        params={\n                            'short_name': modis_name,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\nFor each collection result, print out the CMR concept-id and version:\ncollections = response['feed']['entry']\n\nfor collection in collections:\n    print(f'{collection[\"id\"]} {\"version:\"}{collection[\"version_id\"]}')\nTwo collections are returned, both at version 2019.0. We can see from the suffix of the id that one is associated with “POCLOUD” versus “PODAAC”. That gives us a clue in terms of where the data are hosted, but we can also use the cloud_hosted parameter set to True to confirm.\nresponse = requests.get(cmr_collection_url, \n                        params={\n                            'short_name': modis_name,\n                            'cloud_hosted': 'True',\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\ncollections = response['feed']['entry']\n\nfor collection in collections:\n    print(f'{collection[\"id\"]} {\"version:\"}{collection[\"version_id\"]}')\nWe will save this concept-id to use later on when we access the data granules.\nmodis_cloud_id = collections[0][\"id\"]\nNow we will try our ICESat-2 dataset to see what id’s are returned for a given dataset name.\nresponse = requests.get(cmr_collection_url, \n                        params={\n                            'short_name': icesat2_name,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\ncollections = response['feed']['entry']\n\nfor collection in collections:\n    print(f'{collection[\"id\"]} {\"version:\"}{collection[\"version_id\"]}')\nTwo separate datasets exist in the CMR, one at version 3 and one at version 4. Let’s see if these are cloud_hosted:\nresponse = requests.get(cmr_collection_url, \n                        params={\n                            'short_name': icesat2_name,\n                            'cloud_hosted': 'False',\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\ncollections = response['feed']['entry']\n\nfor collection in collections:\n    print(f'{collection[\"id\"]} {\"version:\"}{collection[\"version_id\"]}')\nWhen set to False, we get our collections back. We have now determined that we have a copy of the MODIS dataset in the cloud, whereas the ICESat-2 dataset (both versions) remains “on premise”, residing in a local data center.\nSave the ATL03 concept ID and the MODIS GHRSST concept ID to variables:\nicesat2_concept_id = 'C1997321091-NSIDC_ECS'\nmodis_concept_id = 'C1940475563-POCLOUD'\n\nSpecify time range and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\nPerform a granule search over our time and area of interest. How many granules are returned?\ngranule_url = f'{cmr_search_url}/{\"granules\"}'\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': icesat2_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\nPrint the file names, size, and links:\ngranules = response.json()['feed']['entry']\n\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n\n\nDownload ICESat-2 ATL03 granule\n[TODO] Describe what services are available, including icepyx (provide references), but just direct download for simplicity. Describe that this is being “downloaded” to our cloud environment - what does that mean in terms of cost, etc.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\nicesat_id = granules[0][\"producer_granule_id\"]\nicesat_url = granules[0]['links'][0]['href']\nYou need Earthdata login credentials to download data from NASA DAACs. These are the credentials you stored in the .netrc file you setup in previous tutorials.\nWe’ll use the netrc package to retrieve your login and password without exposing them.\ninfo = netrc.netrc()\nlogin, account, password = info.authenticators('urs.earthdata.nasa.gov')\nTo retrieve the granule data, we use the requests.get() method, passing Earthdata login credentials as a tuple using the auth keyword.\nr = requests.get(icesat_url, auth=(login, password))\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the mkdir method from the os package.\nos.mkdir('downloads')\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\noutfile = Path('downloads', icesat_id)\nwith open(outfile, 'wb') as f:\n    f.write(r.content)\nCheck to make sure it is downloaded.\nls -l ./downloads\nATL03_20190622061415_12980304_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the height data for ground-track 1 left-beam.\noutfile = Path('downloads', icesat_id)\nds = xr.open_dataset(icesat_id, group='/gt1l/heights')\nds\n\n\nDetermine variables of interest: SST, ocean color, chemistry…\nresponse = requests.get(cmr_collection_url, \n                        params={\n                            'concept_id': 'C1940475563-POCLOUD',\n                            'cloud_hosted': 'True',\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\nvariables = response['feed']['entry'][0]['associations']['variables']\noutput_format = \"umm_json\"\nvar_url = \"https://cmr.earthdata.nasa.gov/search/variables\"\nfor i in range(len(variables)):\n    response = requests.get(f\"{var_url}.{output_format}?concept-id={variables[i]}\")\n    response = response.json()\n    # print(response['items'][0]['umm'])\n    if 'Name' in response['items'][0]['umm']: pprint(response['items'][0]['umm']['Name'])\n\n\nPull those variables into xarray “in place”\n\nFirst, we need to determine the granules returned from our time and area of interest\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': 'C1940475563-POCLOUD',\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\nprint(response.headers['CMR-Hits'])\nmodis_granules_meta = response.json()['feed']['entry']\nfor granule_meta in modis_granules_meta:\n    print(granule_meta['boxes'])\n    print(granule_meta['links'][0]['href'])\nhttps_link = modis_granules_meta[0]['links'][0]['href']\ns3_link = https_link.replace('https://archive.podaac.earthdata.nasa.gov/','s3://')\ns3_link\n\n\n\nGet S3 credentials\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\nOpen a s3 file\nf = s3_fs.open(s3_link)\nds = xr.open_dataset(f, engine='h5netcdf')\n\n\nUse geolocation of ICESat-2 to define the single transect used to pull coincident ocean data out from array\n\n\nCreate a plot of the single transect of gridded data\n(bonus: time series) - describe what this means to egress out of the cloud versus pulling the original data down (benefit to processing in the cloud)"
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#download-modis-ghrsst-data-from-cloud",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#download-modis-ghrsst-data-from-cloud",
    "title": "",
    "section": "Download MODIS GHRSST data from Cloud",
    "text": "from shapely.geometry import box\nmap_proj = ccrs.PlateCarree()\ndef bbox_geometry(boxs, t_crs=ccrs.NorthPolarStereo()):\n    '''Generates a shapely.geometry.box object from boxes metadata'''\n    lat_min, lon_min, lat_max, lon_max = [float(v) for v in bbox.split()]\n    x_min, y_min = t_crs.transform_point(lon_min, lat_min, ccrs.PlateCarree())\n    x_max, y_max = t_crs.transform_point(lon_max, lat_max, ccrs.PlateCarree())\n    return box(x_min, y_min, x_max, y_max)\n\nbbox_features = []\nfor granule in modis_granules_meta:\n    for bbox in granule['boxes']:\n        bbox_features.append(bbox_geometry(bbox))\nfig = plt.figure(figsize=(7,7))\nax = fig.add_subplot(projection=ccrs.NorthPolarStereo())\nax.set_extent([-180.,180.,60.,90.], ccrs.PlateCarree())\nax.coastlines()\nax.add_geometries([bbox_features[0]], crs=map_proj, alpha=0.3)"
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#resources-optional",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#resources-optional",
    "title": "",
    "section": "Resources (optional)",
    "text": ""
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#conclusion",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": ""
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "tutorials/Data_discovery_with_cmr.html#what-is-cmr",
    "href": "tutorials/Data_discovery_with_cmr.html#what-is-cmr",
    "title": "",
    "section": "What is CMR",
    "text": "CMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials/Data_discovery_with_cmr.html#what-is-the-cmr-api",
    "href": "tutorials/Data_discovery_with_cmr.html#what-is-the-cmr-api",
    "title": "",
    "section": "What is the CMR API",
    "text": "API stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials/Data_discovery_with_cmr.html#how-to-search-cmr-from-python",
    "href": "tutorials/Data_discovery_with_cmr.html#how-to-search-cmr-from-python",
    "title": "",
    "section": "How to search CMR from Python",
    "text": "The first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in depth tutorial on requests is here\nimport requests\nfrom pprint import pprint\nThen we need to authenticate with EarthData Login. Since we’ve already set this up in the previous lesson, here you need to enter your username before executing the cell.\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll build this url as a python variable.\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for colections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the url for the root CMR endpoint.\nWe are going to search collections first, so we add collections to the url. I’m using a python format string here.\nurl = f'{CMR_OPS}/{\"collections\"}'\nIn this first example, I want to retrieve a list of collections that are hosted in the cloud. Each collection has a cloud_hosted parameter that is either True if that collection is in the cloud and False if it is not. The migration of NASA data to the cloud is a work in progress. Not all collections tagged as cloud_hosted have granules. To search for only cloud_hosted datasets with granules, I also set has_granules to True.\nI also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\nrequests returns a Response object.\nOften, we want to check that our request was successful. In a notebook or someother interactive environment, we can just type the name of the variable we have saved our requests Response to, in this case the response variable.\n\nresponse\n\n<Response [200]>\n\n\nA cleaner and more understandable method is to check the status_code attribute. Both methods return a HTTP status code. You’ve probably seen a 404 error when you have tried to access a website that doesn’t exist.\n\nresponse.status_code\n\n200\n\n\nTry changing CMR_OPS to https://cmr.earthdata.nasa.gov/searches and run requests.get again. Don’t forget to rerun the cell that assigns the url variable\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. This information is printed below. TODO: maybe some context for where the 2 elements k, v, come from?\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 3820\nConnection: keep-alive\nDate: Tue, 02 Nov 2021 22:07:37 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: 510cf611-d65b-4b46-981b-f4719405676a\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,14000.0,\"SENTINEL-1A_RAW\",\"1\",1214470561,1320]\nCMR-Hits: 917\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 435\nX-Request-Id: 510cf611-d65b-4b46-981b-f4719405676a\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 3f7e5e686bf8f19b9c786efbe99c7589.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: DEN52-C1\nX-Amz-Cf-Id: BXEUlb5ygB9nj9ygUbSZvIc3BE4k0d1Mn1qDd66IkkuX1rC_Z-mN6Q==\n\n\nWe can see that the content returned is in json format in the UTF-8 character set. We can also see from CMR-Hits that 919 collections were found.\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but because it is case-insensitive, both\n\nresponse.headers['CMR-Hits']\n\n'917'\n\n\nand\n\nresponse.headers['cmr-hits']\n\n'917'\n\n\nwork.\nThis is a large number of data sets. I’m going to restrict the search to cloud-hosted datasets from ASF (Alaska SAR Facility) because I’m interested in SAR images of sea ice. To do this, I set the provider parameter to ASF.\nYou can modify the code below to explore all of the cloud-hosted datasets or cloud-hosted datasets from other providers. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWhen search by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs.\nprovider = 'ASF'\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\nresponse.headers['cmr-hits']\n\n'45'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2021-11-02T22:41:34.322Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=ASF\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:39.000Z\",\"dataset_id\":\"SENTINEL-1A_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470488-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:22.000Z\",\"dataset_id\":\"SENTINEL-1B_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985661-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:56.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470533-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:47.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985645-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:49.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985660-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:58.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214471521-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:20.000Z\",\"dataset_id\":\"SENTINEL-1A_RAW\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_RAW\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_RAW\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A level zero product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470561-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:15.000Z\",\"dataset_id\":\"SENTINEL-1A_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1A_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470496-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:02.000Z\",\"dataset_id\":\"SENTINEL-1B_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1B_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985617-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:26.000Z\",\"dataset_id\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Single-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470682-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]}]}}'\n\n\nIt is more convenient to work with json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nStep through response.json(), then to response.json()['feed']['entry'][0]. A reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'ASF',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'ASF',\n 'dataset_id': 'SENTINEL-1A_SLC',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C1214470488-ASF',\n 'links': [{'href': 'https://vertex.daac.asf.alaska.edu/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['ASF', 'ESA/CS1CGS'],\n 'original_format': 'ECHO10',\n 'platforms': ['Sentinel-1A'],\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'SENTINEL-1A_SLC',\n 'summary': 'Sentinel-1A slant-range product',\n 'time_start': '2014-04-03T00:00:00.000Z',\n 'title': 'SENTINEL-1A_SLC',\n 'updated': '2021-07-15T19:16:39.000Z',\n 'version_id': '1'}\n\n\nThe first response is not the result I am looking for TODO: because xyz…but it does show a few variables that we can use to further refine the search. So I want to print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable. TODO: is it worth saying something about what “feed” and “entry” are?\ncollections = response.json()['feed']['entry']\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\n\n\nBut there is a problem. We know from CMR-Hits that there are 49 datasets but only 10 are printed. This is because CMR restricts the number of results returned by a query. The default is 10 but it can be set to a maximum of 2000. Knowing that there were 49 ‘hits’, I’ll set page_size to 49. Then, we can re-run our for loop for the collections.\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'provider': provider,\n                            'page_size': 49,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\nASF SENTINEL-1A_OCN C1214472977-ASF\nASF SENTINEL-1B_RAW C1327985647-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_HIGH_RES C1214470576-ASF\nASF SENTINEL-1B_OCN C1327985579-ASF\nASF SENTINEL-1A_METADATA_RAW C1214470532-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_HIGH_RES C1327985741-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms (BETA) C1595422627-ASF\nASF ALOS_AVNIR_OBS_ORI C1808440897-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_MEDIUM_RES C1214472994-ASF\nASF SENTINEL-1B_METADATA_RAW C1327985650-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_HIGH_RES C1327985571-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_MEDIUM_RES C1327985740-ASF\nASF SENTINEL-1A_METADATA_OCN C1266376001-ASF\nASF SENTINEL-1B_METADATA_OCN C1327985646-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES C1214472336-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_HIGH_RES C1214470732-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1214473170-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_MEDIUM_RES C1327985578-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_HIGH_RES C1327985619-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1327985739-ASF\nASF STS-68_BROWSE_GRD C1661710593-ASF\nASF STS-68_BROWSE_SLC C1661710596-ASF\nASF STS-59_BROWSE_GRD C1661710578-ASF\nASF STS-59_BROWSE_SLC C1661710581-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_FULL_RES C1214471197-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_FULL_RES C1214471960-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_FULL_RES C1214472978-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_FULL_RES C1214473165-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_FULL_RES C1327985697-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_FULL_RES C1327985651-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_FULL_RES C1327985644-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_FULL_RES C1327985674-ASF\nAlaska Satellite Facility Sentinel-1 Unwrapped Interferogram and Coherence Map (BETA) C1379535600-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Amplitude (BETA) C1596065640-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Coherence (BETA) C1596065639-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Connected Components (BETA) C1596065641-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Unwrapped Phase (BETA) C1595765183-ASF\nASF STS-59_GRD C1661710583-ASF\nASF STS-59_METADATA_GRD C1661710586-ASF"
  },
  {
    "objectID": "tutorials/Data_discovery_with_cmr.html#granule-search",
    "href": "tutorials/Data_discovery_with_cmr.html#granule-search",
    "title": "",
    "section": "Granule Search",
    "text": "In NASA speak, Granules are files. In this example, we will search for recent Sentinel-1 Ground Range Detected (GRD) Medium Resolution Synthetic Aperture Radar images over the east coast of Greenland. The data in these files are most useful for sea ice mapping.\nI’ll use the data range 2021-10-17 00:00 to 2021-10-18 23:59:59.\nI’ll use a simple bounding box to search. - SW: 76.08166,-67.1746 - NW: 88.19689,21.04862\nFrom the collections search, I know the concept ids for Sentinel-1A and Sentinel-1B GRD medium resolution are - C1214472336-ASF - C1327985578-ASF\nWe need to change the resource url to look for granules instead of collections\nurl = f'{CMR_OPS}/{\"granules\"}'\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1214472336-ASF',\n                            'temporal': '2020-10-17T00:00:00Z,2020-10-18T23:59:59Z',\n                            'bounding_box': '76.08166,-67.1746,88.19689,21.04862',\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n6\n\n\ngranules = response.json()['feed']['entry']\n#for granule in granules:\n#    print(f'{granule[\"archive_center\"]} {granule[\"dataset_id\"]} {granule[\"id\"]}')\n\npprint(granules)\n\n[{'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954601581-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34836'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.163563 87.942726 -60.893669 89.293564 -59.279579 '\n                '96.119583 -57.619923 94.49958 -59.163563 87.942726']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E',\n  'time_end': '2020-10-17T13:20:39.000Z',\n  'time_start': '2020-10-17T13:20:09.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E-METADATA_GRD_MD',\n  'updated': '2020-10-19T17:13:39.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616816-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-66.104271 69.819366 -69.571243 74.741966 -67.42112 83.209152 '\n                '-64.210938 77.59269 -66.104271 69.819366']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B',\n  'time_end': '2020-10-17T14:57:20.000Z',\n  'time_start': '2020-10-17T14:56:16.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:35:34.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616638-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-62.765087 66.277176 -66.103951 69.81897 -64.211227 77.590164 '\n                '-61.060097 73.427185 -62.765087 66.277176']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE',\n  'time_end': '2020-10-17T14:58:20.000Z',\n  'time_start': '2020-10-17T14:57:20.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:33:31.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954805829-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-65.236397 83.18071 -68.712318 87.887711 -66.630493 96.161102 '\n                '-63.400326 90.779785 -65.236397 83.18071']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659',\n  'time_end': '2020-10-18T14:00:00.000Z',\n  'time_start': '2020-10-18T13:58:56.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659-METADATA_GRD_MD',\n  'updated': '2020-10-20T07:16:54.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954799806-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-61.876564 79.848251 -65.236069 83.180344 -63.400402 90.778 '\n                '-60.217258 86.840034 -61.876564 79.848251']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777',\n  'time_end': '2020-10-18T14:01:00.000Z',\n  'time_start': '2020-10-18T14:00:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:51:16.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954798927-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.113174 77.623962 -61.876228 79.847961 -60.217464 '\n                '86.837784 -57.570774 84.174744 -59.113174 77.623962']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D',\n  'time_end': '2020-10-18T14:01:48.000Z',\n  'time_start': '2020-10-18T14:01:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:46:01.000Z'}]"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html",
    "href": "tutorials/Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html",
    "title": "",
    "section": "",
    "text": "Direct S3 Data Access - Rough PODAAC ECCO SSH Example"
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#summary",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the harmonization of the ICESat-2 ATL03 data product, currently (as of November 2021) available publicaly via direct download at the NSIDC DAAC, with Sea Surface Temperature variables available from PO.DAAC on the Earthdata Cloud.\n\nObjectives\n\n\n\nImport packages\nimport requests\nimport netrc\nfrom pprint import pprint\nimport os\nfrom pathlib import Path\n\nimport s3fs\n\nimport xarray as xr\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n# packages below here are not used\n#from xml.etree import ElementTree as ET\n#import time\n#import zipfile\n#import io\n#import shutil\n#import json\n#from urllib import request\n\n\nDetermine storage location of datasets of interest\nFirst, let’s see whether our datasets of interest reside in the Earthdata Cloud or whether they reside on premise, or “on prem” at a local data center.\nBackground from CMR API (consider removing): The cloud_hosted parameter can be set to “true” or “false”. When true, the results will be restricted to collections that have a DirectDistributionInformation element or have been tagged with gov.nasa.earthdatacloud.s3. curl “https://cmr.earthdata.nasa.gov/search/collections?cloud_hosted=true”\n\n\nDeclare datasets of interest\nIdentify the dataset ID that is used internally within CMR to designate each dataset\nmodis_name = 'MODIS_T-JPL-L2P-v2019.0'\n\nicesat2_name = 'ATL03'\n# icesat2_id = 'C1997321091-NSIDC_ECS'\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nresponse = requests.get(url, \n                        params={\n                            'short_name': modis_name,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"id\"]} {\"version:\"}{collection[\"version_id\"]}')\n\nC1940475563-POCLOUD version:2019.0\nC1693233387-PODAAC version:2019.0\n\n\nStart with the MODIS dataset, setting the cloud_hosted parameter to True:\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1940475563-POCLOUD',\n                            'cloud_hosted': 'True',\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\nprint(response.headers['CMR-Hits'])\ncollections = response.json()['feed']['entry']\npprint(collections)\n\n200\n1\n[{'archive_center': 'NASA/JPL/PODAAC',\n  'associations': {'services': ['S1962070864-POCLOUD', 'S2004184019-POCLOUD'],\n                   'tools': ['TL2108419875-POCLOUD', 'TL2092786348-POCLOUD'],\n                   'variables': ['V1997811750-POCLOUD',\n                                 'V1997811794-POCLOUD',\n                                 'V2112014697-POCLOUD',\n                                 'V1997811877-POCLOUD',\n                                 'V1997811902-POCLOUD',\n                                 'V2028668027-POCLOUD',\n                                 'V2028632036-POCLOUD',\n                                 'V1997811775-POCLOUD',\n                                 'V1997811764-POCLOUD',\n                                 'V1997811783-POCLOUD',\n                                 'V2112014702-POCLOUD',\n                                 'V2028632034-POCLOUD',\n                                 'V2112014700-POCLOUD',\n                                 'V1997811759-POCLOUD',\n                                 'V2028632038-POCLOUD']},\n  'boxes': ['-90 -180 90 180'],\n  'browse_flag': True,\n  'collection_data_type': 'SCIENCE_QUALITY',\n  'coordinate_system': 'CARTESIAN',\n  'data_center': 'POCLOUD',\n  'dataset_id': 'GHRSST Level 2P Global Sea Surface Skin Temperature from the '\n                'Moderate Resolution Imaging Spectroradiometer (MODIS) on the '\n                'NASA Terra satellite (GDS2)',\n  'has_formats': True,\n  'has_spatial_subsetting': True,\n  'has_temporal_subsetting': True,\n  'has_transforms': False,\n  'has_variables': True,\n  'id': 'C1940475563-POCLOUD',\n  'links': [{'href': 'https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst4/',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://oceancolor.gsfc.nasa.gov/reprocessing/r2019/sst/',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst/flag/',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://ghrsst.jpl.nasa.gov',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://modis.gsfc.nasa.gov/data/atbd/atbd_mod25.pdf',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://podaac.jpl.nasa.gov/Podaac/thumbnails/MODIS_T-JPL-L2P-v2019.0.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://podaac.jpl.nasa.gov/forum/viewforum.php?f=18&sid=e2d67e5a01815fc6e39fcd2087ed8bc8',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst/',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://github.com/podaac/data-readers',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'http://www.ghrsst.org',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://podaac.jpl.nasa.gov/CitingPODAAC',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': ' '\n                     'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C1940475563-POCLOUD',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n            {'href': 'https://github.com/podaac/tutorials/blob/master/notebooks/MODIS_L2P_SST_DataCube.ipynb',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n            {'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1940475563-POCLOUD',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_parameters': {'inclination_angle': '98.2',\n                       'number_of_orbits': '1.0',\n                       'period': '98.8',\n                       'swath_width': '2330.0'},\n  'organizations': ['NASA/JPL/PODAAC'],\n  'original_format': 'UMM_JSON',\n  'platforms': ['Terra'],\n  'processing_level_id': '2',\n  'service_features': {'esi': {'has_formats': False,\n                               'has_spatial_subsetting': False,\n                               'has_temporal_subsetting': False,\n                               'has_transforms': False,\n                               'has_variables': False},\n                       'harmony': {'has_formats': True,\n                                   'has_spatial_subsetting': True,\n                                   'has_temporal_subsetting': True,\n                                   'has_transforms': False,\n                                   'has_variables': True},\n                       'opendap': {'has_formats': True,\n                                   'has_spatial_subsetting': True,\n                                   'has_temporal_subsetting': True,\n                                   'has_transforms': False,\n                                   'has_variables': True}},\n  'short_name': 'MODIS_T-JPL-L2P-v2019.0',\n  'summary': 'NASA produces skin sea surface temperature (SST) products from '\n             'the Infrared (IR) channels of the Moderate-resolution Imaging '\n             'Spectroradiometer (MODIS) onboard the Terra satellite. Terra was '\n             'launched by NASA on December 18, 1999, into a sun synchronous, '\n             'polar orbit with a daylight descending node at 10:30 am, to '\n             'study the global dynamics of the Earth atmosphere, land and '\n             'oceans. The MODIS captures data in 36 spectral bands at a '\n             'variety of spatial resolutions.  Two SST products can be present '\n             'in these files. The first is a skin SST produced for both day '\n             'and night observations, derived from the long wave IR 11 and 12 '\n             'micron wavelength channels, using a modified nonlinear SST '\n             'algorithm intended to provide continuity with SST derived from '\n             'heritage and current NASA sensors. At night, a second SST '\n             'product is produced using the mid-infrared 3.95 and 4.05 micron '\n             'channels which are unique to MODIS; the SST derived from these '\n             'measurements is identified as SST4. The SST4 product has lower '\n             'uncertainty, but due to sun glint can only be produced at night. '\n             'MODIS L2P SST data have a 1 km spatial resolution at nadir and '\n             'are stored in 288 five minute granules per day. Full global '\n             'coverage is obtained every two days, with coverage poleward of '\n             '32.3 degree being complete each day. The production of MODIS L2P '\n             'SST files is part of the Group for High Resolution Sea Surface '\n             'Temperature (GHRSST) project, and is a joint collaboration '\n             'between the NASA Jet Propulsion Laboratory (JPL), the NASA Ocean '\n             'Biology Processing Group (OBPG), and the Rosenstiel School of '\n             'Marine and Atmospheric Science (RSMAS). Researchers at RSMAS are '\n             'responsible for SST algorithm development, error statistics and '\n             'quality flagging, while the OBPG, as the NASA ground data '\n             'system, is responsible for the production of daily MODIS ocean '\n             'products. JPL acquires MODIS ocean granules from the OBPG and '\n             'reformats them to the GHRSST L2P netCDF specification with '\n             'complete metadata and ancillary variables, and distributes the '\n             'data as the official Physical Oceanography Data Archive '\n             '(PO.DAAC) for SST.  The R2019.0 supersedes the previous R2014.0 '\n             'datasets which can be found at  '\n             'https://doi.org/10.5067/GHMDT-2PJ02',\n  'time_start': '2000-02-24T00:00:00.000Z',\n  'title': 'GHRSST Level 2P Global Sea Surface Skin Temperature from the '\n           'Moderate Resolution Imaging Spectroradiometer (MODIS) on the NASA '\n           'Terra satellite (GDS2)',\n  'updated': '2019-12-31T19:19:57.627Z',\n  'version_id': '2019.0'}]\n\n\nNow we will try our ICESat-2 dataset to see what id’s are returned for a given dataset name.\n\nresponse = requests.get(url, \n                        params={\n                            'short_name': icesat2_name,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\ncollections = response.json()['feed']['entry']\n# pprint(collections)\n\nfor collection in collections:\n    print(f'{collection[\"id\"]} {\"version:\"}{collection[\"version_id\"]}')\n\nC1705401930-NSIDC_ECS version:003\nC1997321091-NSIDC_ECS version:004\n\n\nTwo separate datasets exist in the CMR. Now let’s take each ID, setting the cloud_hosted parameter to True, to identify which dataset is cloud-hosted:\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1997321091-NSIDC_ECS',\n                            'cloud_hosted': 'False',\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(f'Status code: {response.status_code}')\nprint(f'Hits: {response.headers[\"CMR-Hits\"]}')\ncollections = response.json()['feed']['entry']\npprint(collections)\n\nStatus code: 200\nHits: 1\n[{'archive_center': 'NASA NSIDC DAAC',\n  'associations': {'services': ['S1568899363-NSIDC_ECS',\n                                'S1613689509-NSIDC_ECS',\n                                'S1977894169-NSIDC_ECS',\n                                'S2013502342-NSIDC_ECS'],\n                   'tools': ['TL1950215144-NSIDC_ECS',\n                             'TL1977971361-NSIDC_ECS',\n                             'TL1993837300-NSIDC_ECS',\n                             'TL1952642907-NSIDC_ECS']},\n  'boxes': ['-90 -180 90 180'],\n  'browse_flag': False,\n  'coordinate_system': 'CARTESIAN',\n  'data_center': 'NSIDC_ECS',\n  'dataset_id': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004',\n  'has_formats': True,\n  'has_spatial_subsetting': True,\n  'has_temporal_subsetting': True,\n  'has_transforms': False,\n  'has_variables': True,\n  'id': 'C1997321091-NSIDC_ECS',\n  'links': [{'href': 'https://n5eil01u.ecs.nsidc.org/ATLAS/ATL03.004/',\n             'hreflang': 'en-US',\n             'length': '0.0KB',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n            {'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1997321091-NSIDC_ECS&pg[0][gsk]=-start_date&q=atl03%20v004&tl=1602518008!4!!&m=-9.278314769606354!-105.46875!1!1!0!0%2C2',\n             'hreflang': 'en-US',\n             'length': '0.0KB',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n            {'href': 'http://openaltimetry.org/',\n             'hreflang': 'en-US',\n             'length': '0.0KB',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n            {'href': 'https://nsidc.org/data/data-access-tool/ATL03/versions/4/',\n             'hreflang': 'en-US',\n             'length': '0.0KB',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n            {'href': 'https://doi.org/10.5067/ATLAS/ATL03.004',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n            {'href': 'https://doi.org/10.5067/ATLAS/ATL03.004',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n  'online_access_flag': True,\n  'orbit_parameters': {'inclination_angle': '92.0',\n                       'number_of_orbits': '0.071428571',\n                       'period': '96.8',\n                       'start_circular_latitude': '0.0',\n                       'swath_width': '36.0'},\n  'organizations': ['NASA NSIDC DAAC', 'NASA/GSFC/EOS/ESDIS'],\n  'original_format': 'ISO19115',\n  'platforms': ['ICESat-2'],\n  'processing_level_id': 'Level 2A',\n  'service_features': {'esi': {'has_formats': True,\n                               'has_spatial_subsetting': True,\n                               'has_temporal_subsetting': True,\n                               'has_transforms': False,\n                               'has_variables': True},\n                       'harmony': {'has_formats': False,\n                                   'has_spatial_subsetting': False,\n                                   'has_temporal_subsetting': False,\n                                   'has_transforms': False,\n                                   'has_variables': False},\n                       'opendap': {'has_formats': False,\n                                   'has_spatial_subsetting': False,\n                                   'has_temporal_subsetting': False,\n                                   'has_transforms': False,\n                                   'has_variables': False}},\n  'short_name': 'ATL03',\n  'summary': 'This data set (ATL03) contains height above the WGS 84 ellipsoid '\n             '(ITRF2014 reference frame), latitude, longitude, and time for '\n             'all photons downlinked by the Advanced Topographic Laser '\n             'Altimeter System (ATLAS) instrument on board the Ice, Cloud and '\n             'land Elevation Satellite-2 (ICESat-2) observatory. The ATL03 '\n             'product was designed to be a single source for all photon data '\n             'and ancillary information needed by higher-level ATLAS/ICESat-2 '\n             'products. As such, it also includes spacecraft and instrument '\n             'parameters and ancillary data not explicitly required for ATL03.',\n  'time_start': '2018-10-13T00:00:00.000Z',\n  'title': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004',\n  'version_id': '004'}]\n\n\nWhat happens if we comment out this parameter? Do we see results returned? [TODO: Add instructions on how to comment lines using command slash]\nIt would make more sense to set cloud_hosted to False\nI suggest saving the ATL03 concept ID and the MODIS GHRSST concept ID to a variables\nicesat2_concept_id = 'C1997321091-NSIDC_ECS'\nmodis_concept_id = 'C1940475563-POCLOUD'\nNow we have determined that our Sentinel dataset is provided in the cloud, whereas the ICESat-2 dataset remains “on premise”, residing in a local data center.\n\nSpecify time range and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nurl = f'{CMR_OPS}/{\"granules\"}'\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1997321091-NSIDC_ECS',\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\n\ngranules = response.json()['feed']['entry']\n\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL03_20190622061415_12980304_004_01.h5 1825.3746356964 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.004/2019.06.22/ATL03_20190622061415_12980304_004_01.h5\nATL03_20190622202251_13070304_004_01.h5 3035.5987443924 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.004/2019.06.22/ATL03_20190622202251_13070304_004_01.h5\n\n\n\n\n\nDownload ICESat-2 ATL03 granule\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\nicesat_id = granules[0][\"producer_granule_id\"]\nicesat_url = granules[0]['links'][0]['href']\nYou need Earthdata login credentials to download data from NASA DAACs. These are the credentials you stored in the .netrc file you setup in previous tutorials.\nWe’ll use the netrc package to retrieve your login and password without exposing them.\ninfo = netrc.netrc()\nlogin, account, password = info.authenticators('urs.earthdata.nasa.gov')\nTo retrieve the granule data, we use the requests.get() method, passing Earthdata login credentials as a tuple using the auth keyword.\nr = requests.get(icesat_url, auth=(login, password))\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the mkdir method from the os package.\nos.mkdir('downloads')\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\noutfile = Path('downloads', icesat_id)\nwith open(outfile, 'wb') as f:\n    f.write(r.content)\nCheck to make sure it is downloaded.\nls -l ./downloads\nATL03_20190622061415_12980304_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the height data for ground-track 1 left-beam.\n\noutfile = Path('downloads', icesat_id)\nds = xr.open_dataset(outfile, group='/gt1l/heights')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (delta_time: 19219882, ds_surf_type: 5)\nCoordinates:\n  * delta_time      (delta_time) datetime64[ns] 2019-06-22T06:14:14.882866488...\n    lat_ph          (delta_time) float64 ...\n    lon_ph          (delta_time) float64 ...\nDimensions without coordinates: ds_surf_type\nData variables:\n    dist_ph_across  (delta_time) float32 ...\n    dist_ph_along   (delta_time) float32 ...\n    h_ph            (delta_time) float32 ...\n    pce_mframe_cnt  (delta_time) uint32 ...\n    ph_id_channel   (delta_time) uint8 ...\n    ph_id_count     (delta_time) int8 ...\n    ph_id_pulse     (delta_time) uint8 ...\n    quality_ph      (delta_time) int8 ...\n    signal_conf_ph  (delta_time, ds_surf_type) int8 ...\nAttributes:\n    Description:  Contains arrays of the parameters for each received photon.\n    data_rate:    Data are stored at the photon detection rate.xarray.DatasetDimensions:delta_time: 19219882ds_surf_type: 5Coordinates: (3)delta_time(delta_time)datetime64[ns]2019-06-22T06:14:14.882866488 .....long_name :Elapsed GPS secondsstandard_name :timesource :OperationscontentType :referenceInformationdescription :The transmit time of a given photon, measured in seconds from the ATLAS Standard Data Product Epoch. Note that multiple received photons associated with a single transmit pulse will have the same delta_time. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.array(['2019-06-22T06:14:14.882866488', '2019-06-22T06:14:14.882866488',\n       '2019-06-22T06:14:14.882866488', ..., '2019-06-22T06:19:24.064273608',\n       '2019-06-22T06:19:24.064273608', '2019-06-22T06:19:24.064273608'],\n      dtype='datetime64[ns]')lat_ph(delta_time)float64...long_name :Latitudestandard_name :latitudeunits :degrees_northsource :ATL03g ATBD, Section 3.4valid_min :-90.0valid_max :90.0contentType :modelResultdescription :Latitude of each received photon. Computed from the ECF Cartesian coordinates of the bounce point.[19219882 values with dtype=float64]lon_ph(delta_time)float64...long_name :Longitudestandard_name :longitudeunits :degrees_eastsource :ATL03g ATBD, Section 3.4valid_min :-180.0valid_max :180.0contentType :modelResultdescription :Longitude of each received photon. Computed from the ECF Cartesian coordinates of the bounce point.[19219882 values with dtype=float64]Data variables: (9)dist_ph_across(delta_time)float32...long_name :Distance off RGT.units :meterssource :ATL03 ATBD, Section 3.1contentType :modelResultdescription :Across-track distance projected to the ellipsoid of the received photon from the reference ground track.  This is based on the Along-Track Segment algorithm described in Section 3.1.[19219882 values with dtype=float32]dist_ph_along(delta_time)float32...long_name :Distance from equator crossing.units :meterssource :ATL03 ATBD, Section 3.1contentType :modelResultdescription :Along-track distance in a segment projected to the ellipsoid of the received photon, based on the Along-Track Segment algorithm.  Total along track distance can be found by adding this value to the sum of segment lengths measured from the start of the most recent reference groundtrack.[19219882 values with dtype=float32]h_ph(delta_time)float32...long_name :Photon WGS84 Heightstandard_name :heightunits :meterssource :ATL03g ATBD, Section 3.4contentType :physicalMeasurementdescription :Height of each received photon, relative to the WGS-84 ellipsoid including the geophysical corrections noted in Section 6. Please note that neither the geoid, ocean tide nor the dynamic atmosphere (DAC) corrections are applied to the ellipsoidal heights.[19219882 values with dtype=float32]pce_mframe_cnt(delta_time)uint32...long_name :PCE Major frame counterunits :countssource :Retained from prior a_alt_science_ph packetcontentType :referenceInformationdescription :The major frame counter is read from the digital flow controller in a given PCE card.  The counter identifies individual major frames across diag and science packets. Used as part of the photon ID.[19219882 values with dtype=uint32]ph_id_channel(delta_time)uint8...long_name :Receive channel idunits :1source :Derived as part of Photon IDvalid_min :1valid_max :120contentType :referenceInformationdescription :Channel number assigned for each received photon event. This is part of the photon ID. Values range from 1 to 120 to span all channels and rise/fall edges. Values 1 to 60 are for falling edge; PCE1 (1 to 20), PCE 2 (21 to 40) and PCE3 (41 to 60). Values 61 to 120 are for rising edge; PCE1 (61 to 80), PCE 2 (81 to 100) and PC3 (101 to 120).[19219882 values with dtype=uint8]ph_id_count(delta_time)int8...long_name :photon event counterunits :countssource :Derived as part of Photon IDcontentType :referenceInformationdescription :The photon event counter is part of photon ID and counts from 1 for each channel until reset by laser pulse counter.[19219882 values with dtype=int8]ph_id_pulse(delta_time)uint8...long_name :laser pulse counterunits :countssource :Derived as part of Photon IDcontentType :referenceInformationdescription :The laser pulse counter is part of photon ID and counts from 1 to 200 and is reset for each new major frame.[19219882 values with dtype=uint8]quality_ph(delta_time)int8...long_name :Photon Qualityunits :1source :ATL03 ATBDvalid_min :0valid_max :3contentType :qualityInformationdescription :Indicates the quality of the associated photon. 0=nominal, 1=possible_afterpulse, 2=possible_impulse_response_effect, 3=possible_tep. Use this flag in conjunction with signal_conf_ph to identify those photons that are likely noise or likely signal.flag_meanings :nominal possible_afterpulse possible_impulse_response_effect possible_tepflag_values :[0 1 2 3][19219882 values with dtype=int8]signal_conf_ph(delta_time, ds_surf_type)int8...long_name :Photon Signal Confidenceunits :1source :ATL03 ATBD, Section 5, Confvalid_min :-2valid_max :4contentType :qualityInformationdescription :Confidence level associated with each photon event selected as signal. 0=noise. 1=added to allow for buffer but algorithm classifies as background; 2=low; 3=med; 4=high).  This parameter is a 5xN array where N is the number of photons in the granule, and the 5 rows indicate signal finding for each surface type (in order: land, ocean, sea ice, land ice and inland water). Events not associated with a specific surface type have a confidence level of -1. Events evaluated as TEP returns have a confidence level of -2.flag_meanings :possible_tep not_considered noise buffer low medium highflag_values :[-2 -1  0  1  2  3  4][96099410 values with dtype=int8]Attributes: (2)Description :Contains arrays of the parameters for each received photon.data_rate :Data are stored at the photon detection rate.\n\n\n\n\nDetermine variables of interest: SST, ocean color, chemistry…\n# CMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n# url = f'{CMR_OPS}/{\"collections\"}'\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1940475563-POCLOUD',\n                            'cloud_hosted': 'True',\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\nvariables = response['feed']['entry'][0]['associations']['variables']\noutput_format = \"umm_json\"\nvar_url = \"https://cmr.earthdata.nasa.gov/search/variables\"\nfor i in range(len(variables)):\n    response = requests.get(f\"{var_url}.{output_format}?concept-id={variables[i]}\")\n    response = response.json()\n    # print(response['items'][0]['umm'])\n    if 'Name' in response['items'][0]['umm']: pprint(response['items'][0]['umm']['Name'])\n\n\nPull those variables into xarray “in place”\n\nFirst, we need to determine the granules returned from our time and area of interest\n\ngran_url = f'{CMR_OPS}/{\"granules\"}'\nresponse = requests.get(gran_url, \n                        params={\n                            'concept_id': 'C1940475563-POCLOUD',\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\nprint(response.headers['CMR-Hits'])\n\n200\n11\n\n\n\nmodis_granules_meta = response.json()['feed']['entry']\nfor granule_meta in modis_granules_meta:\n    print(granule_meta['boxes'])\n    print(granule_meta['links'][0]['href'])\n\n['63.232 157.13 88.897 180', '63.232 -180 88.897 -49.902']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622061001-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['68.267 -74.33 89.998 104.028']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MODIS_T-JPL-L2P-v2019.0/20190622093001-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc.md5\n['68.29 -100.13 89.998 79.455']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622111001-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['65.983 -125.049 89.998 54.765']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622125000-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['64.671 -149.702 89.996 30.029']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622142500-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['63.327 -130.186 89.093 27.231']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622143000-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['67.198 -173.585 89.996 5.195']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622160501-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['60.312 -112.093 85.084 -9.865']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MODIS_T-JPL-L2P-v2019.0/20190622161000-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc.md5\n['69.227 170.467 89.994 180', '69.227 -180 89.994 -27.697']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622174501-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc\n['67.177 144.606 89.998 180', '67.177 -180 89.998 -53.628']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MODIS_T-JPL-L2P-v2019.0/20190622192501-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc.md5\n['63.354 -65.91 89.07 91.026']\nhttps://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MODIS_T-JPL-L2P-v2019.0/20190622210001-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc.md5\n\n\n\nhttps_link = modis_granules_meta[0]['links'][0]['href']\ns3_link = https_link.replace('https://archive.podaac.earthdata.nasa.gov/','s3://')\ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_T-JPL-L2P-v2019.0/20190622061001-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0.nc'\n\n\n\n\n\nGet S3 credentials\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\nOpen a s3 file\nf = s3_fs.open(s3_link)\nds = xr.open_dataset(f, engine='h5netcdf')\n\n\nUse geolocation of ICESat-2 to define the single transect used to pull coincident ocean data out from array\n\n\nCreate a plot of the single transect of gridded data\n(bonus: time series) - describe what this means to egress out of the cloud versus pulling the original data down (benefit to processing in the cloud)"
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#download-modis-ghrsst-data-from-cloud",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#download-modis-ghrsst-data-from-cloud",
    "title": "",
    "section": "Download MODIS GHRSST data from Cloud",
    "text": "from shapely.geometry import box\nmap_proj = ccrs.PlateCarree()\ndef bbox_geometry(boxs, t_crs=ccrs.NorthPolarStereo()):\n    '''Generates a shapely.geometry.box object from boxes metadata'''\n    lat_min, lon_min, lat_max, lon_max = [float(v) for v in bbox.split()]\n    x_min, y_min = t_crs.transform_point(lon_min, lat_min, ccrs.PlateCarree())\n    x_max, y_max = t_crs.transform_point(lon_max, lat_max, ccrs.PlateCarree())\n    return box(x_min, y_min, x_max, y_max)\n\nbbox_features = []\nfor granule in modis_granules_meta:\n    for bbox in granule['boxes']:\n        bbox_features.append(bbox_geometry(bbox))\n\nfig = plt.figure(figsize=(7,7))\nax = fig.add_subplot(projection=ccrs.NorthPolarStereo())\nax.set_extent([-180.,180.,60.,90.], ccrs.PlateCarree())\nax.coastlines()\nax.add_geometries([bbox_features[0]], crs=map_proj, alpha=0.3)\n\n<cartopy.mpl.feature_artist.FeatureArtist at 0x7efcbee4b580>"
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#resources-optional",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#resources-optional",
    "title": "",
    "section": "Resources (optional)",
    "text": ""
  },
  {
    "objectID": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#conclusion",
    "href": "tutorials/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": ""
  },
  {
    "objectID": "tutorials/NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials/NASA_Earthdata_Authentication.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is need to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "",
    "section": "Import Required Packages",
    "text": "from netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo password \\{} >> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo password \\{} >> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\nEnter NASA Earthdata Login Username:  ·······\n\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/\n\ntotal 928\ndrwxr-xr-x 14 jovyan jovyan   6144 Nov  4 19:11 .\ndrwxr-xr-x  1 root   root     4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan   7738 Nov  4 17:43 .bash_history\ndrwxr-xr-x  8 jovyan jovyan   6144 Nov  2 17:49 .cache\ndrwxrwsr-x  2 jovyan jovyan   6144 Jun 11 16:31 .conda\ndrwxr-xr-x  4 jovyan jovyan   6144 Nov  2 17:37 .config\ndrwxr-xr-x  2 jovyan jovyan   6144 Jun 11 16:31 .empty\n-rw-r--r--  1 jovyan jovyan     52 Oct 19 20:19 .gitconfig\ndrwxr-xr-x  2 jovyan jovyan   6144 Nov  3 12:58 .ipynb_checkpoints\ndrwxr-xr-x  5 jovyan jovyan   6144 May 13 19:04 .ipython\ndrwxr-xr-x  3 jovyan jovyan   6144 Jun 11 13:09 .jupyter\n-rw-r--r--  1 jovyan jovyan    183 Oct 28 21:32 .jupyter-server-log.txt\ndrwxr-xr-x  3 jovyan jovyan   6144 May 13 19:04 .local\n-rw-------  1 jovyan jovyan     67 Nov  4 19:11 .netrc\ndrwxr-xr-x 10 jovyan jovyan   6144 Nov  4 17:29 2021-Cloud-Hackathon\n-rw-r--r--  1 jovyan jovyan    119 Nov  3 18:07 additional_packages.txt\n-rw-r--r--  1 jovyan jovyan    131 Nov  4 15:55 cookies.txt\ndrwxr-xr-x  5 jovyan jovyan   6144 Nov  4 15:03 corn\n-rw-r--r--  1 jovyan jovyan  11709 Nov  4 14:48 environment.yml\n-rw-r--r--  1 jovyan jovyan 847997 Nov  3 13:35 s6a_atrack_altimetry_demo.ipynb\ndrwxr-xr-x  2 root   root     6144 May 11 19:41 shared\ndrwxr-xr-x  2 root   root     6144 May 11 19:41 shared-readwrite"
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#timing",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#timing",
    "title": "",
    "section": "Timing",
    "text": "Exercise: 30 min"
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#summary",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remove assets.\n\nFour STAC Specifications\nSTAC Item (aka Granule)\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "title": "",
    "section": "What you will learn from this tutorial",
    "text": "how to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets\n\nThis exercise can be found in the 2021 Cloud Hackathon Book"
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#resources",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#resources",
    "title": "",
    "section": "Resources",
    "text": "STAC Specification Webpage\nSTAC API Documentation\nCMR-STAC API Github\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nGeopandas\nHLS Overview"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#summary",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direction in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Eartdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieve data is very quickly\nNo need to download data! Work with data in a more efficient manner\nIncreased capacity to do parallel processing\nYou are working completely with the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nAccess only works within AWS us-west-2 region\nNeed an AWS S3 “token” to access S3 Bucket\nToken expires after 1 hour\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\nS3 on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tool that allow you to do that, not discussed in this tutorial\n\n\n\n\nObjective\n\nConfigure our notebook environment and retrieve temporary S3 credentials for in-region direct S3 bucket access\nAccess a single HLS file\nAccess and clip an HLS file to a region of interest\nCreate an HLS time series data array\n\nLet’s get started!\n\n\n\nImport Required Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport subprocess\nimport requests\nimport boto3\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geopandas\nimport pyproj\nfrom pyproj import Proj\nfrom shapely.ops import transform\nimport geoviews as gv\nfrom cartopy import crs\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#configure-local-environment-and-get-temporary-credentials",
    "title": "",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "To perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ndef get_temp_creds():\n    temp_creds_url = s3_cred_endpoint\n    return requests.get(temp_creds_url).json()\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7fe992047490>"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-s3-links",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-s3-links",
    "title": "",
    "section": "Read in S3 Links",
    "text": "In the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\n\nfor f in os.listdir('./data'):\n    print(f)\n\nHTTPS_T13TGF_B02_Links.txt\nS3_T13TGF_B05_Links.txt\nne_w_agfields.geojson\nHTTPS_T13TGF_Fmask_Links.txt\n.ipynb_checkpoints\nS3_T13TGF_B8A_Links.txt\nHTTPS_T13TGF_B04_Links.txt\nS3_T13TGF_B04_Links.txt\nS3_T13TGF_Fmask_Links.txt\nHTTPS_T13TGF_B8A_Links.txt\ndataset-diagram.png\nHTTPS_T13TGF_B05_Links.txt\nS3_T13TGF_B02_Links.txt\n\n\nWe will safe our list of links and a single link as Python objects for use later.\n\ns3_links = open('./data/S3_T13TGF_B04_Links.txt').read().splitlines()\ns3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\ns3_link = s3_links[0]\ns3_link\n\n's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif'"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-a-single-hls-file",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-a-single-hls-file",
    "title": "",
    "section": "Read in a single HLS file",
    "text": "We’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n\nhls_da = rioxarray.open_rasterio(s3_link, chuncks=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nhls_da = hls_da.squeeze('band', drop=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\nPlot the HLS S3 object\n\nhls_da.hvplot.image(x='x', y='y', cmap='fire', rasterize=True, width=800, height=600, colorbar=True)    # colormaps -> https://holoviews.org/user_guide/Colormaps.html\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can print out the data value as a numpy array by typing .values\n\nhls_da.values\n\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\n\n\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n\nhls_da_data = hls_da.load()\nhls_da_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660-9999 -9999 -9999 -9999 -9999 -9999 ... 1676 1486 1112 954 1127 1133array([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\ndel(hls_da_data)"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-a-single-hls-file",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-a-single-hls-file",
    "title": "",
    "section": "Read in and clip a single HLS file",
    "text": "To clip the HLS file, our feature representing our region of interest must be in the same coordinate reference system (CRS) or projection coordinate system as the HLS file. The map projection for our HLS file is Universal Transverse Mercator (UTM) zone 13N. Our feature is mapped to WGS84 geographic coordinate system grid space. We need to transform the geographic coordinate reference system (CRS) of our feature to the UTM projected coordinate system (i.e., UTM Zone 13N)\n\nRead in our geojson file and transform its CRS\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nLet’s take a look at the bounding coordinate values.\n\nfield_shape = field.geometry[0]\nfield_shape.bounds\n\n(-101.67271614074707,\n 41.04754380304359,\n -101.65344715118408,\n 41.06213891056728)\n\n\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner (-101.67271614074707, 41.04754380304359) and upper right corner (-101.65344715118408, 41.06213891056728) respectively.\n\n\nGet the projection information from the HLS file\n\nhls_proj = hls_da.rio.crs\nhls_proj\n\nCRS.from_epsg(32613)\n\n\n\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\nproject = pyproj.Transformer.from_proj(geo_CRS, hls_proj)                    # Set up the transformation\n\nfsUTM = transform(project.transform, field_shape)\nfsUTM.bounds\n\n(779588.4994601272, 4549370.366049466, 781270.1479326887, 4551052.979639321)\n\n\nThe coordinates for our feature have now been converted to UTM Zone 13N whether meters is the designated unit. Note the difference in the values between field_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our HLS file to our region of insterest!\n\n\nAccess and clip the HLS file\nWe can now use our transformed ROI bounding box to clip the HLS S3 object we accessed before. We’ll use the `rio.clip\nhls_da_clip = rioxarray.open_rasterio(s3_link, chunks=True).squeeze('band', drop=True).rio.clip([fsUTM])\n\nhls_da_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 56, x: 56)>\ndask.array<astype, shape=(56, 56), dtype=int16, chunksize=(56, 56), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Red\n    _FillValue:    -9999xarray.DataArrayy: 56x: 56dask.array<chunksize=(56, 56), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         6.12 kiB \n                         6.12 kiB \n                    \n                    \n                    \n                         Shape \n                         (56, 56) \n                         (56, 56) \n                    \n                    \n                         Count \n                         12 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  56\n  56\n\n        \n    \nCoordinates: (3)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (4)scale_factor :0.0001add_offset :0.0long_name :Red_FillValue :-9999\n\n\n\nhls_da_clip.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32613', cmap='fire', rasterize=True, width=800, height=600, colorbar=True)\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-an-hls-time-series",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-an-hls-time-series",
    "title": "",
    "section": "Read in and clip an HLS time series",
    "text": "Now we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n\ns3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\ndef time_index_from_filenames(file_links):\n    '''\n    Helper function to create a pandas DatetimeIndex\n    '''\n    return [datetime.strptime(f.split('.')[-5], '%Y%jT%H%M%S') for f in file_links]\ntime = xr.Variable('time', time_index_from_filenames(s3_links))\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\nchunks=dict(band=1, x=1024, y=1024)\nNow, we will create our time series.\n\nhls_ts_da = xr.concat([rioxarray.open_rasterio(f, chunks=chunks).squeeze('band', drop=True) for f in s3_links], dim=time)\nhls_ts_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<concatenate, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         485.45 MiB \n                         2.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         1235 Tasks \n                         304 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nCoordinates: (4)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:30:21.000000000', '2021-05-20T17:28:59.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nSince we used the chunks parameter while reading the data, the hls_ts_da object is read into memory. To do that we’ll use the load() function. But, before that, we’ll clip the hls_ts_da object to our roi using our transformed roi coordinates.\n\nhls_ts_da_clip = hls_ts_da.rio.clip([fsUTM]).load()\nhls_ts_da_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 56, x: 56)>\narray([[[-9999, -9999, -9999, ...,   980, -9999, -9999],\n        [-9999, -9999, -9999, ...,   287, -9999, -9999],\n        [ 1573,  1692,  1708, ...,   410, -9999, -9999],\n        ...,\n        [-9999, -9999,  1165, ...,  1808,  1869,  1906],\n        [-9999, -9999,   989, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1085, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   860, -9999, -9999],\n        [-9999, -9999, -9999, ...,   496, -9999, -9999],\n        [ 2681,  2773,  2496, ...,   550, -9999, -9999],\n        ...,\n        [-9999, -9999,  3847, ...,  1997,  1914,  1831],\n        [-9999, -9999,  4062, ..., -9999, -9999, -9999],\n        [-9999, -9999,  4313, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   808, -9999, -9999],\n        [-9999, -9999, -9999, ...,   230, -9999, -9999],\n        [ 1802,  1828,  1863, ...,   306, -9999, -9999],\n        ...,\n...\n        ...,\n        [-9999, -9999,  1124, ...,   804,   934,  1008],\n        [-9999, -9999,  1003, ..., -9999, -9999, -9999],\n        [-9999, -9999,   904, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1313, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1327, -9999, -9999],\n        [ 1091,  1094,  1179, ...,  1223, -9999, -9999],\n        ...,\n        [-9999, -9999,  1145, ...,  1005,  1097,  1197],\n        [-9999, -9999,  1037, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1114, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1272, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1231, -9999, -9999],\n        [ 1086,  1105,  1193, ...,  1205, -9999, -9999],\n        ...,\n        [-9999, -9999,  1045, ...,  1049,  1142,  1219],\n        [-9999, -9999,   926, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1076, ..., -9999, -9999, -9999]]], dtype=int16)\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Red\n    _FillValue:    -9999xarray.DataArraytime: 19y: 56x: 56-9999 -9999 -9999 -9999 -9999 -9999 ... -9999 -9999 -9999 -9999 -9999array([[[-9999, -9999, -9999, ...,   980, -9999, -9999],\n        [-9999, -9999, -9999, ...,   287, -9999, -9999],\n        [ 1573,  1692,  1708, ...,   410, -9999, -9999],\n        ...,\n        [-9999, -9999,  1165, ...,  1808,  1869,  1906],\n        [-9999, -9999,   989, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1085, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   860, -9999, -9999],\n        [-9999, -9999, -9999, ...,   496, -9999, -9999],\n        [ 2681,  2773,  2496, ...,   550, -9999, -9999],\n        ...,\n        [-9999, -9999,  3847, ...,  1997,  1914,  1831],\n        [-9999, -9999,  4062, ..., -9999, -9999, -9999],\n        [-9999, -9999,  4313, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   808, -9999, -9999],\n        [-9999, -9999, -9999, ...,   230, -9999, -9999],\n        [ 1802,  1828,  1863, ...,   306, -9999, -9999],\n        ...,\n...\n        ...,\n        [-9999, -9999,  1124, ...,   804,   934,  1008],\n        [-9999, -9999,  1003, ..., -9999, -9999, -9999],\n        [-9999, -9999,   904, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1313, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1327, -9999, -9999],\n        [ 1091,  1094,  1179, ...,  1223, -9999, -9999],\n        ...,\n        [-9999, -9999,  1145, ...,  1005,  1097,  1197],\n        [-9999, -9999,  1037, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1114, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1272, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1231, -9999, -9999],\n        [ 1086,  1105,  1193, ...,  1205, -9999, -9999],\n        ...,\n        [-9999, -9999,  1045, ...,  1049,  1142,  1219],\n        [-9999, -9999,   926, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1076, ..., -9999, -9999, -9999]]], dtype=int16)Coordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:30:21.000000000', '2021-05-20T17:28:59.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (4)scale_factor :0.0001add_offset :0.0long_name :Red_FillValue :-9999\n\n\nNow, we’ll see what we have. Use hvplot to plot the clipped time series\n\nhls_ts_da_clip.hvplot.image(x='x', y='y', width=800, height=600, colorbar=True, cmap='fire').opts(clim=(hls_ts_da_clip.values.min(), hls_ts_da_clip.values.max()))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n# Exit our context\nrio_env.__exit__()"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#resourses",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#resourses",
    "title": "",
    "section": "Resourses",
    "text": "Build time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse"
  },
  {
    "objectID": "tutorials/Introduction_to_xarray.html#why-do-we-need-xarray",
    "href": "tutorials/Introduction_to_xarray.html#why-do-we-need-xarray",
    "title": "",
    "section": "Why do we need xarray?",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/Introduction_to_xarray.html#what-is-xarray",
    "href": "tutorials/Introduction_to_xarray.html#what-is-xarray",
    "title": "",
    "section": "What is xarray",
    "text": "xarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with builtin methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials/Introduction_to_xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/Introduction_to_xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "",
    "section": "What you will learn from this tutorial",
    "text": "In this tutorial you will learn how to: - load a netcdf file into xarray - interrogate the Dataset and understand the difference between DataArray and Dataset - subset a Dataset - calculate annual and monthly mean fields - calculate a time series of zonal means - plot these results\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\n\n<xarray.core.options.set_options at 0x7f13904dd4c0>\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\nds = xr.tutorial.open_dataset(\"air_temperature\")\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 2920)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\nds['air'] = ds.air - 273.15\nThis approach can also be used to add new variables\nds['air_kelvin'] = ds.air + 273.15\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\nds['air'].attrs['units'] = 'degC'\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "tutorials/Introduction_to_xarray.html#subsetting-and-indexing",
    "href": "tutorials/Introduction_to_xarray.html#subsetting-and-indexing",
    "title": "",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (lat: 25, lon: 53)>\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following.\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 4)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)>\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nPay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 75 N, the last value is 15 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)>\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283, -104.98785545855408). xarray can handle this! If we just want data from the nearest grid point, we can use sel.\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920)>\narray([ -9.25    , -14.950012, -18.649994, ..., -28.259995, -26.160004,\n       -15.359985], dtype=float32)\nCoordinates:\n    lat      float32 40.0\n    lon      float32 255.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-9.25 -14.95 -18.65 -11.36 -8.95 ... -22.76 -28.26 -26.16 -15.36array([ -9.25    , -14.950012, -18.649994, ..., -28.259995, -26.160004,\n       -15.359985], dtype=float32)Coordinates: (3)lat()float3240.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(40., dtype=float32)lon()float32255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255., dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920)>\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "tutorials/Introduction_to_xarray.html#analysis",
    "href": "tutorials/Introduction_to_xarray.html#analysis",
    "title": "",
    "section": "Analysis",
    "text": "As a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (lat, lon) float32 -12.77 -12.97 -13.26 ... 24.19 24.13 24.16\n    air_kelvin  (lat, lon) float32 260.4 260.2 259.9 259.5 ... 297.3 297.3 297.3\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25lon: 53Coordinates: (2)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(lat, lon)float32-12.77 -12.97 ... 24.13 24.16long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[-12.773591 , -12.9669895, -13.263404 , ..., -22.334131 ,\n        -21.211912 , -19.711926 ],\n       [-10.415646 , -10.356057 , -10.400703 , ..., -23.394167 ,\n        -21.564285 , -18.790735 ],\n       [ -8.381272 ,  -8.82274  ,  -9.088349 , ..., -22.542147 ,\n        -19.566486 , -15.4343605],\n       ...,\n       [ 24.5001   ,  23.80355  ,  23.47953  , ...,  23.661144 ,\n         23.138168 ,  22.66664  ],\n       [ 24.979464 ,  24.787296 ,  24.320633 , ...,  23.709766 ,\n         23.627226 ,  23.294033 ],\n       [ 25.216433 ,  25.23599  ,  24.964426 , ...,  24.188444 ,\n         24.131657 ,  24.155333 ]], dtype=float32)air_kelvin(lat, lon)float32260.4 260.2 259.9 ... 297.3 297.3long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[260.37564, 260.1826 , 259.88593, ..., 250.81511, 251.93733,\n        253.43741],\n       [262.7337 , 262.7936 , 262.7489 , ..., 249.75496, 251.5852 ,\n        254.35849],\n       [264.7681 , 264.3271 , 264.0614 , ..., 250.60707, 253.58247,\n        257.71475],\n       ...,\n       [297.64932, 296.95294, 296.62912, ..., 296.81033, 296.28793,\n        295.81622],\n       [298.1287 , 297.93646, 297.47006, ..., 296.8591 , 296.77686,\n        296.44348],\n       [298.36594, 298.38593, 298.11386, ..., 297.33777, 297.28104,\n        297.30502]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat) float32 -31.1 -31.19 -29.42 ... 23.84 24.8 25.62\n    air_kelvin  (time, lat) float32 242.0 242.0 243.7 ... 297.0 297.9 298.8\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920Coordinates: (2)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat)float32-31.1 -31.19 -29.42 ... 24.8 25.62long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[-31.103964, -31.191317, -29.423584, ...,  22.508688,  23.559252,\n         24.767178],\n       [-31.43698 , -31.509811, -29.74585 , ...,  22.161512,  23.242266,\n         24.468681],\n       [-31.166601, -32.06151 , -29.902636, ...,  22.002268,  22.997171,\n         24.162457],\n       ...,\n       [-28.74113 , -30.04868 , -26.656223, ...,  23.294722,  24.121134,\n         25.104156],\n       [-28.39585 , -30.10151 , -26.937357, ...,  22.990942,  24.009811,\n         25.113588],\n       [-27.663773, -30.195848, -27.69396 , ...,  23.841888,  24.796608,\n         25.621134]], dtype=float32)air_kelvin(time, lat)float32242.0 242.0 243.7 ... 297.9 298.8long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[242.046  , 241.95865, 243.7264 , ..., 295.6587 , 296.7093 ,\n        297.91714],\n       [241.71301, 241.64018, 243.40411, ..., 295.3115 , 296.39224,\n        297.61868],\n       [241.9834 , 241.08849, 243.24734, ..., 295.15228, 296.14716,\n        297.31244],\n       ...,\n       [244.40886, 243.10132, 246.49377, ..., 296.44473, 297.27115,\n        298.25415],\n       [244.75415, 243.0485 , 246.21265, ..., 296.14093, 297.1598 ,\n        298.2636 ],\n       [245.48622, 242.95416, 245.45602, ..., 296.99188, 297.94662,\n        298.77115]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (lat, lon) float32 11.75 11.69 11.7 11.78 ... 1.754 1.848 1.925\n    air_kelvin  (lat, lon) float32 11.75 11.69 11.7 11.78 ... 1.754 1.848 1.925\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25lon: 53Coordinates: (2)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(lat, lon)float3211.75 11.69 11.7 ... 1.848 1.925long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[11.747101 , 11.694012 , 11.6985235, ..., 10.668415 , 10.772111 ,\n        10.894435 ],\n       [11.657323 , 11.69503  , 11.782638 , ..., 10.910919 , 11.066576 ,\n        11.024149 ],\n       [12.43451  , 12.759326 , 13.078082 , ..., 11.306735 , 10.977766 ,\n        10.328541 ],\n       ...,\n       [ 1.6068684,  1.5553867,  1.4661906, ...,  1.77796  ,  1.862355 ,\n         1.9495418],\n       [ 1.3046906,  1.2699376,  1.2530344, ...,  1.7605774,  1.9115199,\n         2.0385334],\n       [ 1.0361689,  1.0487068,  1.037612 , ...,  1.7537667,  1.8484358,\n         1.9254034]], dtype=float32)air_kelvin(lat, lon)float3211.75 11.69 11.7 ... 1.848 1.925long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[11.747092 , 11.693981 , 11.698499 , ..., 10.668427 , 10.772092 ,\n        10.894412 ],\n       [11.657324 , 11.69504  , 11.7826395, ..., 10.910913 , 11.066578 ,\n        11.024166 ],\n       [12.434489 , 12.759332 , 13.078072 , ..., 11.306724 , 10.977766 ,\n        10.328547 ],\n       ...,\n       [ 1.606873 ,  1.555383 ,  1.4661945, ...,  1.777959 ,  1.8623606,\n         1.949547 ],\n       [ 1.3046896,  1.26994  ,  1.2530342, ...,  1.7605747,  1.9115177,\n         2.0385323],\n       [ 1.0361763,  1.0487102,  1.0376098, ...,  1.7537634,  1.8484302,\n         1.925401 ]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()"
  },
  {
    "objectID": "tutorials/Introduction_to_xarray.html#plot-results",
    "href": "tutorials/Introduction_to_xarray.html#plot-results",
    "title": "",
    "section": "Plot results",
    "text": "ds_clim.air.sel(month=10).plot()\n\n<matplotlib.collections.QuadMesh at 0x7f9cef458250>"
  },
  {
    "objectID": "tutorials/getting-set-up.html#introduction",
    "href": "tutorials/getting-set-up.html#introduction",
    "title": "Getting set up and connected",
    "section": "Introduction",
    "text": "text here"
  },
  {
    "objectID": "tutorials/getting-set-up.html#how-do-i-get-the-tutorial-repository-into-the-hub",
    "href": "tutorials/getting-set-up.html#how-do-i-get-the-tutorial-repository-into-the-hub",
    "title": "Getting set up and connected",
    "section": "How do I get the tutorial repository into the Hub?",
    "text": ""
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#summary",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direction in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Eartdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieve data is very quickly\n\nNo need to download data! Work with data in a more efficient manner\n\nIncreased capacity to do parallel processing\n\nYou are working completely with the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nIf you’re workflow is in the cloud, choose S3 over HTTPS\n\nAccess only works within AWS us-west-2 region\n\nNeed an AWS S3 “token” to access S3 Bucket\n\nToken expires after 1 hour\n\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\n\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\n\nS3 on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\n\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tool that allow you to do that, not discussed in this tutorial"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#what-you-will-learn-from-this-tutorial",
    "title": "",
    "section": "What you will learn from this tutorial",
    "text": "how to retrieve temporary S3 credentials for in-region direct S3 bucket access\n\nhow to configure our notebook environment for in-region direct S3 bucket access\n\nhow to access a single HLS file via in-region direct S3 bucket access\n\nhow to create an HLS time series data array from cloud assets via in-region direct S3 bucket access\n\nhow to plot results\n\nThis exercise can be found in the 2021 Cloud Hackathon Book"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#import-required-packages",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#import-required-packages",
    "title": "",
    "section": "Import Required Packages",
    "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport subprocess\nimport requests\nimport boto3\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geopandas\nimport pyproj\nfrom pyproj import Proj\nfrom shapely.ops import transform\nimport geoviews as gv\nfrom cartopy import crs\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#configure-local-environment-and-get-temporary-credentials",
    "title": "",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "To perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ndef get_temp_creds():\n    temp_creds_url = s3_cred_endpoint\n    return requests.get(temp_creds_url).json()\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f1e4336f460>"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#read-in-s3-links",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#read-in-s3-links",
    "title": "",
    "section": "Read in S3 Links",
    "text": "In the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\n\n[f for f in os.listdir('./data') if '.txt' in f]\n\n['HTTPS_T13TGF_B02_Links.txt',\n 'S3_T13TGF_B05_Links.txt',\n 'HTTPS_T13TGF_Fmask_Links.txt',\n 'S3_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B04_Links.txt',\n 'S3_T13TGF_B04_Links.txt',\n 'S3_T13TGF_Fmask_Links.txt',\n 'HTTPS_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B05_Links.txt',\n 'S3_T13TGF_B02_Links.txt']\n\n\nWe will safe our list of links and a single link as Python objects for use later.\n\ns3_links = open('./data/S3_T13TGF_B04_Links.txt').read().splitlines()\ns3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\ns3_link = s3_links[0]\ns3_link\n\n's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif'"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#read-in-a-single-hls-file",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#read-in-a-single-hls-file",
    "title": "",
    "section": "Read in a single HLS file",
    "text": "We’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n\nhls_da = rioxarray.open_rasterio(s3_link, chuncks=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nhls_da = hls_da.squeeze('band', drop=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\nPlot the HLS S3 object\n\nhls_da.hvplot.image(x='x', y='y', cmap='fire', rasterize=True, width=800, height=600, colorbar=True)    # colormaps -> https://holoviews.org/user_guide/Colormaps.html\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can print out the data value as a numpy array by typing .values\n\nhls_da.values\n\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\n\n\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n\nhls_da_data = hls_da.load()\nhls_da_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660-9999 -9999 -9999 -9999 -9999 -9999 ... 1676 1486 1112 954 1127 1133array([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\ndel(hls_da_data)"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#read-in-hls-as-a-time-series",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#read-in-hls-as-a-time-series",
    "title": "",
    "section": "Read in HLS as a time series",
    "text": "Now we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n\ns3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\ndef time_index_from_filenames(file_links):\n    '''\n    Helper function to create a pandas DatetimeIndex\n    '''\n    return [datetime.strptime(f.split('.')[-5], '%Y%jT%H%M%S') for f in file_links]\ntime = xr.Variable('time', time_index_from_filenames(s3_links))\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\nchunks=dict(band=1, x=512, y=512)\nNow, we will create our time series.\n\nhls_ts_da = xr.concat([rioxarray.open_rasterio(f, chunks=chunks).squeeze('band', drop=True) for f in s3_links], dim=time)\nhls_ts_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<concatenate, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 512, 512), chunktype=numpy.ndarray>\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 512, 512), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         485.45 MiB \n                         512.00 kiB \n                    \n                    \n                    \n                         Shape \n                         (19, 3660, 3660) \n                         (1, 512, 512) \n                    \n                    \n                         Count \n                         4883 Tasks \n                         1216 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n        \n    \nCoordinates: (4)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:30:21.000000000', '2021-05-20T17:28:59.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nSince we used the chunks parameter while reading the data, the hls_ts_da object is not read into memory yet. To do that we’ll use the load() function.\nNow, we’ll see what we have. Use hvplot to plot our time series\nhls_ts_da_data = hls_ts_da.load()\n\nhls_ts_da_data.hvplot.image(x='x', y='y', rasterize=True, width=800, height=600, colorbar=True, cmap='fire').opts(clim=(hls_ts_da_data.values.min(), hls_ts_da_data.values.max()))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n# Exit our context\nrio_env.__exit__()"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#concluding-remarks",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#concluding-remarks",
    "title": "",
    "section": "Concluding Remarks",
    "text": "The above exercise demonstrated how to perform in-region direct S3 bucket access for HLS cloud data assets. HLS cloud data assets are stored as Cloud Optimized GeoTIFFs, a format that has been the benifactor of data discovery and access advancements within the Python ecosystem. Knowing what the data storage format is (e.g., COG, netcdf4, or zarr store) and/or what data access protocol you’re using is critical in determining what Python data access method you will use. For COG data, rioxarray package is often prefered due to is ability to bring the geospatial data format into an xarray object. For netcdf4 files, the standard xarray package incombination with s3fs allow users to perform in-region direct access reads into an xarray object. Finally, if you are using OPeNDAP to connect to data, specialized packages like pydap have been integrated into xarray for streamline access directly to an xarray object."
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access.html#resourses",
    "href": "tutorials/Data_Access__Direct_S3_Access.html#resourses",
    "title": "",
    "section": "Resourses",
    "text": "Build time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse"
  },
  {
    "objectID": "tutorials/Data_Access__Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "href": "tutorials/Data_Access__Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "title": "",
    "section": "Using the Harmony-Py library to access customized data from NASA Earthdata",
    "text": ""
  },
  {
    "objectID": "tutorials/Data_Access__Harmony_Subsetting.html#summary",
    "href": "tutorials/Data_Access__Harmony_Subsetting.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Harmony allows you to seamlessly analyze Earth observation data from different NASA data centers… https://harmony.earthdata.nasa.gov/\n\nBenefits\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow users to request only the data they want, in the format and projection they want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n\nHarmony-Py is a Python library for integrating with NASA’s Harmony Services.\nHarmony-Py provides a Python alternative to directly using Harmony’s RESTful API. It handles NASA Earthdata Login (EDL) authentication and optionally integrates with the CMR Python Wrapper by accepting collection results as a request parameter. It’s convenient for scientists who wish to use Harmony from Jupyter notebooks as well as machine-to-machine communication with larger Python applications.\n\n\nObjectives\n\nPractice skills learned from intro to CMR tutorial to discover what access and service options exist for a given data set"
  },
  {
    "objectID": "tutorials/Data_Access__Harmony_Subsetting.html#import-packages",
    "href": "tutorials/Data_Access__Harmony_Subsetting.html#import-packages",
    "title": "",
    "section": "Import Packages",
    "text": "from harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nimport requests\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nimport xarray as xr"
  },
  {
    "objectID": "tutorials/Data_Access__Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "href": "tutorials/Data_Access__Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "title": "",
    "section": "Discover service options for a given data set",
    "text": "First, what do we mean by a “service”? [TODO] Describe how we define services and their benefits, and how not all datasets have services on them due to level of support, etc….\nLet’s see what the collection metadata tells us\n\nBuilding off of CMR introduction tutorial:\ncmr_search_url = 'https://cmr.earthdata.nasa.gov/search'\nWe want to search by collection to inspect the access and service options that exist:\ncmr_collection_url = f'{cmr_search_url}/{\"collections\"}'\nIn the CMR introduction tutorial, we explored cloud-hosted collections from different DAAC providers, and identified the CMR concept-id for a given data set id (also referred to as a short_name).\nHere we are jumping ahead and already know the concept_id we are interested in, by browsing cloud-hosted datasets from PO.DAAC in Earthdata Search: https://search.earthdata.nasa.gov/portal/podaac-cloud/search.\nWe are going to focus on MODIS_A-JPL-L2P-v2019.0: GHRSST Level 2P Global Sea Surface Skin Temperature from the Moderate Resolution Imaging Spectroradiometer (MODIS) on the NASA Aqua satellite (GDS2). Let’s first save this as a variable that we can use later on once we request data from Harmony.\nshort_name= 'MODIS_A-JPL-L2P-v2019.0'\nconcept_id = 'C1940473819-POCLOUD'\nWe will view the top-level metadata for this collection to see what additional service and variable metadata exist.\nresponse = requests.get(cmr_collection_url, \n                        params={\n                            'concept_id': concept_id,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\nPrint the response:\n\npprint(response)\n\n{'feed': {'entry': [{'archive_center': 'NASA/JPL/PODAAC',\n                     'associations': {'services': ['S1962070864-POCLOUD',\n                                                   'S2004184019-POCLOUD'],\n                                      'tools': ['TL2108419875-POCLOUD',\n                                                'TL2092786348-POCLOUD'],\n                                      'variables': ['V1997812737-POCLOUD',\n                                                    'V1997812697-POCLOUD',\n                                                    'V2112014688-POCLOUD',\n                                                    'V1997812756-POCLOUD',\n                                                    'V1997812688-POCLOUD',\n                                                    'V1997812670-POCLOUD',\n                                                    'V1997812724-POCLOUD',\n                                                    'V2112014684-POCLOUD',\n                                                    'V1997812701-POCLOUD',\n                                                    'V1997812681-POCLOUD',\n                                                    'V2112014686-POCLOUD',\n                                                    'V1997812663-POCLOUD',\n                                                    'V1997812676-POCLOUD',\n                                                    'V1997812744-POCLOUD',\n                                                    'V1997812714-POCLOUD']},\n                     'boxes': ['-90 -180 90 180'],\n                     'browse_flag': True,\n                     'collection_data_type': 'SCIENCE_QUALITY',\n                     'coordinate_system': 'CARTESIAN',\n                     'data_center': 'POCLOUD',\n                     'dataset_id': 'GHRSST Level 2P Global Sea Surface Skin '\n                                   'Temperature from the Moderate Resolution '\n                                   'Imaging Spectroradiometer (MODIS) on the '\n                                   'NASA Aqua satellite (GDS2)',\n                     'has_formats': True,\n                     'has_spatial_subsetting': True,\n                     'has_temporal_subsetting': True,\n                     'has_transforms': False,\n                     'has_variables': True,\n                     'id': 'C1940473819-POCLOUD',\n                     'links': [{'href': 'https://podaac.jpl.nasa.gov/Podaac/thumbnails/MODIS_A-JPL-L2P-v2019.0.jpg',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n                               {'href': 'https://github.com/podaac/data-readers',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://ghrsst.jpl.nasa.gov',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst/flag/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/reprocessing/r2019/sst/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst4/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://modis.gsfc.nasa.gov/data/atbd/atbd_mod25.pdf',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'http://www.ghrsst.org',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://podaac.jpl.nasa.gov/forum/viewforum.php?f=18&sid=e2d67e5a01815fc6e39fcd2087ed8bc8',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://podaac.jpl.nasa.gov/CitingPODAAC',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C1940473819-POCLOUD',\n                                'hreflang': 'en-US',\n                                'length': '75.0MB',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n                               {'href': 'https://github.com/podaac/tutorials/blob/master/notebooks/MODIS_L2P_SST_DataCube.ipynb',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1940473819-POCLOUD',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n                     'online_access_flag': True,\n                     'orbit_parameters': {'inclination_angle': '98.1',\n                                          'number_of_orbits': '1.0',\n                                          'period': '98.4',\n                                          'swath_width': '2330.0'},\n                     'organizations': ['NASA/JPL/PODAAC'],\n                     'original_format': 'UMM_JSON',\n                     'platforms': ['Aqua'],\n                     'processing_level_id': '2',\n                     'service_features': {'esi': {'has_formats': False,\n                                                  'has_spatial_subsetting': False,\n                                                  'has_temporal_subsetting': False,\n                                                  'has_transforms': False,\n                                                  'has_variables': False},\n                                          'harmony': {'has_formats': True,\n                                                      'has_spatial_subsetting': True,\n                                                      'has_temporal_subsetting': True,\n                                                      'has_transforms': False,\n                                                      'has_variables': True},\n                                          'opendap': {'has_formats': True,\n                                                      'has_spatial_subsetting': True,\n                                                      'has_temporal_subsetting': True,\n                                                      'has_transforms': False,\n                                                      'has_variables': True}},\n                     'short_name': 'MODIS_A-JPL-L2P-v2019.0',\n                     'summary': 'NASA produces skin sea surface temperature '\n                                '(SST) products from the Infrared (IR) '\n                                'channels of the Moderate-resolution Imaging '\n                                'Spectroradiometer (MODIS) onboard the Aqua '\n                                'satellite. Aqua was launched by NASA on May '\n                                '4, 2002, into a sun synchronous, polar orbit '\n                                'with a daylight ascending node at 1:30 pm, '\n                                'formation flying in the A-train with other '\n                                'Earth Observation Satellites (EOS), to study '\n                                'the global dynamics of the Earth atmosphere, '\n                                'land and oceans. MODIS captures data in 36 '\n                                'spectral bands at a variety of spatial '\n                                'resolutions.  Two SST products can be present '\n                                'in these files. The first is a skin SST '\n                                'produced for both day and night (NSST) '\n                                'observations, derived from the long wave IR '\n                                '11 and 12 micron wavelength channels, using a '\n                                'modified nonlinear SST algorithm intended to '\n                                'provide continuity of SST derived from '\n                                'heritage and current NASA sensors. At night, '\n                                'a second SST product is generated using the '\n                                'mid-infrared 3.95 and 4.05 micron  wavelength '\n                                'channels which are unique to MODIS; the SST '\n                                'derived from these measurements is identified '\n                                'as SST4. The SST4 product has lower '\n                                'uncertainty, but due to sun glint can only be '\n                                'used at night. MODIS L2P SST data have a 1 km '\n                                'spatial resolution at nadir and are stored in '\n                                '288 five minute granules per day. Full global '\n                                'coverage is obtained every two days, with '\n                                'coverage poleward of 32.3 degree being '\n                                'complete each day.  The production of MODIS '\n                                'L2P SST files is part of the Group for High '\n                                'Resolution Sea Surface Temperature (GHRSST) '\n                                'project and is a joint collaboration between '\n                                'the NASA Jet Propulsion Laboratory (JPL), the '\n                                'NASA Ocean Biology Processing Group (OBPG), '\n                                'and the Rosenstiel School of Marine and '\n                                'Atmospheric Science (RSMAS). Researchers at '\n                                'RSMAS are responsible for SST algorithm '\n                                'development, error statistics and quality '\n                                'flagging, while the OBPG, as the NASA ground '\n                                'data system, is responsible for the '\n                                'production of daily MODIS ocean products. JPL '\n                                'acquires MODIS ocean granules from the OBPG '\n                                'and reformats them to the GHRSST L2P netCDF '\n                                'specification with complete metadata and '\n                                'ancillary variables, and distributes the data '\n                                'as the official Physical Oceanography Data '\n                                'Archive (PO.DAAC) for SST.  The R2019.0 '\n                                'supersedes the previous R2014.0 datasets '\n                                'which can be found at '\n                                'https://doi.org/10.5067/GHMDA-2PJ02',\n                     'time_start': '2002-07-04T00:00:00.000Z',\n                     'title': 'GHRSST Level 2P Global Sea Surface Skin '\n                              'Temperature from the Moderate Resolution '\n                              'Imaging Spectroradiometer (MODIS) on the NASA '\n                              'Aqua satellite (GDS2)',\n                     'updated': '2019-12-02T22:59:24.849Z',\n                     'version_id': '2019.0'}],\n          'id': 'https://cmr.earthdata.nasa.gov:443/search/collections.json?concept_id=C1940473819-POCLOUD',\n          'title': 'ECHO dataset metadata',\n          'updated': '2021-11-04T00:50:26.948Z'}}\n\n\nLet’s walk through what each of these service values mean:\n\nAssociations\n\nCMR is a large web of interconnected metadata “schemas”, including Collections, Granules, Services, Tools, and Variables. In this case, this collection is associated with two unique services, two tools, and several unique variables.\n\nTags\n\nThere are also tags that describe what service options exist at a high-level. In this case, we see that this dataset supports the ability to reformat, subset by space and time, as well as by variable. This is used in web applications like Earthdata Search to surface those customization options more readily.\n\nService Features\n\nIn this case, we see three separate “features” listed here: esi, Harmony, and OPeNDAP.\n\n\nWe will dig into more details on what Harmony offers for this dataset.\nFirst, we need to isolate the services returned for this dataset:\n\nservices = response['feed']['entry'][0]['associations']['services']\nprint(services)\n\n['S1962070864-POCLOUD', 'S2004184019-POCLOUD']\n\n\ncmr_service_url = \"https://cmr.earthdata.nasa.gov/search/services\"\nInspect the first service returned. Now we’re going to search the services endpoint to view that individual service’s metadata, like we did with our dataset above.\nTODO: Explain why we need the output format in umm_json\noutput_format = \"umm_json\"\nservice_response = requests.get(f\"{cmr_service_url}.{output_format}?concept-id={services[0]}\")\n\npprint(service_response.json())\n\n{'hits': 1,\n 'items': [{'meta': {'concept-id': 'S1962070864-POCLOUD',\n                     'concept-type': 'service',\n                     'deleted': False,\n                     'format': 'application/vnd.nasa.cmr.umm+json',\n                     'native-id': 'POCLOUD_podaac_l2_cloud_subsetter',\n                     'provider-id': 'POCLOUD',\n                     'revision-date': '2021-11-02T22:57:03.597Z',\n                     'revision-id': 19,\n                     'user-id': 'podaaccloud'},\n            'umm': {'AccessConstraints': 'None',\n                    'Description': 'Endpoint for subsetting L2 Subsetter via '\n                                   'Harmony',\n                    'LongName': 'PODAAC Level 2 Cloud Subsetter',\n                    'MetadataSpecification': {'Name': 'UMM-S',\n                                              'URL': 'https://cdn.earthdata.nasa.gov/umm/service/v1.4',\n                                              'Version': '1.4'},\n                    'Name': 'PODAAC L2 Cloud Subsetter',\n                    'OperationMetadata': [{'OperationName': 'SPATIAL_SUBSETTING'},\n                                          {'OperationName': 'VARIABLE_SUBSETTING'},\n                                          {'OperationName': 'TEMPORAL_SUBSETTING'}],\n                    'ServiceKeywords': [{'ServiceCategory': 'EARTH SCIENCE '\n                                                            'SERVICES',\n                                         'ServiceTerm': 'SUBSETTING/SUPERSETTING',\n                                         'ServiceTopic': 'DATA MANAGEMENT/DATA '\n                                                         'HANDLING'}],\n                    'ServiceOptions': {'Subset': {'SpatialSubset': {'BoundingBox': {'AllowMultipleValues': False}},\n                                                  'TemporalSubset': {'AllowMultipleValues': False},\n                                                  'VariableSubset': {'AllowMultipleValues': True}},\n                                       'SupportedReformattings': [{'SupportedInputFormat': 'HDF5',\n                                                                   'SupportedOutputFormats': ['NETCDF-4']},\n                                                                  {'SupportedInputFormat': 'NETCDF-4',\n                                                                   'SupportedOutputFormats': ['NETCDF-4']}]},\n                    'ServiceOrganizations': [{'LongName': 'Physical '\n                                                          'Oceanography '\n                                                          'Distributed Active '\n                                                          'Archive Center, Jet '\n                                                          'Propulsion '\n                                                          'Laboratory, NASA',\n                                              'Roles': ['ORIGINATOR'],\n                                              'ShortName': 'NASA/JPL/PODAAC'}],\n                    'Type': 'Harmony',\n                    'URL': {'Description': 'PROJECT HOME PAGE',\n                            'URLValue': 'https://harmony.earthdata.nasa.gov'},\n                    'Version': '1.1.0'}}],\n 'took': 14}\n\n\nTODO: Describe these different service options and broader Harmony / backend subsetter context."
  },
  {
    "objectID": "tutorials/Data_Access__Harmony_Subsetting.html#discover-variable-names",
    "href": "tutorials/Data_Access__Harmony_Subsetting.html#discover-variable-names",
    "title": "",
    "section": "Discover variable names",
    "text": "TODO: Could this be an “exercise” to gain more familiarity with CMR?\n\nvariables = response['feed']['entry'][0]['associations']['variables']\nprint(variables)\n\n['V1997812737-POCLOUD', 'V1997812697-POCLOUD', 'V2112014688-POCLOUD', 'V1997812756-POCLOUD', 'V1997812688-POCLOUD', 'V1997812670-POCLOUD', 'V1997812724-POCLOUD', 'V2112014684-POCLOUD', 'V1997812701-POCLOUD', 'V1997812681-POCLOUD', 'V2112014686-POCLOUD', 'V1997812663-POCLOUD', 'V1997812676-POCLOUD', 'V1997812744-POCLOUD', 'V1997812714-POCLOUD']\n\n\ncmr_var_url = \"https://cmr.earthdata.nasa.gov/search/variables\"\nvar_response = requests.get(f\"{cmr_var_url}.{output_format}?concept-id={variables[0]}\")\n\npprint(var_response.json())\n\n{'hits': 1,\n 'items': [{'associations': {'collections': [{'concept-id': 'C1940473819-POCLOUD'}]},\n            'meta': {'concept-id': 'V1997812737-POCLOUD',\n                     'concept-type': 'variable',\n                     'deleted': False,\n                     'format': 'application/vnd.nasa.cmr.umm+json',\n                     'native-id': 'MODIS_A-JPL-L2P-v2019.0-sses_standard_deviation_4um',\n                     'provider-id': 'POCLOUD',\n                     'revision-date': '2021-10-19T02:26:51.560Z',\n                     'revision-id': 6,\n                     'user-id': 'jmcnelis'},\n            'umm': {'DataType': 'byte',\n                    'Definition': 'mid-IR SST standard deviation error; non '\n                                  'L2P core field; signed byte array:  WARNING '\n                                  'Some applications are unable to properly '\n                                  'handle signed byte values. If values are '\n                                  'encountered > 127, please subtract 256 from '\n                                  'this reported value',\n                    'Dimensions': [{'Name': 'time',\n                                    'Size': 1,\n                                    'Type': 'TIME_DIMENSION'},\n                                   {'Name': 'nj',\n                                    'Size': 2030,\n                                    'Type': 'ALONG_TRACK_DIMENSION'},\n                                   {'Name': 'ni',\n                                    'Size': 1354,\n                                    'Type': 'CROSS_TRACK_DIMENSION'}],\n                    'FillValues': [{'Type': 'SCIENCE_FILLVALUE',\n                                    'Value': -128}],\n                    'LongName': 'SSES standard deviation error based on '\n                                'proximity confidence flags',\n                    'Name': 'sses_standard_deviation_4um',\n                    'Offset': 10.0,\n                    'Scale': 0.07874016,\n                    'Sets': [{'Index': 1,\n                              'Name': 'sses_standard_deviation_4um',\n                              'Size': 1,\n                              'Type': 'General'}],\n                    'Units': 'kelvin',\n                    'ValidRanges': [{'Max': 127, 'Min': -127}],\n                    'VariableType': 'ANCILLARY_VARIABLE'}}],\n 'took': 14}\n\n\nLet’s print out a simple list of all associated variable names.\nvar_list = []\nfor i in range(len(variables)):\n    response = requests.get(f\"{cmr_var_url}.{output_format}?concept-id={variables[i]}\")\n    response = response.json()\n    var_list.append(response['items'][0]['umm']['Name'])\n\npprint(var_list)\n\n['sses_standard_deviation_4um',\n 'l2p_flags',\n 'time',\n 'dt_analysis',\n 'sses_standard_deviation',\n 'sst_dtime',\n 'sses_bias_4um',\n 'lat',\n 'sea_surface_temperature_4um',\n 'sses_bias',\n 'lon',\n 'sea_surface_temperature',\n 'quality_level',\n 'wind_speed',\n 'quality_level_4um']"
  },
  {
    "objectID": "tutorials/Data_Access__Harmony_Subsetting.html#harmony-py-set-up",
    "href": "tutorials/Data_Access__Harmony_Subsetting.html#harmony-py-set-up",
    "title": "",
    "section": "Harmony-Py set up",
    "text": "[TODO] Describe Harmony-Py in more detail (connection between the library and the API).\nNext steps adopted from the intro tutorial notebook in the Harmony-Py library: https://github.com/nasa/harmony-py/blob/main/examples/intro_tutorial.ipynb\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\nWhen creating the Client, we need to provide Earthdata Login credentials, which are required to access data from NASA EOSDIS. This basic line below assumes that we have a .netrc available.\nharmony_client = Client()\n\n\nCreate Harmony Request\nThe following are common request parameters:\n\ncollection: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n\nProvide a concept ID, which is an ID provided in the Common Metadata Repository (CMR) metadata\nData product short name (e.g. SENTINEL-1_INTERFEROGRAMS).\n\nspatial: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\ntemporal: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n\nOther advanced parameters that may be of interest. Note that many reformatting or advanced projection options may not be available for your requested dataset. See the documentation for details on how to construct these parameters.\n[TODO] Describe area/time use case with this dataset - maybe demonstrating Earthdata Search for browsing??\nrequest = Request(\n    collection=Collection(id=short_name),\n    spatial=BBox(60,-45.75,90,-45), # bounding box example that can be used as an alternative to shapefile input\n    temporal={\n        'start': dt.datetime(2021, 11, 1),\n        'stop': dt.datetime(2021, 11, 2),\n    },\n    # variables=variables,\n)\n\n\nCheck Request validity\nBefore submitting a Harmony Request, we can test your request to see if it’s valid and how to fix it if not. In particular, request.is_valid will check to ensure that the spatial BBox bounds and temporal ranges are entered correctly.\n\nrequest.is_valid()\n\nTrue\n\n\n\n\nSubmit request\nNow that the request is created, we can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents the submitted request.\n\njob_id = harmony_client.submit(request)\njob_id\n\n'9e0cb72b-9662-4b2e-9828-bc85a024b93d'\n\n\n\n\nCheck request status\nWe can check on the progress of a processing job with status(). This method blocks while communicating with the server but returns quickly.\n\nharmony_client.status(job_id)\n\n{'status': 'running',\n 'message': 'There were 2 collections that matched the provided short name MODIS_A-JPL-L2P-v2019.0. See https://cmr.earthdata.nasa.gov/concepts/C1940473819-POCLOUD for details on the selected collection. The version ID for the selected collection is 2019.0. To use a different collection submit a new request specifying the desired CMR concept ID instead of the collection short name.',\n 'progress': 0,\n 'created_at': datetime.datetime(2021, 11, 4, 0, 50, 34, 333000, tzinfo=tzlocal()),\n 'updated_at': datetime.datetime(2021, 11, 4, 0, 50, 34, 333000, tzinfo=tzlocal()),\n 'request': 'https://harmony.earthdata.nasa.gov/MODIS_A-JPL-L2P-v2019.0/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?forceAsync=true&subset=lat(-45.75%3A-45)&subset=lon(60%3A90)&subset=time(%222021-11-01T00%3A00%3A00%22%3A%222021-11-02T00%3A00%3A00%22)',\n 'num_input_granules': 7}\n\n\nDepending on the size of the request, it may be helpful to wait until the request has completed processing before the remainder of the code is executed. The wait_for_processing() method will block subsequent lines of code while optionally showing a progress bar.\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n [ Processing: 100% ] |###################################################| [|]\n\n\n\n\nView Harmony job response and output URLs\nOnce the data request has finished processing, we can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on the request if available.\nresult_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete.\n\ndata = harmony_client.result_json(job_id)\npprint(data)\n\n{'createdAt': '2021-11-04T00:50:34.333Z',\n 'jobID': '9e0cb72b-9662-4b2e-9828-bc85a024b93d',\n 'links': [{'href': 'https://harmony.earthdata.nasa.gov/stac/9e0cb72b-9662-4b2e-9828-bc85a024b93d/',\n            'rel': 'stac-catalog-json',\n            'title': 'STAC catalog',\n            'type': 'application/json'},\n           {'bbox': [66.801, -66.443, 113.49, -44.231],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101083501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T08:39:58.000Z',\n                         'start': '2021-11-01T08:35:01.000Z'},\n            'title': '20211101083501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [66.7, -45.7, 90, -45],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101084001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T08:44:58.000Z',\n                         'start': '2021-11-01T08:40:01.000Z'},\n            'title': '20211101084001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [60, -45.7, 73.9, -45],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101101501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T10:19:58.000Z',\n                         'start': '2021-11-01T10:15:01.000Z'},\n            'title': '20211101101501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [88.05, -48.131, 120.198, -26.81],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101180501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T18:09:58.000Z',\n                         'start': '2021-11-01T18:05:01.000Z'},\n            'title': '20211101180501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [86.7, -45.7, 90, -45],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T18:14:58.000Z',\n                         'start': '2021-11-01T18:10:01.000Z'},\n            'title': '20211101181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [61.9, -45.7, 90, -45],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101194501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T19:49:58.000Z',\n                         'start': '2021-11-01T19:45:01.000Z'},\n            'title': '20211101194501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [60, -45.7, 69.2, -45],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101212501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-11-01T21:29:58.000Z',\n                         'start': '2021-11-01T21:25:01.000Z'},\n            'title': '20211101212501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'href': 'https://harmony.earthdata.nasa.gov/jobs/9e0cb72b-9662-4b2e-9828-bc85a024b93d?linktype=https&page=1&limit=2000',\n            'rel': 'self',\n            'title': 'The current page',\n            'type': 'application/json'}],\n 'message': 'There were 2 collections that matched the provided short name '\n            'MODIS_A-JPL-L2P-v2019.0. See '\n            'https://cmr.earthdata.nasa.gov/concepts/C1940473819-POCLOUD for '\n            'details on the selected collection. The version ID for the '\n            'selected collection is 2019.0. To use a different collection '\n            'submit a new request specifying the desired CMR concept ID '\n            'instead of the collection short name.',\n 'numInputGranules': 7,\n 'progress': 100,\n 'request': 'https://harmony.earthdata.nasa.gov/MODIS_A-JPL-L2P-v2019.0/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?forceAsync=true&subset=lat(-45.75%3A-45)&subset=lon(60%3A90)&subset=time(%222021-11-01T00%3A00%3A00%22%3A%222021-11-02T00%3A00%3A00%22)',\n 'status': 'successful',\n 'updatedAt': '2021-11-04T00:51:11.841Z',\n 'username': 'amy.steiker'}\n\n\n\n\nDirect cloud access\nNote that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response.\n\nRetrieve list of output URLs.\nThe result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below.\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\nurls = list(results)\npprint(urls)\n\n['s3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101083501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101084001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101101501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101180501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101194501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101212501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4']\n\n\nWe can see that the first file returned does not include the _subsetted suffix, which indicates that a blank file was returned, as no data values were located within our subsetted region. We’ll select the second URL in the list to bring into xarray below.\n\nurl = urls[1]\nurl\n\n's3://harmony-prod-staging/public/podaac/l2-subsetter/28b897a7-7349-4e1b-8141-2a06290c196f/20211101084001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4'\n\n\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\ncreds = harmony_client.aws_credentials()\n\n\n\nOpen staged files with s3fs and xarray\nWe use the AWS s3fs package to create a file system that can then be read by xarray:\nnetcdf_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\n\n**Note, Ideally, we’d show how to bring in all files into a single xarray ds but I’m getting an error:\n# # Iterate through remote_files to create a fileset\n# fileset = [netcdf_fs.open(file) for file in urls]\n# fileset\n# ds = xr.open_mfdataset(fileset)\nNow that we have our s3 file system set, including our declared credentials, we’ll use that to open the url, and read in the file through xarray:\n\nwith netcdf_fs.open(url) as f:\n        ds= xr.open_dataset(f)\n        display(ds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 366, ni: 1154, time: 1)\nCoordinates:\n    lat                      (nj, ni, time) float32 ...\n    lon                      (nj, ni, time) float32 ...\n  * time                     (time) datetime64[ns] 2021-11-01T08:40:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/50)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\\n2021-11-04 ...\n    ...                         ...\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Ascending\n    day_night_flag:             Day\n    history_json:               [{\"date_time\": \"2021-11-04T00:50:49.990002+00...xarray.DatasetDimensions:nj: 366ni: 1154time: 1Coordinates: (3)lat(nj, ni, time)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :[-90.]valid_max :[90.]comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[422364 values with dtype=float32]lon(nj, ni, time)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :[-180.]valid_max :[180.]comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[422364 values with dtype=float32]time(time)datetime64[ns]2021-11-01T08:40:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2021-11-01T08:40:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :[-1000]valid_max :[10000]comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[422364 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :[-32767]valid_max :[32767]comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[422364 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :[0]valid_max :[5]comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :[0 1 2 3 4 5]flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[422364 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :[-127]valid_max :[127]comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[422364 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :[-127]valid_max :[127]comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[422364 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :[0]valid_max :[16]comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :[ 1  2  4  8 16]coverage_content_type :qualityInformation[422364 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :[0.001]valid_max :[100.]comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[422364 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :[50]valid_max :[30000]comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[422364 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :[-127]valid_max :[127]comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :[2.]height :10 mcoverage_content_type :auxiliaryInformation[422364 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :[-127]valid_max :[127]comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[422364 values with dtype=float32]Attributes: (50)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAAC\n2021-11-04 00:50:49.989964 l2ss-py v1.1.0 (bbox=[[60.0, 90.0], [-45.75, -45.0]] cut=True)comment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Ascending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Quicklooklicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20211101T110613Zfile_quality_level :[3]spatial_resolution :1kmstart_time :20211101T084001Ztime_coverage_start :20211101T084001Zstop_time :20211101T084458Ztime_coverage_end :20211101T084458Znorthernmost_latitude :[-27.3202]southernmost_latitude :[-48.6134]easternmost_longitude :[97.5941]westernmost_longitude :[65.1593]source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :[0.01]geospatial_lon_units :degrees_eastgeospatial_lon_resolution :[0.01]acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Ascendingday_night_flag :Dayhistory_json :[{\"date_time\": \"2021-11-04T00:50:49.990002+00:00\", \"derived_from\": \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20211101084001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\", \"program\": \"l2ss-py\", \"version\": \"1.1.0\", \"parameters\": \"bbox=[[60.0, 90.0], [-45.75, -45.0]] cut=True\", \"program_ref\": \"https://cmr.earthdata.nasa.gov:443/search/concepts/S1962070864-POCLOUD\", \"$schema\": \"https://harmony.earthdata.nasa.gov/schemas/history/0.1.0/history-v0.1.0.json\"}]\n\n\n\n\nPlot the data\n# # Determine the lat/lon coordinate names\n# for coord_name, coord in ds.coords.items():\n#     if 'units' not in coord.attrs:\n#         continue\n#     if coord.attrs['units'] == 'degrees_north':\n#         lat_var = coord_name\n#     if coord.attrs['units'] == 'degrees_east':\n#         lon_var = coord_name\n\n# print(f'lat_var={lat_var}')\n# print(f'lon_var={lon_var}')\n\n# # if ds[variable].size == 0:\n# #     print(\"No data in subsetted region. Exiting\")\n# #     sys.exit(0)\n    \n# import matplotlib.pyplot as plt\n# import math\n\n# fig, axes = plt.subplots(ncols=3, nrows=math.ceil((len(ds.data_vars)/3)))\n# fig.set_size_inches((15,15))\n\n# for count, xvar in enumerate(ds.data_vars):\n#     if  ds[xvar].dtype == \"timedelta64[ns]\":\n#         continue\n#         #ds[xvar].astype('timedelta64[D]').plot(ax=axes[int(count/3)][count%3])\n#     ds[xvar].plot(ax=axes[int(count/3)][count%3])\n\n\nExtra code attempting to read in directly from s3 into xarray:\n# netcdf_stores = [netcdf_fs.get_mapper(root=u, check=False) for u in urls]\n# netcdf_stores\n\n# xr.open_mfdataset(netcdf_stores)"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__gdalvrt.html#summary",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__gdalvrt.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Hello World"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__gdalvrt.html#exercise",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__gdalvrt.html#exercise",
    "title": "",
    "section": "Exercise",
    "text": "Import Required Packages\n#\n\n\nGet Temporary Credentials and Configure Local Environment\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME.\n#\n#\n\nInsert the credentials into our boto3 session and configure out rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session can then be used to pass those credentials and get S3 objects from applicable buckets.\n#\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n#\n\n\n\nRead In and Process STAC Asset Links\nIn the previous section, we used the NASA CMR-STAC API to discover HLS assets the intersect with our search criteria, i.e., ROI, Date range, and collections. The search results were filtered and saved as text files by individual bands for each tile. We will read in the text files for tile T13TGF for the RED (L30: B04 & S30: B04), NIR (L30: B05 & S30: B8A), and Fmask bands.\n\nList text files with HLS links\n#\n\n\nRead in our asset links for BO4 (RED)\n#\n\n\nRead in and combine our asset links for BO5 (Landsat NIR) and B8A (Sentinel-2 NIR)\nThe near-infrared (NIR) band for Landsat is B05 while the NIR band for Sentinel-2 is B8A. In the next step we will read in and combine the lists into a single NIR list.\n#\n\n\nRead in our asset links for Fmask\n#\nIn this example we will use the gdalbuildvrt.exe utility to create a time series virtual raster format (VRT) file. The utility, however, expects the links to be formated with the GDAL virtual file system (VSI) path, rather than the actual asset links. We will therefore use the VSI path to access our assets. The examples below show the VSI path substitution for S3 (vsis3) links.\n/vsis3/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2020191T172901.v1.5.B04.tif\nSee the GDAL Virtual File Systems for more information regarding GDAL VSI.\n\n\nWrite out a new text file containing the vsis3 path\n#\n#\n#\n\n\n\nRead in geoJSON for subsetting\nWe will use the input geoJSON file to clip the source data to our desired region of interest.\n#\nTo clip the source data to our input feature boundary, we need to transform the feature boundary from its original WGS84 coordinate reference system to the projected reference system of the source HLS file (i.e., UTM Zone 13).\n#\n\nTransform geoJSON feature from WGS84 to UTM\n#\n\n\n\nDirect S3 Data Access\n\nStart up a dask client\n#\n#\nThere are multiple way to read COG data in as a time series. The subprocess package is used in this example to run GDAL’s build virtual raster file (gdalbuildvrt) executable outside our python session. First we’ll need to construct a string object with the command and it’s parameter parameters (including our temporary credentials). Then, we run the command using the subprocess.call() function.\n\n\nBuild GDAL VRT Files\n\nConstruct the GDAL VRT call\n#\nWe now have a fully configured gdalbuildvrt string that we can pass to Python’s subprocess module to run the gdalbuildvrt executable outside our Python environment.\n\n\n\nExecute gdalbuildvrt to construct a VRT on disk from the S3 links\n#\n0 means success! We’ll have some troubleshooting to do you get any other value. In this tutorial, the path for the output VRT file or the input file list are the first things to check.\nWhile we’re here, we’ll build the VRT files for the NIR layers and the Fmask layers.\n#\n#\n\n\n\nReading in an HLS time series\nWe can now read the VRT files into our Python session. A drawback of reading VRTs into Python is that the time coordinate variable needs to be contructed. Below we not only read in the VRT file using rioxarray, but we also repurpose the band variable, which is generated automatically, to hold out time information.\n\nRead the RED VRT in as xarray with Dask backing\n#\nAbove we use the parameter chunk in the rioxarray.open_rasterio() function to enable the Dask backing. What this allows is lazy reading of the data, which means the data is not actually read in into memory at this point. What we have is an object with some metadata and pointer to the source data. The data will be streamed to us when we call for it, but not stored in memory until with call the Dask compute() or persist() methods.\n\n\nPrint out the time coordinate\n#\n\n\nClip out the ROI and persist the result in memory\nUp until now, we haven’t read any of the HLS data into memory. Now we will use the persist() method to load the data into memory.\n#\nAbove, we persisted the clipped results to memory using the persist() method. This doesn’t necessarily need to be done, but it will substantially improve the performance of the visualization of the time series below.\n\n\nPlot red_clip with hvplot\n#\n\n\n\nRead in the NIR and Fmask VRT files\n#\n#\n\n\nCreate an xarray dataset\nWe will now combine the RED, NIR, and Fmask arrays into a dataset and create/add a new NDVI variable.\n#\nAbove, we created a new NDVI variable. Now, we will clip and plot our results.\n#\n\nPlot NDVI\n#\nYou may have notices that some images for some of the time step are ‘blurrier’ than other. This is because they are contaminated in some way, be it clouds, cloud shadows, snow, ice.\n\n\n\nApply quality filter\nWe want to keep NDVI data values where Fmask equals 0 (no clouds, no cloud shadow, no snow/ice, no water.\n#\n#\n\nAggregate by month\nFinally, we will use xarray’s groupby operation to aggregate by month.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__gdalvrt.html#references",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__gdalvrt.html#references",
    "title": "",
    "section": "References",
    "text": "https://rasterio.readthedocs.io/en/latest/\nhttps://corteva.github.io/rioxarray/stable/index.html\nhttps://tutorial.dask.org/index.html\nhttps://examples.dask.org/applications/satellite-imagery-geotiff.html"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#summary",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the harmonization of the ICESat-2 ATL03 data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, with Sea Surface Temperature variables available from PO.DAAC on the Earthdata Cloud.\n\nObjectives\n[TODO]\n\n\n\nImport packages\n#\n\n\nDetermine storage location of datasets of interest\nFirst, let’s see whether our datasets of interest reside in the Earthdata Cloud or whether they reside on premise, or “on prem” at a local data center.\nBackground from CMR API [TODO: consider removing]: The cloud_hosted parameter can be set to “true” or “false”. When true, the results will be restricted to collections that have a DirectDistributionInformation element or have been tagged with gov.nasa.earthdatacloud.s3.\nWe are building off of the CMR introductory tutorial, beginning with a collection search.\n#\nWe want to search by collection to inspect the access and service options that exist:\n#\nIn the CMR introduction tutorial, we explored cloud-hosted collections from different DAAC providers, and identified the CMR concept-id for a given dataset id (also referred to as a short_name). Here we’ll start with two datasets that we want to explore over a coincident area and time:\n#\nLike in the intro tutorial, we’re going to first determine what concept-ids are returned for the MODIS dataset. First, retrieve collection results based on the MODIS short_name:\n#\nFor each collection result, print out the CMR concept-id and version:\n#\nTwo collections are returned, both at version 2019.0. We can see from the suffix of the id that one is associated with “POCLOUD” versus “PODAAC”. That gives us a clue in terms of where the data are hosted, but we can also use the cloud_hosted parameter set to True to confirm.\n#\n#\nWe will save this concept-id to use later on when we access the data granules.\n#\nNow we will try our ICESat-2 dataset to see what id’s are returned for a given dataset name.\n#\n#\nTwo separate datasets exist in the CMR, one at version 3 and one at version 4. Let’s see if these are cloud_hosted:\n#\n#\nWhen set to False, we get our collections back. We have now determined that we have a copy of the MODIS dataset in the cloud, whereas the ICESat-2 dataset (both versions) remains “on premise”, residing in a local data center.\nSave the ATL03 concept ID and the MODIS GHRSST concept ID to variables:\n#\n\nSpecify time range and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below\n#\nPerform a granule search over our time and area of interest. How many granules are returned?\n#\n#\nPrint the file names, size, and links:\n#\n\n\n\nDownload ICESat-2 ATL03 granule\n[TODO] Describe what services are available, including icepyx (provide references), but just direct download for simplicity. Describe that this is being “downloaded” to our cloud environment - what does that mean in terms of cost, etc.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n#\nYou need Earthdata login credentials to download data from NASA DAACs. These are the credentials you stored in the .netrc file you setup in previous tutorials.\nWe’ll use the netrc package to retrieve your login and password without exposing them.\n#\nTo retrieve the granule data, we use the requests.get() method, passing Earthdata login credentials as a tuple using the auth keyword.\n#\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n#\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the mkdir method from the os package.\n#\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n#\nCheck to make sure it is downloaded.\n#\nATL03_20190622061415_12980304_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the height data for ground-track 1 left-beam.\n#\n\n\nDetermine variables of interest: SST, ocean color, chemistry…\n#\n\n\nPull those variables into xarray “in place”\n\nFirst, we need to determine the granules returned from our time and area of interest\n#\n#\n#\n\n\n\nGet S3 credentials\n#\n#\n\n\nOpen a s3 file\n#\n\n\nUse geolocation of ICESat-2 to define the single transect used to pull coincident ocean data out from array\n\n\nCreate a plot of the single transect of gridded data\n(bonus: time series) - describe what this means to egress out of the cloud versus pulling the original data down (benefit to processing in the cloud)"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#download-modis-ghrsst-data-from-cloud",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#download-modis-ghrsst-data-from-cloud",
    "title": "",
    "section": "Download MODIS GHRSST data from Cloud",
    "text": "#\n#\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#resources-optional",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#resources-optional",
    "title": "",
    "section": "Resources (optional)",
    "text": ""
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#conclusion",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": ""
  },
  {
    "objectID": "tutorials-templates/index.html",
    "href": "tutorials-templates/index.html",
    "title": "Tutorial Templates",
    "section": "",
    "text": "Templates of all tutorials — .ipynb notebooks with code removed — are available at: https://github.com/NASA-Openscapes/2021-Cloud-Hackathon/tree/main/tutorials-templates.\nPlease open these tutorial templates to follow along and live-code with the tutorial lead."
  },
  {
    "objectID": "tutorials-templates/Data_discovery_with_cmr.html#what-is-cmr",
    "href": "tutorials-templates/Data_discovery_with_cmr.html#what-is-cmr",
    "title": "",
    "section": "What is CMR",
    "text": "CMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials-templates/Data_discovery_with_cmr.html#what-is-the-cmr-api",
    "href": "tutorials-templates/Data_discovery_with_cmr.html#what-is-the-cmr-api",
    "title": "",
    "section": "What is the CMR API",
    "text": "API stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials-templates/Data_discovery_with_cmr.html#how-to-search-cmr-from-python",
    "href": "tutorials-templates/Data_discovery_with_cmr.html#how-to-search-cmr-from-python",
    "title": "",
    "section": "How to search CMR from Python",
    "text": "The first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in depth tutorial on requests is here\n#\nThen we need to authenticate with EarthData Login. Since we’ve already set this up in the previous lesson, here you need to enter your username before executing the cell.\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll build this url as a python variable.\n#\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for colections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the url for the root CMR endpoint.\nWe are going to search collections first, so we add collections to the url. I’m using a python format string here.\n#\nIn this first example, I want to retrieve a list of collections that are hosted in the cloud. Each collection has a cloud_hosted parameter that is either True if that collection is in the cloud and False if it is not. The migration of NASA data to the cloud is a work in progress. Not all collections tagged as cloud_hosted have granules. To search for only cloud_hosted datasets with granules, I also set has_granules to True.\nI also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n#\nrequests returns a Response object.\nOften, we want to check that our request was successful. In a notebook or someother interactive environment, we can just type the name of the variable we have saved our requests Response to, in this case the response variable.\n#\nA cleaner and more understandable method is to check the status_code attribute. Both methods return a HTTP status code. You’ve probably seen a 404 error when you have tried to access a website that doesn’t exist.\n#\nTry changing CMR_OPS to https://cmr.earthdata.nasa.gov/searches and run requests.get again. Don’t forget to rerun the cell that assigns the url variable\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. This information is printed below. TODO: maybe some context for where the 2 elements k, v, come from?\n#\nWe can see that the content returned is in json format in the UTF-8 character set. We can also see from CMR-Hits that 919 collections were found.\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but because it is case-insensitive, both\n#\nand\n#\nwork.\nThis is a large number of data sets. I’m going to restrict the search to cloud-hosted datasets from ASF (Alaska SAR Facility) because I’m interested in SAR images of sea ice. To do this, I set the provider parameter to ASF.\nYou can modify the code below to explore all of the cloud-hosted datasets or cloud-hosted datasets from other providers. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWhen search by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs.\n#\n#\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n#\nIt is more convenient to work with json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nStep through response.json(), then to response.json()['feed']['entry'][0]. A reminder that python starts indexing at 0, not 1!\n#\nThe first response is not the result I am looking for TODO: because xyz…but it does show a few variables that we can use to further refine the search. So I want to print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable. TODO: is it worth saying something about what “feed” and “entry” are?\n#\n#\nBut there is a problem. We know from CMR-Hits that there are 49 datasets but only 10 are printed. This is because CMR restricts the number of results returned by a query. The default is 10 but it can be set to a maximum of 2000. Knowing that there were 49 ‘hits’, I’ll set page_size to 49. Then, we can re-run our for loop for the collections.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_discovery_with_cmr.html#granule-search",
    "href": "tutorials-templates/Data_discovery_with_cmr.html#granule-search",
    "title": "",
    "section": "Granule Search",
    "text": "In NASA speak, Granules are files. In this example, we will search for recent Sentinel-1 Ground Range Detected (GRD) Medium Resolution Synthetic Aperture Radar images over the east coast of Greenland. The data in these files are most useful for sea ice mapping.\nI’ll use the data range 2021-10-17 00:00 to 2021-10-18 23:59:59.\nI’ll use a simple bounding box to search. - SW: 76.08166,-67.1746 - NW: 88.19689,21.04862\nFrom the collections search, I know the concept ids for Sentinel-1A and Sentinel-1B GRD medium resolution are - C1214472336-ASF - C1327985578-ASF\nWe need to change the resource url to look for granules instead of collections\n#\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n#\n#\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html",
    "title": "",
    "section": "",
    "text": "Direct S3 Data Access - Rough PODAAC ECCO SSH Example"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#summary",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the harmonization of the ICESat-2 ATL03 data product, currently (as of November 2021) available publicaly via direct download at the NSIDC DAAC, with Sea Surface Temperature variables available from PO.DAAC on the Earthdata Cloud.\n\nObjectives\n\n\n\nImport packages\n#\n\n\nDetermine storage location of datasets of interest\nFirst, let’s see whether our datasets of interest reside in the Earthdata Cloud or whether they reside on premise, or “on prem” at a local data center.\nBackground from CMR API (consider removing): The cloud_hosted parameter can be set to “true” or “false”. When true, the results will be restricted to collections that have a DirectDistributionInformation element or have been tagged with gov.nasa.earthdatacloud.s3. curl “https://cmr.earthdata.nasa.gov/search/collections?cloud_hosted=true”\n\n\nDeclare datasets of interest\nIdentify the dataset ID that is used internally within CMR to designate each dataset\n#\n#\nStart with the MODIS dataset, setting the cloud_hosted parameter to True:\n#\nNow we will try our ICESat-2 dataset to see what id’s are returned for a given dataset name.\n#\nTwo separate datasets exist in the CMR. Now let’s take each ID, setting the cloud_hosted parameter to True, to identify which dataset is cloud-hosted:\n#\nWhat happens if we comment out this parameter? Do we see results returned? [TODO: Add instructions on how to comment lines using command slash]\nIt would make more sense to set cloud_hosted to False\nI suggest saving the ATL03 concept ID and the MODIS GHRSST concept ID to a variables\nicesat2_concept_id = 'C1997321091-NSIDC_ECS'\nmodis_concept_id = 'C1940475563-POCLOUD'\nNow we have determined that our Sentinel dataset is provided in the cloud, whereas the ICESat-2 dataset remains “on premise”, residing in a local data center.\n\nSpecify time range and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below\n#\n#\n#\n\n\n\nDownload ICESat-2 ATL03 granule\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n#\nYou need Earthdata login credentials to download data from NASA DAACs. These are the credentials you stored in the .netrc file you setup in previous tutorials.\nWe’ll use the netrc package to retrieve your login and password without exposing them.\n#\nTo retrieve the granule data, we use the requests.get() method, passing Earthdata login credentials as a tuple using the auth keyword.\n#\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n#\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the mkdir method from the os package.\n#\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n#\nCheck to make sure it is downloaded.\n#\nATL03_20190622061415_12980304_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the height data for ground-track 1 left-beam.\n#\n\n\nDetermine variables of interest: SST, ocean color, chemistry…\n#\n\n\nPull those variables into xarray “in place”\n\nFirst, we need to determine the granules returned from our time and area of interest\n#\n#\n#\n\n\n\nGet S3 credentials\n#\n#\n\n\nOpen a s3 file\n#\n\n\nUse geolocation of ICESat-2 to define the single transect used to pull coincident ocean data out from array\n\n\nCreate a plot of the single transect of gridded data\n(bonus: time series) - describe what this means to egress out of the cloud versus pulling the original data down (benefit to processing in the cloud)"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#download-modis-ghrsst-data-from-cloud",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#download-modis-ghrsst-data-from-cloud",
    "title": "",
    "section": "Download MODIS GHRSST data from Cloud",
    "text": "#\n#\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#resources-optional",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#resources-optional",
    "title": "",
    "section": "Resources (optional)",
    "text": ""
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#conclusion",
    "href": "tutorials-templates/Data_Access__Harmonize-cloud-non-cloud_andy_to_avoid_merge_conflicts.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": ""
  },
  {
    "objectID": "tutorials-templates/NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials-templates/NASA_Earthdata_Authentication.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is need to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials-templates/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials-templates/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "",
    "section": "Import Required Packages",
    "text": "#\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n#\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#timing",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#timing",
    "title": "",
    "section": "Timing",
    "text": "Exercise: 30 min"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#summary",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remove assets.\n\nFour STAC Specifications\nSTAC Item (aka Granule)\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "title": "",
    "section": "What you will learn from this tutorial",
    "text": "how to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#import-required-packages",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#import-required-packages",
    "title": "",
    "section": "Import Required Packages",
    "text": "#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#explored-available-nasa-providers",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#explored-available-nasa-providers",
    "title": "",
    "section": "Explored available NASA Providers",
    "text": "#\n\nConnect to the CMR-STAC API\n#\nWe’ll create a providers variable so we can take a deeper look into available data providers - subcategories are referred to as “children”. We can then print them as a for loop.\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "title": "",
    "section": "Connect to the LPCLOUD Provider/STAC Catalog",
    "text": "For this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#list-stac-collections",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#list-stac-collections",
    "title": "",
    "section": "List STAC Collections",
    "text": "We’ll create a products variable to view deeper in the STAC Catalog.\n#\n\nPrint one of the STAC Collection records\nTo view the products variable we just created, let’s look at one entry as a dictionary.\n#\n\n\nPrint the STAC Collection ids with their title\nIn the above output, id and title are two elements of interest that we can print for all products using a for loop.\nTODO: pystac/pystac-client objects\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "title": "",
    "section": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API",
    "text": "We will define our region of interest (ROI) using the geojson file from the previous exercise, while also specifying the data collections and time range of needed for our example.\n\nRead in a geojson file\nReading in a geojson file with geopandas will return the geometry of our polygon (our ROI).\n#\n\n\nVisualize contents of geojson file\nWe can use that geometry to visualize the polygon: here, a square. But wait for it –\n#\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n#\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the region of interest (ROI), and the data collections, that we will pass to the STAC API.\n\n\nExtract the coordinates for the region of interest (ROI)\n#\nSo, what just happen there? Let’s take a quick detour to break it down.\n\n\n\n\n\n\n\nfield.to_json()\ngeojson representation as a string\n\n\njson.loads()\nparse a json string into a Python Dictionary\n\n\nfeatures\nDictionary key that contrains the geometry object with coordinates (returned as a list)\n\n\ngeometry\nDictionary key that contains the coordinates for the ROI\n\n\n\n\n\nSpecify date range\nNext up is to specify our date range using ISO_8601 date formatting.\n#\n\n\nSpecify the STAC Collections\nSTAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. We can use the ids that we printed from our products for loop above. Let’s focus on S30 and L30 collections.\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Now we can put all our search criteria together using catalog.search from the pystac_client package.\n#\n\nPrint out how many STAC Items match our search query\n#\nWe now have a search object containing the STAC Items that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\n#\nLet’s list a few of these item_collections:\n#\n\n\nPrint an item as a dictionary\nWe can view a single item as a dictionary, as we did above with products.\n#\nTODO Aaron: identify a few things of note"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#filtering-stac-items",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#filtering-stac-items",
    "title": "",
    "section": "Filtering STAC Items",
    "text": "While the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property, for example cloud cover. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis.\nWe’ll make a cloudcover variable where we will set the maximum allowable cloud cover and extract the band links for those Items that match or are less than the max cloud cover.\n#\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above).\nTODO Aaron. We’ll set these bands that are reasonable for EVI calculations: read more about B8A and friends here: TODO.\n#\nAnd now to loop through and filter the item_collection by cloud cover and bands:\n#\nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n#\nNOTICE that in the list of links that we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. TODO: We can notice this by HOW.\nThese two tiles represent neighboring Universal Transverse Mercator (UTM) zones. We will split the list of links into separate lists for each tile.\nWe now have a list of links to data assets that meet our search and filtering criteria. The commands that follow will split this list into logical groupings using python routines."
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "title": "",
    "section": "Split Data Links List into Logical Groupings",
    "text": "TODO Aaron: a bit more description about what all these steps do :)\nSplit by UTM tile specified in the file name (e.g., T14TKL & T13TGF)\n#\n#\n\nPrint dictionary keys and values, i.e. the data links\n#\n#\nNow we will create a separate list of data links for each tile\n#\n\n\nPrint band/layer links for HLS tile T13TGF\n#\n\n\nSplit the links by band\n#\n#\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#save-links-to-a-text-file",
    "href": "tutorials-templates/Data_Discovery__CMR-STAC_API.html#save-links-to-a-text-file",
    "title": "",
    "section": "Save links to a text file",
    "text": "To complete this exercise, we will save the individual link lists as separate text files with descriptive names.\n\nWrite links from CMR-STAC API to a file\n#\n\n\nWrite links to file for S3 access\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#summary",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direction in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Eartdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieve data is very quickly\nNo need to download data! Work with data in a more efficient manner\nIncreased capacity to do parallel processing\nYou are working completely with the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nAccess only works within AWS us-west-2 region\nNeed an AWS S3 “token” to access S3 Bucket\nToken expires after 1 hour\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\nS3 on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tool that allow you to do that, not discussed in this tutorial\n\n\n\n\nObjective\n\nConfigure our notebook environment and retrieve temporary S3 credentials for in-region direct S3 bucket access\nAccess a single HLS file\nAccess and clip an HLS file to a region of interest\nCreate an HLS time series data array\n\nLet’s get started!\n\n\n\nImport Required Packages\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#configure-local-environment-and-get-temporary-credentials",
    "title": "",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "To perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\n#\n#\n#\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n#\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-s3-links",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-s3-links",
    "title": "",
    "section": "Read in S3 Links",
    "text": "In the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\n#\nWe will safe our list of links and a single link as Python objects for use later.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-a-single-hls-file",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-a-single-hls-file",
    "title": "",
    "section": "Read in a single HLS file",
    "text": "We’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n#\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n#\n\nPlot the HLS S3 object\n#\nWe can print out the data value as a numpy array by typing .values\n#\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-a-single-hls-file",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-a-single-hls-file",
    "title": "",
    "section": "Read in and clip a single HLS file",
    "text": "To clip the HLS file, our feature representing our region of interest must be in the same coordinate reference system (CRS) or projection coordinate system as the HLS file. The map projection for our HLS file is Universal Transverse Mercator (UTM) zone 13N. Our feature is mapped to WGS84 geographic coordinate system grid space. We need to transform the geographic coordinate reference system (CRS) of our feature to the UTM projected coordinate system (i.e., UTM Zone 13N)\n\nRead in our geojson file and transform its CRS\n#\nLet’s take a look at the bounding coordinate values.\n#\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner (-101.67271614074707, 41.04754380304359) and upper right corner (-101.65344715118408, 41.06213891056728) respectively.\n\n\nGet the projection information from the HLS file\n#\n\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n#\n#\n#\nThe coordinates for our feature have now been converted to UTM Zone 13N whether meters is the designated unit. Note the difference in the values between field_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our HLS file to our region of insterest!\n\n\nAccess and clip the HLS file\nWe can now use our transformed ROI bounding box to clip the HLS S3 object we accessed before. We’ll use the `rio.clip\n#\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-an-hls-time-series",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-an-hls-time-series",
    "title": "",
    "section": "Read in and clip an HLS time series",
    "text": "Now we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n#\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\n#\n#\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\n#\nNow, we will create our time series.\n#\nSince we used the chunks parameter while reading the data, the hls_ts_da object is read into memory. To do that we’ll use the load() function. But, before that, we’ll clip the hls_ts_da object to our roi using our transformed roi coordinates.\n#\nNow, we’ll see what we have. Use hvplot to plot the clipped time series\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#resourses",
    "href": "tutorials-templates/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#resourses",
    "title": "",
    "section": "Resourses",
    "text": "Build time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse"
  },
  {
    "objectID": "tutorials-templates/Introduction_to_xarray.html#why-do-we-need-xarray",
    "href": "tutorials-templates/Introduction_to_xarray.html#why-do-we-need-xarray",
    "title": "",
    "section": "Why do we need xarray?",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials-templates/Introduction_to_xarray.html#what-is-xarray",
    "href": "tutorials-templates/Introduction_to_xarray.html#what-is-xarray",
    "title": "",
    "section": "What is xarray",
    "text": "xarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with builtin methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials-templates/Introduction_to_xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials-templates/Introduction_to_xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "",
    "section": "What you will learn from this tutorial",
    "text": "In this tutorial you will learn how to: - load a netcdf file into xarray - interrogate the Dataset and understand the difference between DataArray and Dataset - subset a Dataset - calculate annual and monthly mean fields - calculate a time series of zonal means - plot these results\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n#\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n#\nAs we are in an interactive environment, we can just type ds to see what we have.\n#\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n#\n#\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n#\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n#\nThis approach can also be used to add new variables\n#\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Introduction_to_xarray.html#subsetting-and-indexing",
    "href": "tutorials-templates/Introduction_to_xarray.html#subsetting-and-indexing",
    "title": "",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n#\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following.\n#\n#\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nPay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 75 N, the last value is 15 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180.\n#\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283, -104.98785545855408). xarray can handle this! If we just want data from the nearest grid point, we can use sel.\n#\n#\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n#\nsel() and interp() can also be used on Dataset objects.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Introduction_to_xarray.html#analysis",
    "href": "tutorials-templates/Introduction_to_xarray.html#analysis",
    "title": "",
    "section": "Analysis",
    "text": "As a simple example, let’s try to calculate a mean field for the whole time range.\n#\nWe can also calculate a zonal mean (averaging over longitude)\n#\nOther aggregation methods include min(), max(), std(), along with others.\n#\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n#\n#\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n#"
  },
  {
    "objectID": "tutorials-templates/Introduction_to_xarray.html#plot-results",
    "href": "tutorials-templates/Introduction_to_xarray.html#plot-results",
    "title": "",
    "section": "Plot results",
    "text": "#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#summary",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direction in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Eartdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieve data is very quickly\n\nNo need to download data! Work with data in a more efficient manner\n\nIncreased capacity to do parallel processing\n\nYou are working completely with the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nIf you’re workflow is in the cloud, choose S3 over HTTPS\n\nAccess only works within AWS us-west-2 region\n\nNeed an AWS S3 “token” to access S3 Bucket\n\nToken expires after 1 hour\n\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\n\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\n\nS3 on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\n\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tool that allow you to do that, not discussed in this tutorial"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#what-you-will-learn-from-this-tutorial",
    "title": "",
    "section": "What you will learn from this tutorial",
    "text": "how to retrieve temporary S3 credentials for in-region direct S3 bucket access\n\nhow to configure our notebook environment for in-region direct S3 bucket access\n\nhow to access a single HLS file via in-region direct S3 bucket access\n\nhow to create an HLS time series data array from cloud assets via in-region direct S3 bucket access\n\nhow to plot results"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#import-required-packages",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#import-required-packages",
    "title": "",
    "section": "Import Required Packages",
    "text": "#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#configure-local-environment-and-get-temporary-credentials",
    "title": "",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "To perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\n#\n#\n#\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n#\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#read-in-s3-links",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#read-in-s3-links",
    "title": "",
    "section": "Read in S3 Links",
    "text": "In the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\n#\nWe will safe our list of links and a single link as Python objects for use later.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#read-in-a-single-hls-file",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#read-in-a-single-hls-file",
    "title": "",
    "section": "Read in a single HLS file",
    "text": "We’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n#\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n#\n\nPlot the HLS S3 object\n#\nWe can print out the data value as a numpy array by typing .values\n#\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#read-in-hls-as-a-time-series",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#read-in-hls-as-a-time-series",
    "title": "",
    "section": "Read in HLS as a time series",
    "text": "Now we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n#\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\n#\n#\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\n#\nNow, we will create our time series.\n#\nSince we used the chunks parameter while reading the data, the hls_ts_da object is not read into memory yet. To do that we’ll use the load() function.\nNow, we’ll see what we have. Use hvplot to plot our time series\n#\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#concluding-remarks",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#concluding-remarks",
    "title": "",
    "section": "Concluding Remarks",
    "text": "The above exercise demonstrated how to perform in-region direct S3 bucket access for HLS cloud data assets. HLS cloud data assets are stored as Cloud Optimized GeoTIFFs, a format that has been the benifactor of data discovery and access advancements within the Python ecosystem. Knowing what the data storage format is (e.g., COG, netcdf4, or zarr store) and/or what data access protocol you’re using is critical in determining what Python data access method you will use. For COG data, rioxarray package is often prefered due to is ability to bring the geospatial data format into an xarray object. For netcdf4 files, the standard xarray package incombination with s3fs allow users to perform in-region direct access reads into an xarray object. Finally, if you are using OPeNDAP to connect to data, specialized packages like pydap have been integrated into xarray for streamline access directly to an xarray object."
  },
  {
    "objectID": "tutorials-templates/Data_Access__Direct_S3_Access.html#resourses",
    "href": "tutorials-templates/Data_Access__Direct_S3_Access.html#resourses",
    "title": "",
    "section": "Resourses",
    "text": "Build time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "href": "tutorials-templates/Data_Access__Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "title": "",
    "section": "Using the Harmony-Py library to access customized data from NASA Earthdata",
    "text": ""
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmony_Subsetting.html#summary",
    "href": "tutorials-templates/Data_Access__Harmony_Subsetting.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Harmony allows you to seamlessly analyze Earth observation data from different NASA data centers… https://harmony.earthdata.nasa.gov/\n\nBenefits\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow users to request only the data they want, in the format and projection they want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n\nHarmony-Py is a Python library for integrating with NASA’s Harmony Services.\nHarmony-Py provides a Python alternative to directly using Harmony’s RESTful API. It handles NASA Earthdata Login (EDL) authentication and optionally integrates with the CMR Python Wrapper by accepting collection results as a request parameter. It’s convenient for scientists who wish to use Harmony from Jupyter notebooks as well as machine-to-machine communication with larger Python applications.\n\n\nObjectives\n\nPractice skills learned from intro to CMR tutorial to discover what access and service options exist for a given data set"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmony_Subsetting.html#import-packages",
    "href": "tutorials-templates/Data_Access__Harmony_Subsetting.html#import-packages",
    "title": "",
    "section": "Import Packages",
    "text": "#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "href": "tutorials-templates/Data_Access__Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "title": "",
    "section": "Discover service options for a given data set",
    "text": "First, what do we mean by a “service”? [TODO] Describe how we define services and their benefits, and how not all datasets have services on them due to level of support, etc….\nLet’s see what the collection metadata tells us\n\nBuilding off of CMR introduction tutorial:\n#\nWe want to search by collection to inspect the access and service options that exist:\n#\nIn the CMR introduction tutorial, we explored cloud-hosted collections from different DAAC providers, and identified the CMR concept-id for a given data set id (also referred to as a short_name).\nHere we are jumping ahead and already know the concept_id we are interested in, by browsing cloud-hosted datasets from PO.DAAC in Earthdata Search: https://search.earthdata.nasa.gov/portal/podaac-cloud/search.\nWe are going to focus on MODIS_A-JPL-L2P-v2019.0: GHRSST Level 2P Global Sea Surface Skin Temperature from the Moderate Resolution Imaging Spectroradiometer (MODIS) on the NASA Aqua satellite (GDS2). Let’s first save this as a variable that we can use later on once we request data from Harmony.\n#\nWe will view the top-level metadata for this collection to see what additional service and variable metadata exist.\n#\nPrint the response:\n#\nLet’s walk through what each of these service values mean:\n\nAssociations\n\nCMR is a large web of interconnected metadata “schemas”, including Collections, Granules, Services, Tools, and Variables. In this case, this collection is associated with two unique services, two tools, and several unique variables.\n\nTags\n\nThere are also tags that describe what service options exist at a high-level. In this case, we see that this dataset supports the ability to reformat, subset by space and time, as well as by variable. This is used in web applications like Earthdata Search to surface those customization options more readily.\n\nService Features\n\nIn this case, we see three separate “features” listed here: esi, Harmony, and OPeNDAP.\n\n\nWe will dig into more details on what Harmony offers for this dataset.\nFirst, we need to isolate the services returned for this dataset:\n#\n#\nInspect the first service returned. Now we’re going to search the services endpoint to view that individual service’s metadata, like we did with our dataset above.\nTODO: Explain why we need the output format in umm_json\n#\n#\n#\nTODO: Describe these different service options and broader Harmony / backend subsetter context."
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmony_Subsetting.html#discover-variable-names",
    "href": "tutorials-templates/Data_Access__Harmony_Subsetting.html#discover-variable-names",
    "title": "",
    "section": "Discover variable names",
    "text": "TODO: Could this be an “exercise” to gain more familiarity with CMR?\n#\n#\n#\n#\nLet’s print out a simple list of all associated variable names.\n#\n#"
  },
  {
    "objectID": "tutorials-templates/Data_Access__Harmony_Subsetting.html#harmony-py-set-up",
    "href": "tutorials-templates/Data_Access__Harmony_Subsetting.html#harmony-py-set-up",
    "title": "",
    "section": "Harmony-Py set up",
    "text": "[TODO] Describe Harmony-Py in more detail (connection between the library and the API).\nNext steps adopted from the intro tutorial notebook in the Harmony-Py library: https://github.com/nasa/harmony-py/blob/main/examples/intro_tutorial.ipynb\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\nWhen creating the Client, we need to provide Earthdata Login credentials, which are required to access data from NASA EOSDIS. This basic line below assumes that we have a .netrc available.\n#\n\n\nCreate Harmony Request\nThe following are common request parameters:\n\ncollection: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n\nProvide a concept ID, which is an ID provided in the Common Metadata Repository (CMR) metadata\nData product short name (e.g. SENTINEL-1_INTERFEROGRAMS).\n\nspatial: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\ntemporal: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n\nOther advanced parameters that may be of interest. Note that many reformatting or advanced projection options may not be available for your requested dataset. See the documentation for details on how to construct these parameters.\n[TODO] Describe area/time use case with this dataset - maybe demonstrating Earthdata Search for browsing??\n#\n\n\nCheck Request validity\nBefore submitting a Harmony Request, we can test your request to see if it’s valid and how to fix it if not. In particular, request.is_valid will check to ensure that the spatial BBox bounds and temporal ranges are entered correctly.\n#\n\n\nSubmit request\nNow that the request is created, we can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents the submitted request.\n#\n\n\nCheck request status\nWe can check on the progress of a processing job with status(). This method blocks while communicating with the server but returns quickly.\n#\nDepending on the size of the request, it may be helpful to wait until the request has completed processing before the remainder of the code is executed. The wait_for_processing() method will block subsequent lines of code while optionally showing a progress bar.\n#\n\n\nView Harmony job response and output URLs\nOnce the data request has finished processing, we can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on the request if available.\nresult_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete.\n#\n\n\nDirect cloud access\nNote that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response.\n\nRetrieve list of output URLs.\nThe result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below.\n#\nWe can see that the first file returned does not include the _subsetted suffix, which indicates that a blank file was returned, as no data values were located within our subsetted region. We’ll select the second URL in the list to bring into xarray below.\n#\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n#\n\n\n\nOpen staged files with s3fs and xarray\nWe use the AWS s3fs package to create a file system that can then be read by xarray:\n#\n\n\n**Note, Ideally, we’d show how to bring in all files into a single xarray ds but I’m getting an error:\n#\nNow that we have our s3 file system set, including our declared credentials, we’ll use that to open the url, and read in the file through xarray:\n#\n\n\nPlot the data\n#\n\n\nExtra code attempting to read in directly from s3 into xarray:\n#"
  },
  {
    "objectID": "projects/hackathon-projects.html",
    "href": "projects/hackathon-projects.html",
    "title": "Hackathon Projects",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "projects/index.html#purpose-of-the-projects",
    "href": "projects/index.html#purpose-of-the-projects",
    "title": "Projects",
    "section": "Purpose of the projects:",
    "text": "During the Cloud Hackathon we will be facilitating team hacking sessions in the second half of each day. The purpose of these sessions is for you to gain hands-on experience in working together on a well-defined problem, in a collaborative space where you can talk things through and get help."
  },
  {
    "objectID": "projects/index.html#what-is-hacking",
    "href": "projects/index.html#what-is-hacking",
    "title": "Projects",
    "section": "What is hacking?",
    "text": "Hacking is a session of focused, highly collaborative work time – often involving coding – in which the group creates conditions for rapid absorption of new ideas and methods. The word “hack” or “hackathon” has many different interpretations, both positive and negative. Here our intention is to foster the idea of hacking as a fun, interactive and welcoming environment to explore and experiment with computer code."
  },
  {
    "objectID": "projects/index.html#how-will-the-projects-be-conducted",
    "href": "projects/index.html#how-will-the-projects-be-conducted",
    "title": "Projects",
    "section": "How will the projects be conducted?",
    "text": "Participants are invited to start conversations about projects in the Slack channel 2021-nasacloudhack-projects one week before the Cloud Hackathon begins.\n\nIf you have a project idea brewing, please pitch it in this channel (even if you have signed up for the cloud hackathon as a team; tag your proposed teammates if you already have that worked out).\nStart a thread with “Project idea:” and then provide a few sentences. Include whether you are looking for teammates to join this project. Others who are interested can respond in a thread.\nWe welcome a broad range of project topics. People often use project time to dig deeper into concepts introduced in tutorials, to explore problems within their own research, or to advance community data sharing and software building efforts.\nThe Cloud Hackathon team is here to help you get clear on project ideas and decide on what is possible within 5 days. Feel free to reach out to any of us between now and the Cloud Hackathon in the 2021-nasacloudhack-help channel, or @ us – we all have “helper” appended to the front of our names. !\n\nAt the end of Day 1 of the Cloud Hackathon we will have a Pitchfest where proposer(s) can pitch their idea, and mention whether they are still looking for teammates or if they have already formed a team using the Slack _#2021-nasacloudhack-projects _channel. At this time we will finalize the project teams for the week.\nTeam hacktime will begin on Day 2.\nEach team is encouraged to identify a project lead, likely the person who pitched the idea, who has knowledge of the datasets and the specific problem to be explored. But roles can be assigned as the group decides to best fit skills and needs.\nThroughout the hackathon we will have optional morning office hours 8-9am PT for additional support or team check-in time.\nOn the final day of the Cloud Hackathon, each team will present their work in a series of lightning talks."
  },
  {
    "objectID": "projects/index.html#what-can-i-do-to-prepare-in-advance",
    "href": "projects/index.html#what-can-i-do-to-prepare-in-advance",
    "title": "Projects",
    "section": "What can I do to prepare in advance?",
    "text": "If you have a project idea already brewing, we encourage you to share that with participants on our Slack channel 2021-nasacloudhack-projects.\nFeel free to explore various projects and initiate conversations. The goal is to gather as much information as you can to inform your decision about which team to join during the Cloud Hackathon"
  },
  {
    "objectID": "projects/index.html#suggested-github-workflow",
    "href": "projects/index.html#suggested-github-workflow",
    "title": "Projects",
    "section": "Suggested GitHub Workflow",
    "text": "Coming soon!"
  },
  {
    "objectID": "external/zarr-eosdis-store.html",
    "href": "external/zarr-eosdis-store.html",
    "title": "",
    "section": "",
    "text": "Zarr Example\nimported on: 2021-11-09\n\nThis notebook is from NASA’s Zarr EOSDIS store notebook\n\n\nThe original source for this document is https://github.com/nasa/zarr-eosdis-store\n\n\n\nzarr-eosdis-store example\nInstall dependencies\nimport sys\n\n# zarr and zarr-eosdis-store, the main libraries being demoed\n!{sys.executable} -m pip install zarr zarr-eosdis-store\n\n# Notebook-specific libraries\n!{sys.executable} -m pip install matplotlib\nImportant: To run this, you must first create an Earthdata Login account (https://urs.earthdata.nasa.gov) and place your credentials in ~/.netrc e.g.:\n   machine urs.earthdata.nasa.gov login YOUR_USER password YOUR_PASSWORD\nNever share or commit your password / .netrc file!\nBasic usage. After these lines, we work with ds as though it were a normal Zarr dataset\nimport zarr\nfrom eosdis_store import EosdisStore\n\nurl = 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210715090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n\nds = zarr.open(EosdisStore(url))\nView the file’s variable structure\n\nprint(ds.tree())\n\n/\n ├── analysed_sst (1, 17999, 36000) int16\n ├── analysis_error (1, 17999, 36000) int16\n ├── dt_1km_data (1, 17999, 36000) int16\n ├── lat (17999,) float32\n ├── lon (36000,) float32\n ├── mask (1, 17999, 36000) int16\n ├── sea_ice_fraction (1, 17999, 36000) int16\n ├── sst_anomaly (1, 17999, 36000) int16\n └── time (1,) int32\n\n\nFetch the latitude and longitude arrays and determine start and end indices for our area of interest. In this case, we’re looking at the Great Lakes, which have a nice, recognizeable shape. Latitudes 41 to 49, longitudes -93 to 76.\nlats = ds['lat'][:]\nlons = ds['lon'][:]\nlat_range = slice(lats.searchsorted(41), lats.searchsorted(49))\nlon_range = slice(lons.searchsorted(-93), lons.searchsorted(-76))\nGet the analysed sea surface temperature variable over our area of interest and apply scale factor and offset from the file metadata. In a future release, scale factor and add offset will be automatically applied.\nvar = ds['analysed_sst']\nanalysed_sst = var[0, lat_range, lon_range] * var.attrs['scale_factor'] + var.attrs['add_offset']\nDraw a pretty picture\n\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = [16, 8]\nplt.imshow(analysed_sst[::-1, :])\nNone\n\n\n\n\nIn a dozen lines of code and a few seconds, we have managed to fetch and visualize the 3.2 megabyte we needed from a 732 megabyte file using the original archive URL and no processing services"
  },
  {
    "objectID": "external/cof-zarr-reformat.html#getting-started",
    "href": "external/cof-zarr-reformat.html#getting-started",
    "title": "",
    "section": "Getting Started",
    "text": "We will access monthly ocean bottom pressure (OBP) data from ECCO V4r4 (10.5067/ECG5M-OBP44), which are provided as a monthly time series on a 0.5-degree latitude/longitude grid.\nThe data are archived in netCDF format. However, this notebook demonstration will request conversion to Zarr format for files covering the period between 2010 and 2018. Upon receiving our request, Harmony’s backend will convert the files and stage them in S3 for native access in AWS (us-west-2 region, specifically). We will access the new Zarr datasets as an aggregated dataset using xarray, and leverage the S3 native protocols for direct access to the data in an efficient manner.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\n\n\n\n\nRequirements\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport requests\nimport json\nimport time\nimport s3fs\n\nShortName = \"ECCO_L4_OBP_05DEG_MONTHLY_V4R4\"\n\n\nStudy period\nSet some “master” inputs to define the time and place contexts for our case studies in the ipynb. This example will be requesting time subsets and receiving global data back from the Harmony API.\nstart_date = \"2010-01-01\"\nend_date   = \"2018-12-31\"\n\n\nData Access\nSome features in the Harmony API require us to identify the target dataset/collection by its concept-id (which uniquely idenfifies it among the other datasets in the Common Metadata Repository). Support for selection by the dataset ShortName will be added in a future release.\n\nCommon Metadata Repository (CMR)\nFor now, we will need to get the concept-id that corresponds to our dataset by accessing its metadata from the CMR. Read more about the CMR at: https://cmr.earthdata.nasa.gov/\nRequest the UMM Collection metadata (i.e. metadata about the dataset) from the CMR and select the concept-id as a new variable ccid.\n\nresponse = requests.get(\n    url='https://cmr.earthdata.nasa.gov/search/collections.umm_json', \n    params={'provider': \"POCLOUD\",\n            'ShortName': ShortName,\n            'page_size': 1}\n)\n\nummc = response.json()['items'][0]\n\nccid = ummc['meta']['concept-id']\n\nccid\n\n'C1990404791-POCLOUD'\n\n\n\n\nHarmony API\nAnd get the Harmony API endpoint and zarr parameter like we did for SMAP before:\n\nbase = f\"https://harmony.earthdata.nasa.gov/{ccid}\"\nhreq = f\"{base}/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset\"\nrurl = f\"{hreq}?format=application/x-zarr\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr\n\n\nECCO monthly collections have 312 granules in V4r4 (you can confirm with the granule listing from CMR Search API) so we can get the entire time series for 2010 to 2018 with one request to the Harmony API.\nFormat a string of query parameters to limit the processing to the desired time period. Then, append the string of time subset parameters to the variable rurl.\n\nsubs = '&'.join([f'subset=time(\"{start_date}T00:00:00.000Z\":\"{end_date}T23:59:59.999Z\")'])\n\nrurl = f\"{rurl}&{subs}\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=time(\"2010-01-01T00:00:00.000Z\":\"2018-12-31T23:59:59.999Z\")\n\n\nSubmit the request and monitor the processing status in a while loop, breaking it on completion of the request job:\n\nresponse = requests.get(url=rurl).json()\n\n# Monitor status in a while loop. Wait 10 seconds for each check.\nwait = 10\nwhile True:\n    response = requests.get(url=response['links'][0]['href']).json()\n    if response['status']!='running':\n        break\n    print(f\"Job in progress ({response['progress']}%)\")\n    time.sleep(wait)\n\nprint(\"DONE!\")\n\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nDONE!\n\n\nAccess the staged cloud datasets over native AWS interfaces\nCheck the message field in the response for clues about how to proceed:\n\nprint(response['message'])\n\nThe job has completed successfully. Contains results in AWS S3. Access from AWS us-west-2 with keys from https://harmony.earthdata.nasa.gov/cloud-access.sh\n\n\nThe third item in the list of links contains the shell script from the job status message printed above. Let’s download the same information in JSON format. It should be the fourth item; check to be sure:\n\nlen(response['links'])\n\n102\n\n\nSelect the url and download the json, then load to Python dictionary and print the keys:\n\nwith requests.get(response['links'][3]['href']) as r:\n    creds = r.json()\n\nprint(creds.keys())\n\ndict_keys(['AccessKeyId', 'SecretAccessKey', 'SessionToken', 'Expiration'])\n\n\nCheck the expiration timestamp for the temporary credentials:\n\ncreds['Expiration']\n\n'2021-06-11T02:36:29.000Z'\n\n\nOpen zarr datasets with s3fs and xarray\nGet the s3 output directory and list of zarr datasets from the list of links. The s3 directory should be the fifth item; the urls are from item six onward:\n\ns3_dir = response['links'][4]['href']\n\nprint(s3_dir)\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/\n\n\nNow select the URLs for the staged files and print the first one:\n\ns3_urls = [u['href'] for u in response['links'][5:]]\n\nprint(s3_urls[0])\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/OCEAN_BOTTOM_PRESSURE_mon_mean_2009-12_ECCO_V4r4_latlon_0p50deg.zarr\n\n\nUse the AWS s3fs package and your temporary aws_creds to open the zarr directory storage:\n\ns3 = s3fs.S3FileSystem(\n    key=creds['AccessKeyId'],\n    secret=creds['SecretAccessKey'],\n    token=creds['SessionToken'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nlen(s3.ls(s3_dir))\n\n97\n\n\nPlot the first Ocean Bottom Pressure dataset\nCheck out the documentation for xarray’s open_zarr method at this link. Open the first dataset and plot the OBP variable:\n\nds0 = xr.open_zarr(s3.get_mapper(s3_urls[0]), decode_cf=True, mask_and_scale=True)\n\n# Mask the dataset where OBP is not within the bounds of the variable's valid min/max:\nds0_masked = ds0.where((ds0.OBP>=ds0.OBP.valid_min) & (ds0.OBP<=ds0.OBP.valid_max))\n\n# Plot the masked dataset\nds0_masked.OBP.isel(time=0).plot.imshow(size=10)\n\n<matplotlib.image.AxesImage at 0x7f28ed2ba4c0>\n\n\n\n\n\nLoad the zarr datasets into one large xarray dataset\nLoad all the datasets in a loop and concatenate them:\n\nzds = xr.concat([xr.open_zarr(s3.get_mapper(u)) for u in s3_urls], dim=\"time\")\n\nprint(zds)\n\n<xarray.Dataset>\nDimensions:         (latitude: 360, longitude: 720, nv: 2, time: 97)\nCoordinates:\n  * latitude        (latitude) float64 -89.75 -89.25 -88.75 ... 89.25 89.75\n    latitude_bnds   (latitude, nv) float64 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n  * longitude       (longitude) float64 -179.8 -179.2 -178.8 ... 179.2 179.8\n    longitude_bnds  (longitude, nv) float64 -180.0 -179.5 -179.5 ... 179.5 180.0\n  * time            (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T...\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    OBP             (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    OBPGMAP         (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    Conventions:                  CF-1.8, ACDD-1.3\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2010-01-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2009-12-01T00:00:00\n    title:                        ECCO Ocean Bottom Pressure - Monthly Mean 0...\n    uuid:                         297c8df0-4158-11eb-b208-0cc47a3f687b\n\n\nReference OBP and mask the dataset according to the valid minimum and maximum:\n\nobp = zds.OBP\n\nprint(obp)\n\n<xarray.DataArray 'OBP' (time: 97, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(97, 360, 720), dtype=float64, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * latitude   (latitude) float64 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float64 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\n  * time       (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T06:00:00\nAttributes:\n    comment:                OBP excludes the contribution from global mean at...\n    coverage_content_type:  modelResult\n    long_name:              Ocean bottom pressure given as equivalent water t...\n    units:                  m\n    valid_max:              72.07011413574219\n    valid_min:              -1.7899188995361328\n\n\nGet the valid min and max from the corresponding CF attributes:\n\nobp_vmin, obp_vmax = obp.valid_min, obp.valid_max\n\nobp_vmin, obp_vmax\n\n(-1.7899188995361328, 72.07011413574219)\n\n\nMask the dataset according to the OBP min and max and plot a series:\n\n# Mask dataset where not inside OBP variable valid min/max:\nzds_masked = zds.where((obp>=obp_vmin)&(obp<=obp_vmax))\n\n# Plot SSH again for the first 12 time slices:\nobpp = zds_masked.OBP.isel(time=slice(0, 6)).plot(\n    x=\"longitude\", \n    y=\"latitude\", \n    col=\"time\",\n    levels=8,\n    col_wrap=3, \n    add_colorbar=False,\n    figsize=(14, 8)\n)\n\n# Plot a colorbar on a secondary axis\nmappable = obpp.axes[0][0].collections[0]\ncax = plt.axes([0.05, -0.04, 0.95, 0.04])\ncbar1 = plt.colorbar(mappable, cax=cax, orientation='horizontal')"
  }
]