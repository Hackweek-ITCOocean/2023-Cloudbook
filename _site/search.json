[
  {
    "objectID": "projects/hacking.html",
    "href": "projects/hacking.html",
    "title": "Hacking at the Cloud Hackathon",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "projects/hackathon-projects.html",
    "href": "projects/hackathon-projects.html",
    "title": "Hackathon Projects",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2021 Cloud Hackathon",
    "section": "Welcome",
    "text": "Welcome to Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud, co-hosted by the NASA EOSDIS Physical Oceanography Distributed Active Archive Center (PO.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Land Processes Distributed Active Archive Center (LP.DAAC), with support provided by ASDC DAAC, GES DISC and NASA Openscapes.\nThe Cloud Hackathon will take place virtually from November 15-19, 2021. The event is free to attend, but an application is required. The application period (September 21 - October 12, 2021) is now closed. Those who applied will be informed of the outcome on or around October 20th, 2021."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2021 Cloud Hackathon",
    "section": "About",
    "text": "The Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud is a virtual 5-day (4 hours per day) collaborative open science learning experience aimed at exploring, creating, and promoting effective cloud-based science and applications workflows using NASA Earthdata Cloud data, tools, and services (among others), in support of Earth science data processing and analysis in the era of big data. Its goals are to:\n\nIntroduce Earth science data users to NASA Earthdata cloud-based data products, tools and services in order to increase awareness and support transition to cloud-based science and applications workflows.\nEnable science and applications workflows in the cloud that leverage NASA Earth Observations and capabilities (services) from within the NASA Earthdata Cloud, hosted in Amazon Web Services (AWS) cloud, thus increasing NASA Earthdata data utility and meaningfulness for science and applications use cases.\nFoster community engagement utilizing Earthdata cloud tools and services in support of open science and open data.\n\nOutcome: Participants prototype their science and applications workflows (via hackathon projects) that leverage Earthdata Cloud data and services (focusing on, but not limited to, oceanography, cryosphere, hydrology and land data), which supports them in their transition to cloud-based or hybrid workflows for data processing and analysis.\nThis is an opportunity for researchers that might not yet have had the opportunity to work in the Cloud to explore, learn and prototype workflows with NASA Earthdata in the Cloud, but more intermediate or advanced cloud users interested in further exploring cloud workflows with Earthdata Cloud data and service are also welcome."
  },
  {
    "objectID": "index.html#application",
    "href": "index.html#application",
    "title": "2021 Cloud Hackathon",
    "section": "Application",
    "text": "Information for applicants\nThe Cloud Hackathon will be a virtual event held November 15-19, 2021, where participants will explore the intersection of Earth science data, cloud computing, and big data analysis through demonstration tutorials and hands-on “hacking” projects. To best benefit from the event, we recommend some familiarity or experience with:\n\nNASA Earthdata data (focusing on oceanography, cryosphere, hydrology, cryosphere and land data, including interdisciplinary applications); and\nProgramming skills using Python. We plan to accept participants with diverse skill levels and backgrounds in programming. However, to best benefit from and contribute to the program, participants are expected to have some experience with Python programming.\n\nNo cloud computing experience is required, but we encourage both beginner and more experienced participants with AWS cloud to apply.\nIf selected, participants will have the option to attend a Carpentries-style github, python, shell scripting clinic ahead of the Cloud Hachathon.\n\n\nApplication Form\nIn the application form, we encourage you to think about and provide a science use case that you would like to prototype in the cloud. At the beginning of the hackathon, participants will be able to pitch their use case to support the formation of “hack” projects - by which we mean collaboratively experiment working in with NASA Earthdata data and capabilities in the Cloud. During the hackathon, participants will get into teams of their choosing, around a common use case to “hack” in the cloud. The use cases provided in the application form will also help the organizers best prepare materials tailored to those use cases.\nThe application period has now closed. Thank you for your interest."
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "2021 Cloud Hackathon",
    "section": "What to expect",
    "text": "During the Cloud Hackathon, the selected participants will have access to cloud environments in AWS through a JupyterHub interface, provided through 2i2c.\nParticipants will be guided on how to log into the cloud environment, import needed data recipes and resources, and will have the opportunity to explore and develop science and applications workflows in a cloud environment (hosted in AWS) using example tutorials as building blocks.\nThe Cloud Hackathon is an open science event: all tutorials and examples are developed openly and will be publicly available during and following hackathon. Participants will strengthen their practice of open science, using open source code and “hacking” their projects openly to enable further discovery and contributions by the broader open community following the hackathon.\nThroughout the hackathon, participants will learn about NASA’s Earthdata move to the cloud and Earthdata APIs for data discovery, access, and transformations to enable faster, more efficient time to science.\n\nIn the two to three weeks leading up to the hackathon, participants are encouraged to review background resources that will facilitate a more effective hackathon experience. These resources will be shared here leading up to the Hackathon dates, and will be accessible to all data users, whether they attend the hackathon or not.\nThe following datasets are currently available from the NASA Earthdata Cloud. Participants can choose to prototype a cloud-based science workflow using a combination of these datasets, as well as other non-Earthdata Cloud data. If your preferred dataset is not yet available in the Earthdata Cloud, consider using a current cloud-based dataset as proxy to explore prototyping.\n\nhttps://search.earthdata.nasa.gov/search?ff=Available%20from%20AWS%20Cloud\n\nExample use cases to explore in the cloud (note these are for inspiration only, you are not limited to these workflows):\n\nUse the advanced wildcard search capabilities in Earthdata Search Client/Common Metadata Repository (CMR) to precisely search/select all cloud-archived Sentinel-6A granules\n\nfrom a specific cycle (i.e. a sequence orbits that together provide global spatial coverage), and/or\nfrom a specific pass(es) over multiple cycles (i.e. selected orbits over a series of cycles that together provide a time series coverage).\nThen, prepare the data for gridding or for local analysis at space/time scales which are appropriate for the target analysis (and limited by default given the length of S6A data record…)\n\nTime series analysis across multi-mission measurements spanning data housed both within and outside of NASA Earthdata Cloud, to develop a workflow that can accommodate different data locations, as data continue to migrate to the Cloud:\n\nProgrammatically search for a data variable (e.g. altimetry measurements) at a single point or area of interest across multiple datasets and identify whether the data are available in the Cloud\nAcquire the data based on archived location and combine in order to produce a homogenous time series\n\nExplore/leverage cloud-optimized formats (COFs) such as Zarr to compute global or regional climatology and anomalies for a large-volume dataset (e.g. 1-km MUR SST) without having to download data (in-cloud analysis).\nSubset Level 2 swath dataset of interest spatially and for specific variable and do some exploratory analysis and visualization from within the cloud.\nUse NASA’s CMR-STAC API to search and discover Harmonized Landsat Sentinel-2 (HLS) cloud assets based on cloud data products, area of interest, and date range query parameters.\nHarmonized Landsat Sentinel-2 (HLS) for land monitoring: access, explore, and visualize time series surface reflectance data in the cloud.\n\nThis event is motivated by the dawn of the era of Big Data. NASA’s Earth Observing System Data and Information System (EOSDIS) is in the process of moving EOSDIS data to the cloud, driven by a rapid rate of data ingest into the EOSDIS archive. NASA remote sensing data from both upcoming (e.g. SWOT) and existing (e.g. Terra, Aqua, ICESat-2) missions will be available in the Earthdata Cloud platform in the coming years. The paradigm shift from on-premise (local) to cloud-based data distribution, and that from “download and analyze” to “analysis in place” present opportunities and challenges. Guiding users through this transition is of the utmost importance.\nCloud Hackathon: Transitioning Earthdata Workflows to the Cloud is co-hosted by NASA’s PO.DAAC, NSIDC DAAC, LP.DAAC, with support from ASDC DAAC, GES DISC and the NASA Openscapes Project, and cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "2021 Cloud Hackathon",
    "section": "Code of Conduct",
    "text": "The 2021 Cloud Hackathon is a safe learning space and all participants are required to abide by our Code of Conduct."
  },
  {
    "objectID": "tutorials/NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials/NASA_Earthdata_Authentication.html#summary",
    "title": "",
    "section": "Summary",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is need to access NASA Earthdata assets from a scripting environent like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "",
    "section": "Import Required Packages",
    "text": "from netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | chmod og-rw {0}.{2} | echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo password \\{} >> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n# Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo password \\{} >> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\nEnter NASA Earthdata Login Username \n(or create an account at urs.earthdata.nasa.gov):  ·······\nEnter NASA Earthdata Login Password:  ···········\n\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al\n\ntotal 6524\ndrwxrws--x 18 root   40138    6144 Oct  7 19:48 .\ndrwxr-xr-x  1 root   root       20 Jun 24 15:49 ..\ndrwxr-sr-x  3 jovyan 40138    6144 Feb 22  2021 .aws\n-rw-------  1 jovyan 40138   38519 Oct  6 23:26 .bash_history\ndrwxr-sr-x  7 jovyan 40138    6144 Mar 31  2021 .cache\ndrwxrwsr-x  2 jovyan 40138    6144 Jun  7 20:18 .conda\ndrwxr-sr-x  5 jovyan 40138    6144 Dec 21  2020 .config\n-rw-r--r--  1 jovyan 40138       0 Jan 21  2021 .dodsrc\n-rw-r--r--  1 jovyan 40138      52 Apr 14 21:20 .gitconfig\n-rw-r--r--  1 jovyan 40138      65 Aug  3 15:37 .gitignore\ndrwxr-sr-x  3 jovyan 40138    6144 Feb 17  2021 .intake\ndrwxr-sr-x  2 jovyan 40138    6144 Oct  7 18:34 .ipynb_checkpoints\ndrwxr-sr-x  5 jovyan 40138    6144 Dec 21  2020 .ipython\ndrwxr-sr-x  3 jovyan 40138    6144 Dec 21  2020 .jupyter\ndrwxr-sr-x  2 jovyan 40138    6144 Feb  8  2021 .keras\ndrwxr-sr-x  3 jovyan 40138    6144 Dec 21  2020 .local\n-rw-r--r--  1 jovyan 40138      66 Oct  7 19:47 .netrc\n-rw-r--r--  1 jovyan 40138   12288 Oct  7 19:47 .netrc.swp\n-rw-r--r--  1 jovyan 40138   12288 Oct  7 19:23 .nfs2bc676b524ca6d3900000003\n-rw-r--r--  1 jovyan 40138   12288 Oct  7 19:33 .nfs42638417e5e9eb7700000004\n-rw-r--r--  1 jovyan 40138      54 Oct  7 19:19 .nfs4872e5596aa5419900000001\n-rw-r--r--  1 jovyan 40138   12288 Oct  7 18:41 .nfsbded039ab2f12a9f00000002\ndrwxr-sr-x  3 jovyan 40138    6144 Dec 21  2020 .npm\n-rw-------  1 jovyan 40138       7 Jul 29 01:30 .python_history\ndrwx--S---  2 jovyan 40138    6144 Jun 25 15:45 .ssh\n-rw-------  1 jovyan 40138   11787 Oct  7 19:35 .viminfo\ndrwxr-sr-x 11 jovyan 40138    6144 Oct  7 18:30 2021-Cloud-Hackathon\n-rw-r--r--  1 jovyan 40138 6458487 Aug 31 20:56 HRRR-Explorer.ipynb\n-rw-r--r--  1 jovyan 40138    6521 Oct  7 19:48 Untitled.ipynb\ndrwxr-sr-x  5 jovyan 40138    6144 Aug  6 15:26 appeears-cloud-prototype\n-rw-r--r--  1 jovyan 40138     240 Aug  4 19:41 conda_env.txt\n-rw-r--r--  1 jovyan 40138    1541 Oct  7 17:43 cookies.txt\n-rw-r--r--  1 jovyan 40138     110 May  3 19:09 linux_cmds.txt\ndrwxr-sr-x  6 jovyan 40138    6144 Sep 30 15:32 lpdaac_cloud_data_access\ndrwxr-sr-x  5 jovyan 40138    6144 Sep 23 19:34 nasa_direct_s3_data_access\n-rw-r--r--  1 jovyan 40138     168 Jun 17 15:48 vim_guide.txt"
  },
  {
    "objectID": "tutorials/getting-started.html#introduction",
    "href": "tutorials/getting-started.html#introduction",
    "title": "Getting Started",
    "section": "Introduction",
    "text": "text here"
  },
  {
    "objectID": "tutorials/getting-started.html#how-do-i-get-the-tutorial-repository-into-the-hub",
    "href": "tutorials/getting-started.html#how-do-i-get-the-tutorial-repository-into-the-hub",
    "title": "Getting Started",
    "section": "How do I get the tutorial repository into the Hub?",
    "text": ""
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#summary",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#summary",
    "title": "",
    "section": "Summary",
    "text": "In this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nWhat is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remove assets.\n\nFour STAC Specifications\n\nSTAC Item\nSTAC Catalog\nSTAC Collection\nSTAC API\n\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#exercise",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#exercise",
    "title": "",
    "section": "Exercise",
    "text": "Import Required Packages\n\nfrom pystac_client import Client       # https://pystac-client.readthedocs.io/en/latest/index.html  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\nExplored available providers\nprovider_cat = Client.open('https://cmr.earthdata.nasa.gov/stac/')\n\nproviders = [p for p in provider_cat.get_children()]\n\nfor count, provider in enumerate(providers):\n    print(f'{count} - {provider.title}')\n\n0 - LARC_ASDC\n1 - USGS_EROS\n2 - ESA\n3 - GHRC\n4 - LAADS\n5 - OBPG\n6 - OB_DAAC\n7 - ECHO\n8 - ISRO\n9 - LPCUMULUS\n10 - EDF_DEV04\n11 - GES_DISC\n12 - ASF\n13 - OMINRT\n14 - EUMETSAT\n15 - NCCS\n16 - NSIDCV0\n17 - PODAAC\n18 - LARC\n19 - USGS\n20 - SCIOPS\n21 - LANCEMODIS\n22 - CDDIS\n23 - JAXA\n24 - AU_AADC\n25 - ECHO10_OPS\n26 - LPDAAC_ECS\n27 - NSIDC_ECS\n28 - ORNL_DAAC\n29 - LM_FIRMS\n30 - SEDAC\n31 - LANCEAMSR2\n32 - NOAA_NCEI\n33 - USGS_LTA\n34 - GESDISCCLD\n35 - ASIPS\n36 - ESDIS\n37 - POCLOUD\n38 - NSIDC_CPRD\n39 - ORNL_CLOUD\n40 - XYZ_PROV\n41 - GHRC_DAAC\n42 - CSDA\n43 - MOPITT\n44 - GHRC_CLOUD\n45 - LPCLOUD\n\n\n\n\nSet up an API instance for the LPCLOUD provider\nFor this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\ncatalog = Client.open('https://cmr.earthdata.nasa.gov/stac/LPCLOUD/')\n\nPrint catalog as a Python dictionary\n\ncatalog.to_dict()\n\n{'type': 'Catalog',\n 'id': 'LPCLOUD',\n 'stac_version': '1.0.0',\n 'description': 'Root catalog for LPCLOUD',\n 'links': [{'rel': <RelType.SELF: 'self'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': 'collections',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections',\n   'type': 'application/json',\n   'title': 'Provider Collections'},\n  {'rel': 'search',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search',\n   'type': 'application/geo+json',\n   'title': 'Provider Item Search',\n   'method': 'GET'},\n  {'rel': 'search',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search',\n   'type': 'application/geo+json',\n   'title': 'Provider Item Search',\n   'method': 'POST'},\n  {'rel': 'conformance',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance',\n   'type': 'application/geo+json',\n   'title': 'Conformance Classes'},\n  {'rel': 'service-desc',\n   'href': 'https://api.stacspec.org/v1.0.0-beta.1/openapi.yaml',\n   'type': 'application/vnd.oai.openapi;version=3.0',\n   'title': 'OpenAPI Doc'},\n  {'rel': 'service-doc',\n   'href': 'https://api.stacspec.org/v1.0.0-beta.1/index.html',\n   'type': 'text/html',\n   'title': 'HTML documentation'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003',\n   'type': 'application/json'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0',\n   'type': 'application/json'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5',\n   'type': 'application/json'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5',\n   'type': 'application/json'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v2.0',\n   'type': 'application/json'}],\n 'stac_extensions': [],\n 'conformsTo': ['https://api.stacspec.org/v1.0.0-beta.1/core',\n  'https://api.stacspec.org/v1.0.0-beta.1/item-search',\n  'https://api.stacspec.org/v1.0.0-beta.1/item-search#fields',\n  'https://api.stacspec.org/v1.0.0-beta.1/item-search#query',\n  'https://api.stacspec.org/v1.0.0-beta.1/item-search#sort',\n  'https://api.stacspec.org/v1.0.0-beta.1/item-search#context',\n  'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/core',\n  'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/oas30',\n  'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/geojson'],\n 'title': 'LPCLOUD'}\n\n\nNow we will get all of the child links that are within the STAC Catalog\nproducts = [c for c in catalog.get_children()]\n#products\n\n\nPrint the STAC Collection ids with their title\n\nfor p in products: \n    print(f\"{p.id}: {p.title}\")\n\nASTGTM.v003: ASTER Global Digital Elevation Model V003\nHLSL30.v2.0: HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\nHLSL30.v1.5: HLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5\nHLSS30.v1.5: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30 m V1.5\nHLSS30.v2.0: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\n\n\n\n\nPrint out one of the STAC Collection records\n\nproducts[1].to_dict()\n\n{'type': 'Collection',\n 'id': 'HLSL30.v2.0',\n 'stac_version': '1.0.0',\n 'description': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.',\n 'links': [{'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': 'items',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items',\n   'type': 'application/json',\n   'title': 'Granules in this collection'},\n  {'rel': 'about',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.html',\n   'type': 'text/html',\n   'title': 'HTML metadata for collection'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.json',\n   'type': 'application/json',\n   'title': 'CMR JSON metadata for collection'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2020',\n   'type': 'application/json',\n   'title': '2020 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2021',\n   'type': 'application/json',\n   'title': '2021 catalog'},\n  {'rel': <RelType.SELF: 'self'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': <RelType.PARENT: 'parent'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>}],\n 'stac_extensions': [],\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2013-05-01T00:00:00Z', None]]}},\n 'license': 'not-provided'}\n\n\n\n\n\nSet up query parameters to submit to the CMR-STAC API\nWe will define our area of interest using the geojson file from the previous exercise, while also specifying the data collections and time range of needed for our example.\n\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nfield\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POLYGON ((-101.67272 41.04754, -101.65345 41.0...\n    \n  \n\n\n\n\n\nfieldShape = field['geometry'][0]\nfieldShape\n\n\n\n\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the region of interest (roi), and the data collections, to pass to the STAC API\n\nSpecify the region of interest\n\nroi = json.loads(field.to_json())['features'][0]['geometry']\nroi\n\n{'type': 'Polygon',\n 'coordinates': [[[-101.67271614074707, 41.04754380304359],\n   [-101.65344715118408, 41.04754380304359],\n   [-101.65344715118408, 41.06213891056728],\n   [-101.67271614074707, 41.06213891056728],\n   [-101.67271614074707, 41.04754380304359]]]}\n\n\n\n\nSpecify date range\n#date_range = \"2021-05-01T00:00:00Z/2021-08-30T23:59:59Z\"    # closed interval\n#date_range = \"2021-05-01T00:00:00Z/..\"                      # open interval - does not currently work with the CMR-STAC API\ndate_range = \"2021-05/2021-08\"\n\n\nSpecify the STAC Collections\nNote, a STAC Collection is synonomous with what we usually consider a data product.\n\ncollections = ['HLSL30.v1.5', 'HLSS30.v1.5']\ncollections\n\n['HLSL30.v1.5', 'HLSS30.v1.5']\n\n\n\n\n\nPerform Search Against the CMR-STAC API\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\nPrint out how many STAC Items match our search query\n\nsearch.matched()\n\n77\n\n\nWe now have a search object containing the STAC records that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\nitem_collection = search.get_all_items()\n\nlist(item_collection)[0:5]\n\n[<Item id=HLS.L30.T14TKL.2021124T173013.v1.5>,\n <Item id=HLS.L30.T13TGF.2021124T173013.v1.5>,\n <Item id=HLS.S30.T14TKL.2021125T172901.v1.5>,\n <Item id=HLS.S30.T13TGF.2021125T172901.v1.5>,\n <Item id=HLS.S30.T14TKL.2021128T173901.v1.5>]\n\n\n\n\nGrab the first Item and print it out as a dictionary\n\nitem_collection[0].to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T14TKL.2021124T173013.v1.5',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 35},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-102.5408226, 40.5084628],\n    [-101.5339397, 40.5349543],\n    [-101.2721766, 41.3037739],\n    [-101.2800519, 41.5292355],\n    [-102.5941245, 41.4956464],\n    [-102.5408226, 40.5084628]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5/items/HLS.L30.T14TKL.2021124T173013.v1.5'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2047834723-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2047834723-LPCLOUD.umm_json'}],\n 'assets': {'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.Fmask.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.Fmask.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B10.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B10.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B03.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B03.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B01.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B01.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B09.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B09.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.SZA.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.SZA.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B07.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B07.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B04.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B04.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.VZA.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.VZA.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B02.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B02.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.VAA.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.VAA.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.SAA.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B06.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B06.tif'},\n  'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B11.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B11.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.B05.tif',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.B05.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.015/HLS.L30.T14TKL.2021124T173013.v1.5.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T14TKL.2021124T173013.v1.5.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2047834723-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.594124, 40.508463, -101.272177, 41.529236],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v1.5'}\n\n\n\n\n\nFiltering STAC Items\nWhile the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property like cloud cover for example. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we need to do an Enhanced Vegetation Index (EVI) calculation later.\nNow we will set the max cloud cover allowable and extract the band links for those Items that match or are less than the max cloud cover.\ncloudcover = 25\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v1.5':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v1.5':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n#len(evi_band_links)/4    # Print the number of Items that match our cloud criteria \nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n\nevi_band_links[:10]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021133T172406.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021133T172406.v1.5.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021133T172406.v1.5.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T14TKL.2021133T172406.v1.5.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T14TKL.2021133T173859.v1.5.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T14TKL.2021133T173859.v1.5.B04.tif']\n\n\nNOTICE that in the list of links that we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. These tiles represent neighboring UTM zones. We will split the list of links into seperate lists for each tile.\nWe now have a list of links to data assets that meet our search and filtering criteria. The commands that follow will split this list into logical groupings using python routines.\n\n\nSplit Data Links List into Logical Groupings\nSplit by Universal Transverse Mercator (UTM) tile specified in the file name (e.g., T14TKL & T13TGF)\ntile_dicts = defaultdict(list)    # https://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nfor l in evi_band_links:\n    tile = l.split('.')[-6]\n    tile_dicts[tile].append(l)\n\nPrint dictionary keys and values, i.e. the data links\n\ntile_dicts.keys()\n\ndict_keys(['T14TKL', 'T13TGF'])\n\n\n\ntile_dicts['T13TGF'][:5]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B02.tif']\n\n\nNow we will create a seperate list of data links for each tile\ntile_links_T14TKL = tile_dicts['T14TKL']\ntile_links_T13TGF = tile_dicts['T13TGF']\n\n\nPrint band/layer links for HLS tile T13TGF\n\ntile_links_T13TGF[:10]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B8A.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif']\n\n\n\n\nSplit the links by band\nbands_dicts = defaultdict(list)\nfor b in tile_links_T13TGF:\n    band = b.split('.')[-2]\n    bands_dicts[band].append(b)\n\nbands_dicts.keys()\n\ndict_keys(['Fmask', 'B02', 'B05', 'B04', 'B8A'])\n\n\n\nbands_dicts['B04']\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\n\n\nSave links to a text file\nTo finish off this exercise, we will save the idividual link lists as seperate text files with descriptive names.\n\nWrite links from CMR-STAC API to a file\nfor k, v in bands_dicts.items():\n    name = (f'HTTPS_T12TGF_{k}_Links.txt')\n    with open(f'./data/{name}', 'w') as f:\n        for l in v:\n            f.write(f\"{l}\" + '\\n')\n\n\nWrite links to file for S3 access\nfor k, v in bands_dicts.items():\n    name = (f'S3_T12TGF_{k}_Links.txt')\n    with open(f'./data/{name}', 'w') as f:\n        for l in v:\n            s3l = l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n            f.write(f\"{s3l}\" + '\\n')"
  },
  {
    "objectID": "tutorials/Data_Discovery__CMR-STAC_API.html#resources",
    "href": "tutorials/Data_Discovery__CMR-STAC_API.html#resources",
    "title": "",
    "section": "Resources",
    "text": "https://github.com/nasa/cmr-stac\nhttps://stacspec.org/\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nhttps://pystac-client.readthedocs.io/en/latest/index.html\nhttps://pystac.readthedocs.io/en/1.0/"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access_gdalvrt.html#summary",
    "href": "tutorials/Data_Access__Direct_S3_Access_gdalvrt.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Hello World"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access_gdalvrt.html#exercise",
    "href": "tutorials/Data_Access__Direct_S3_Access_gdalvrt.html#exercise",
    "title": "",
    "section": "Exercise",
    "text": "Import Required Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport subprocess\nimport requests\nimport boto3\nfrom pystac_client import Client\nfrom collections import defaultdict\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geopandas\nimport pyproj\nfrom pyproj import Proj\nfrom shapely.ops import transform\nimport geoviews as gv\nfrom cartopy import crs\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\nGet Temporary Credentials and Configure Local Environment\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME.\ndef get_temp_creds():\n    temp_creds_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n    return requests.get(temp_creds_url).json()\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\nInsert the credentials into our boto3 session and configure out rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session can then be used to pass those credentials and get S3 objects from applicable buckets.\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f0295010b10>\n\n\n\n\n\nRead In and Process STAC Asset Links\nIn the previous section, we used the NASA CMR-STAC API to discover HLS assets the intersect with our search criteria, i.e., ROI, Date range, and collections. The search results were filtered and saved as text files by individual bands for each tile. We will read in the text files for tile T13TGF for the RED (L30: B04 & S30: B04), NIR (L30: B05 & S30: B8A), and Fmask bands.\n\nList text files with HLS links\n\n[t for t in os.listdir('./data') if '.txt' in t]\n\n['S3_T12TGF_B04_Links.txt',\n 'S3_T12TGF_NIR_VSI_Links.txt',\n 'S3_T12TGF_Fmask_Links.txt',\n 'HTTPS_T12TGF_B8A_Links.txt',\n 'S3_T12TGF_RED_VSI_Links.txt',\n 'S3_T12TGF_B02_Links.txt',\n 'HTTPS_T12TGF_B05_Links.txt',\n 'S3_T12TGF_B05_Links.txt',\n 'HTTPS_T12TGF_Fmask_Links.txt',\n 'S3_T12TGF_B8A_Links.txt',\n 'S3_T12TGF_FMASK_VSI_Links.txt',\n 'HTTPS_T12TGF_B04_Links.txt',\n 'HTTPS_T12TGF_B02_Links.txt']\n\n\n\n\nRead in our asset links for BO4 (RED)\n\nred_s3_links = open('./data/S3_T12TGF_B04_Links.txt').read().splitlines()\nred_s3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\n\nRead in and combine our asset links for BO5 (Landsat NIR) and B8A (Sentinel-2 NIR)\nThe near-infrared (NIR) band for Landsat is B05 while the NIR band for Sentinel-2 is B8A. In the next step we will read in and combine the lists into a single NIR list.\n\nnir_bands = ['B05', 'B8A']\nnir_link_text = [x for x in os.listdir('./data') if any(b in x for b in nir_bands) and 'S3' in x]\nnir_s3_links = []\nfor file in nir_link_text:\n    nir_s3_links.extend(open(f'./data/{file}').read().splitlines())\nnir_s3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B8A.tif']\n\n\n\n\nRead in our asset links for Fmask\nfmask_s3_links = open('./data/S3_T12TGF_Fmask_Links.txt').read().splitlines()\n#fmask_s3_links\nIn this example we will use the gdalbuildvrt.exe utility to create a time series virtual raster format (VRT) file. The utility, however, expects the links to be formated with the GDAL virtual file system (VSI) path, rather than the actual asset links. We will therefore use the VSI path to access our assets. The examples below show the VSI path substitution for S3 (vsis3) links.\n/vsis3/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2020191T172901.v1.5.B04.tif\nSee the GDAL Virtual File Systems for more information regarding GDAL VSI.\n\n\nWrite out a new text file containing the vsis3 path\nwith open('./data/S3_T12TGF_RED_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in red_s3_links]\n    for link in links_vsi:\n        f.write(link)\nwith open('./data/S3_T12TGF_NIR_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in nir_s3_links]\n    for link in links_vsi:\n        f.write(link)\nwith open('./data/S3_T12TGF_FMASK_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in fmask_s3_links]\n    for link in links_vsi:\n        f.write(link)\n\n\n\nRead in geoJSON for subsetting\nWe will use the input geoJSON file to clip the source data to our desired region of interest.\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]  \nTo clip the source data to our input feature boundary, we need to transform the feature boundary from its original WGS84 coordinate reference system to the projected reference system of the source HLS file (i.e., UTM Zone 13).\n\nfoa_url = red_s3_links[0]\nwith rio.open(foa_url) as src:\n    hls_proj = src.crs.to_string()\n\nhls_proj    \n\n'PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n\n\n\nTransform geoJSON feature from WGS84 to UTM\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\nproject = pyproj.Transformer.from_proj(geo_CRS, hls_proj)                    # Set up the transformation\nfsUTM = transform(project.transform, fieldShape)\n\n\n\nDirect S3 Data Access\n\nStart up a dask client\n#from dask.distributed import Client\n#client = Client(n_workers=2)\n#client\nThere are multiple way to read COG data in as a time series. The subprocess package is used in this example to run GDAL’s build virtual raster file (gdalbuildvrt) executable outside our python session. First we’ll need to construct a string object with the command and it’s parameter parameters (including our temporary credentials). Then, we run the command using the subprocess.call() function.\n\n\nBuild GDAL VRT Files\n\nConstruct the GDAL VRT call\nbuild_red_vrt = f\"gdalbuildvrt ./data/red_stack.vrt -separate -input_file_list ./data/S3_T12TGF_RED_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\n#build_red_vrt    # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\nWe now have a fully configured gdalbuildvrt string that we can pass to Python’s subprocess module to run the gdalbuildvrt executable outside our Python environment.\n\n\n\nExecute gdalbuildvrt to construct a VRT on disk from the S3 links\n\n%%time\n\nsubprocess.call(build_red_vrt, shell=True)\n\nCPU times: user 1.92 ms, sys: 4.2 ms, total: 6.11 ms\nWall time: 4.18 s\n\n\n0\n\n\n0 means success! We’ll have some troubleshooting to do you get any other value. In this tutorial, the path for the output VRT file or the input file list are the first things to check.\nWhile we’re here, we’ll build the VRT files for the NIR layers and the Fmask layers.\n\nbuild_nir_vrt = f\"gdalbuildvrt ./data/nir_stack.vrt -separate -input_file_list ./data/S3_T12TGF_NIR_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\nsubprocess.call(build_nir_vrt, shell=True)\n\n0\n\n\n\nbuild_fmask_vrt = f\"gdalbuildvrt ./data/fmask_stack.vrt -separate -input_file_list ./data/S3_T12TGF_FMASK_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\nsubprocess.call(build_fmask_vrt, shell=True)\n\n0\n\n\n\n\n\nReading in an HLS time series\nWe can now read the VRT files into our Python session. A drawback of reading VRTs into Python is that the time coordinate variable needs to be contructed. Below we not only read in the VRT file using rioxarray, but we also repurpose the band variable, which is generated automatically, to hold out time information.\n\nRead the RED VRT in as xarray with Dask backing\n\n%%time\n\nchunks=dict(band=1, x=1024, y=1024)\n#chunks=dict(band=1, x=512, y=512)\nred = rioxarray.open_rasterio('./data/red_stack.vrt', chunks=chunks)                    # Read in VRT\nred = red.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nred['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]   # Extract the time information from the input file names and assign them to the time coordinate variable\nred = red.sortby('time')                                                                # Sort by the time coordinate variable\nred\n\nCPU times: user 39.8 ms, sys: 4.85 ms, total: 44.6 ms\nWall time: 54.2 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<getitem, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  485.45 MiB   2.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  609 Tasks  304 Chunks \n     Type  int16  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\nAbove we use the parameter chunk in the rioxarray.open_rasterio() function to enable the Dask backing. What this allows is lazy reading of the data, which means the data is not actually read in into memory at this point. What we have is an object with some metadata and pointer to the source data. The data will be streamed to us when we call for it, but not stored in memory until with call the Dask compute() or persist() methods.\n\n\nPrint out the time coordinate\n\nred.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 19)>\narray(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0xarray.DataArray'time'time: 192021-05-13T17:24:06 2021-05-13T17:38:59 ... 2021-08-17T17:24:41array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')Coordinates: (2)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (0)\n\n\n\n\nClip out the ROI and persist the result in memory\nUp until now, we haven’t read any of the HLS data into memory. Now we will use the persist() method to load the data into memory.\n\nred_clip = red.rio.clip([fsUTM]).persist()\nred_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 56, x: 56)>\ndask.array<astype, shape=(19, 56, 56), dtype=int16, chunksize=(1, 56, 56), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    _FillValue:    -9999xarray.DataArraytime: 19y: 56x: 56dask.array<chunksize=(1, 56, 56), meta=np.ndarray>\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  116.38 kiB   6.12 kiB \n     Shape  (19, 56, 56)   (1, 56, 56) \n     Count  19 Tasks  19 Chunks \n     Type  int16  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  56\n  56\n  19\n\n\n\nCoordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (3)scale_factor :0.0001add_offset :0.0_FillValue :-9999\n\n\nAbove, we persisted the clipped results to memory using the persist() method. This doesn’t necessarily need to be done, but it will substantially improve the performance of the visualization of the time series below.\n\n\nPlot red_clip with hvplot\n\nred_clip.hvplot.image(x='x', y='y', width=800, height=600, colorbar=True, cmap='Reds').opts(clim=(0.0, red_clip.values.max()))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nRead in the NIR and Fmask VRT files\n\n%%time\nchunks=dict(band=1, x=1024, y=1024)\nnir = rioxarray.open_rasterio('./data/nir_stack.vrt', chunks=chunks)                    # Read in VRT\nnir = nir.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nnir['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]   # Extract the time information from the input file names and assign them to the time coordinate variable\nnir = nir.sortby('time')                                                                # Sort by the time coordinate variable\nnir\n\nCPU times: user 37.5 ms, sys: 160 µs, total: 37.6 ms\nWall time: 45.4 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<getitem, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  485.45 MiB   2.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  609 Tasks  304 Chunks \n     Type  int16  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n%%time\nchunks=dict(band=1, x=1024, y=1024)\nfmask = rioxarray.open_rasterio('./data/fmask_stack.vrt', chunks=chunks)                    # Read in VRT\nfmask = fmask.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nfmask['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]     # Extract the time information from the input file names and assign them to the time coordinate variable\nfmask = fmask.sortby('time')                                                                # Sort by the time coordinate variable\nfmask\n\nCPU times: user 36 ms, sys: 0 ns, total: 36 ms\nWall time: 45 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 19, y: 3660, x: 3660)>\ndask.array<getitem, shape=(19, 3660, 3660), dtype=uint8, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    255.0\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  242.73 MiB   1.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  609 Tasks  304 Chunks \n     Type  uint8  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :255.0scale_factor :1.0add_offset :0.0\n\n\n\n\nCreate an xarray dataset\nWe will now combine the RED, NIR, and Fmask arrays into a dataset and create/add a new NDVI variable.\n\nhls_ndvi = xr.Dataset({'red': red, 'nir': nir, 'fmask': fmask, 'ndvi': (nir - red) / (nir + red)})\nhls_ndvi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (time: 19, x: 3660, y: 3660)\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nData variables:\n    red          (time, y, x) int16 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    nir          (time, y, x) int16 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    fmask        (time, y, x) uint8 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    ndvi         (time, y, x) float64 dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>xarray.DatasetDimensions:time: 19x: 3660y: 3660Coordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Data variables: (4)red(time, y, x)int16dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  485.45 MiB   2.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  609 Tasks  304 Chunks \n     Type  int16  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nnir(time, y, x)int16dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  485.45 MiB   2.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  609 Tasks  304 Chunks \n     Type  int16  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nfmask(time, y, x)uint8dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>_FillValue :255.0scale_factor :1.0add_offset :0.0\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  242.73 MiB   1.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  609 Tasks  304 Chunks \n     Type  uint8  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nndvi(time, y, x)float64dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  1.90 GiB   8.00 MiB \n     Shape  (19, 3660, 3660)   (1, 1024, 1024) \n     Count  2130 Tasks  304 Chunks \n     Type  float64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  19\n\n\n\nAttributes: (0)\n\n\nAbove, we created a new NDVI variable. Now, we will clip and plot our results.\n\nndvi_clip = hls_ndvi.ndvi.rio.clip([fsUTM]).persist()\nndvi_clip\n\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: divide by zero encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: invalid value encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'ndvi' (time: 19, y: 56, x: 56)>\ndask.array<getitem, shape=(19, 56, 56), dtype=float64, chunksize=(1, 56, 56), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0xarray.DataArray'ndvi'time: 19y: 56x: 56dask.array<chunksize=(1, 56, 56), meta=np.ndarray>\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  465.50 kiB   24.50 kiB \n     Shape  (19, 56, 56)   (1, 56, 56) \n     Count  19 Tasks  19 Chunks \n     Type  float64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  56\n  56\n  19\n\n\n\nCoordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (0)\n\n\n\nPlot NDVI\n\nndvi_clip.hvplot.image(x='x', y='y', groupby='time', width=800, height=600, colorbar=True, cmap='YlGn').opts(clim=(0.0, 1.0))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nYou may have notices that some images for some of the time step are ‘blurrier’ than other. This is because they are contaminated in some way, be it clouds, cloud shadows, snow, ice.\n\n\n\nApply quality filter\nWe want to keep NDVI data values where Fmask equals 0 (no clouds, no cloud shadow, no snow/ice, no water.\n\nndvi_clip_filter = hls_ndvi.ndvi.where(fmask==0, np.nan).rio.clip([fsUTM]).persist()\n\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: divide by zero encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: invalid value encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n\n\n\nndvi_clip_filter.hvplot.image(x='x', y='y', groupby='time', width=800, height=600, colorbar=True, cmap='YlGn').opts(clim=(0.0, 1.0))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nAggregate by month\nFinally, we will use xarray’s groupby operation to aggregate by month.\n\nndvi_clip_filter.groupby('time.month').mean('time').hvplot.image(x = 'x', y = 'y', crs = hls_proj, groupby='month', cmap='YlGn', width=800, height=600, colorbar=True).opts(clim=(0.0, 1.0))\n\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide\n  x = np.divide(x1, x2, out)\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide\n  x = np.divide(x1, x2, out)\n\n\nUnable to display output for mime type(s): \n\n\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide\n  x = np.divide(x1, x2, out)\n/srv/conda/envs/pangeo/lib/python3.7/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide\n  x = np.divide(x1, x2, out)\n\n\n\n\n\n\n\n\n  \n\n\n\n\nrio_env.__exit__()"
  },
  {
    "objectID": "tutorials/Data_Access__Direct_S3_Access_gdalvrt.html#references",
    "href": "tutorials/Data_Access__Direct_S3_Access_gdalvrt.html#references",
    "title": "",
    "section": "References",
    "text": "https://rasterio.readthedocs.io/en/latest/\nhttps://corteva.github.io/rioxarray/stable/index.html\nhttps://tutorial.dask.org/index.html\nhttps://examples.dask.org/applications/satellite-imagery-geotiff.html"
  },
  {
    "objectID": "clinic/notebooks.html#intro-python",
    "href": "clinic/notebooks.html#intro-python",
    "title": "Notebooks, Python, GitHub",
    "section": "Intro Python",
    "text": "How to import libraries, talk through some code structure"
  },
  {
    "objectID": "clinic/notebooks.html#run-notebook",
    "href": "clinic/notebooks.html#run-notebook",
    "title": "Notebooks, Python, GitHub",
    "section": "Run notebook",
    "text": "see how it works, but that you might want to make a change"
  },
  {
    "objectID": "clinic/notebooks.html#commit-changes",
    "href": "clinic/notebooks.html#commit-changes",
    "title": "Notebooks, Python, GitHub",
    "section": "Commit changes",
    "text": "Make some edits – TODO\nCommit"
  },
  {
    "objectID": "clinic/notebooks.html#push-changes",
    "href": "clinic/notebooks.html#push-changes",
    "title": "Notebooks, Python, GitHub",
    "section": "Push changes",
    "text": "TODO"
  },
  {
    "objectID": "clinic/notebooks.html#commit-and-push-again",
    "href": "clinic/notebooks.html#commit-and-push-again",
    "title": "Notebooks, Python, GitHub",
    "section": "Commit and push again",
    "text": "some more python concepts?\nTODO"
  },
  {
    "objectID": "clinic/notebooks.html#update-your-fork",
    "href": "clinic/notebooks.html#update-your-fork",
    "title": "Notebooks, Python, GitHub",
    "section": "Update your fork",
    "text": "(During the break, Luis/someone added something to the notebook)\nAnd how to update your fork from main. 2 step process:\n\nFrom github.com, click these buttons\nFrom 2i2c, git fetch (I think?)\n\nDiscuss merge conflicts and philosophy: best way to deal with merge conflicts is to avoid them; talk to your colleagues, commit often. Also can work with branches (out of the scope of this Clinic?)"
  },
  {
    "objectID": "clinic/notebooks.html#closing",
    "href": "clinic/notebooks.html#closing",
    "title": "Notebooks, Python, GitHub",
    "section": "Closing",
    "text": "Thanks for coming!"
  },
  {
    "objectID": "clinic/index.html#before-the-clinic",
    "href": "clinic/index.html#before-the-clinic",
    "title": "Pre-Hackathon Clinic",
    "section": "Before the Clinic",
    "text": "Please follow the set up prerequisites before the Clinic."
  },
  {
    "objectID": "clinic/index.html#further-resources",
    "href": "clinic/index.html#further-resources",
    "title": "Pre-Hackathon Clinic",
    "section": "Further resources",
    "text": "The Clinic will introduce concepts that you can learn self-paced in more detail in the following materials:\n\nCarpentries intro Git\netc"
  },
  {
    "objectID": "clinic/jupyterhub.html#welcome",
    "href": "clinic/jupyterhub.html#welcome",
    "title": "JupyterHub, repos, environments",
    "section": "Welcome!",
    "text": "Thanks for being here.\nWho instructors & helpers are, how to ask for help.\nCode of Conduct reminder\nSummary of what we’ll do"
  },
  {
    "objectID": "clinic/jupyterhub.html#log-into-2i2c",
    "href": "clinic/jupyterhub.html#log-into-2i2c",
    "title": "JupyterHub, repos, environments",
    "section": "Log into 2i2c",
    "text": "Go to https://openscapes.2i2c.cloud/hub/. You will be asked to log in with your GitHub Account.\nThis will take several minutes. While we wait, we’ll get set up with GitHub and a brief overview."
  },
  {
    "objectID": "clinic/jupyterhub.html#github-overview",
    "href": "clinic/jupyterhub.html#github-overview",
    "title": "JupyterHub, repos, environments",
    "section": "GitHub overview",
    "text": "TODO Julie - images and narration:\n\nmain unit: a repo\nremote and local\ncommits, push, pull\nfork\n\n\nFork and Clone\nGo to https://github.com/NASA-Openscapes/2021-Cloud-Clinic\nClick Fork. This will make a copy of this repo in your GitHub account."
  },
  {
    "objectID": "clinic/jupyterhub.html#jupyter-hub-overview",
    "href": "clinic/jupyterhub.html#jupyter-hub-overview",
    "title": "JupyterHub, repos, environments",
    "section": "Jupyter Hub Overview",
    "text": "Now an intro to 2i2c, a which is a Jupyter Hub.\nTODO: insert images\nOrient to 2i2c\n\nOpen the terminal\nThe command line, also called the Shell or Terminal, is available in 2i2c.\nOpen a terminal in 2i2c: File > New… > Terminal. Note that you can toggle between “Simple” display mode in the bottom left."
  },
  {
    "objectID": "clinic/jupyterhub.html#clone-repo-to-2i2c",
    "href": "clinic/jupyterhub.html#clone-repo-to-2i2c",
    "title": "JupyterHub, repos, environments",
    "section": "Clone repo to 2i2c",
    "text": "We will clone our repo into 2i2c so that we can run it in the Cloud. We do this from the Terminal.\nMake sure you clone from your forked repo. For example, my url has my username “betolink”, not “NASA-Openscapes”.\ngit clone https://github.com/betolink/2021-Cloud-Clinic"
  },
  {
    "objectID": "clinic/jupyterhub.html#python-environments",
    "href": "clinic/jupyterhub.html#python-environments",
    "title": "JupyterHub, repos, environments",
    "section": "Python environments",
    "text": "TODO overview discussion\nWalk through environment.yml file from https://github.com/NASA-Openscapes/2021-Cloud-Clinic"
  },
  {
    "objectID": "clinic/jupyterhub.html#jupyter-notebook",
    "href": "clinic/jupyterhub.html#jupyter-notebook",
    "title": "JupyterHub, repos, environments",
    "section": "Jupyter notebook",
    "text": "Talk through the example from https://github.com/NASA-Openscapes/2021-Cloud-Clinic\nImport the library you defined in the environment. (If you don’t have the library defined in the environment and try to import it, it will give you an error (unless it’s part of core Python))"
  },
  {
    "objectID": "logistics/schedule.html#pre-hackathon-clinic-november-9-optional",
    "href": "logistics/schedule.html#pre-hackathon-clinic-november-9-optional",
    "title": "Schedule",
    "section": "Pre-Hackathon Clinic: November 9 (Optional)",
    "text": "All times listed below are in PST (UTC-8) unless otherwise noted.\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00-8:05\nWelcome\nTBD\n\n\n8:05-9:00\nJupyterHub, repos, environments\nLuis Lopez\n\n\n9:00-9:05\nBreak\n\n\n\n9:05-10:00\nNotebooks, python, syncing\nMakhan Virdi"
  },
  {
    "objectID": "logistics/schedule.html#cloud-hackathon-november-15-19",
    "href": "logistics/schedule.html#cloud-hackathon-november-15-19",
    "title": "Schedule",
    "section": "Cloud Hackathon: November 15-19",
    "text": "All times listed below are in PST (UTC-8) unless otherwise noted.\n\nDay 1 (November 15)\n\n\n\nTime\nEvent\nLeads/Instructors\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2 (November 16)\n\n\n\nTime\nEvent\nLeads/Instructors\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3 (November 17)\n\n\n\nTime\nEvent\nLeads/Instructors\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4 (November 18)\n\n\n\nTime\nEvent\nLeads/Instructors\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5 (November 19)\n\n\n\nTime\nEvent\nLeads/Instructors\n\n\n\n\nTODO"
  },
  {
    "objectID": "logistics/prerequisites.html",
    "href": "logistics/prerequisites.html",
    "title": "Prerequisites & setup",
    "section": "",
    "text": "Before the Hackathon, please do the following (20 minutes). All software is free. If you are attending the Clinic, please do this in advance of the Clinic.\n\nSend us your GitHub username\n\nPlease send via email, replying to our acceptance email\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nRemember your username, email and password; you will need to be logged in them during the workshop!\n\nCreate an Earthdata Login account (if you do not have one already) at https://urs.earthdata.nasa.gov\nOthers? TODO revisit\nGet comfortable. Consider your computer set-up in advance, including an external monitor if possible. You will be following along in Jupyter Hub on your own computer while also watching an instructor live-code over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links."
  },
  {
    "objectID": "logistics/index.html",
    "href": "logistics/index.html",
    "title": "Logistics",
    "section": "",
    "text": "text here"
  },
  {
    "objectID": "logistics/getting-help.html",
    "href": "logistics/getting-help.html",
    "title": "Getting Help",
    "section": "",
    "text": "text here, including:\n\nSlack Channel\n(Zoom) Chat\nBreaks/Breakout Groups"
  },
  {
    "objectID": "tutorials/data_discovery_with_cmr.html#what-is-cmr",
    "href": "tutorials/data_discovery_with_cmr.html#what-is-cmr",
    "title": "",
    "section": "What is CMR",
    "text": "CMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials/data_discovery_with_cmr.html#what-is-the-cmr-api",
    "href": "tutorials/data_discovery_with_cmr.html#what-is-the-cmr-api",
    "title": "",
    "section": "What is the CMR API",
    "text": "API stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials/data_discovery_with_cmr.html#how-to-search-cmr-from-python",
    "href": "tutorials/data_discovery_with_cmr.html#how-to-search-cmr-from-python",
    "title": "",
    "section": "How to search CMR from Python",
    "text": "The first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in depth tutorial on requests is here\nimport getpass\n\nimport requests\nfrom requests.auth import HTTPBasicAuth\nfrom pprint import pprint\n\nusername = 'apbarret'\npassword = getpass.getpass()\n\n ·············\n\n\nCMR_TOKEN_OPS = \"https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens\"\nTOKEN_DATA = ('<token>'\n              '<username>%s</username>'\n              '<password>%s</password>'\n              '<client_id>CMR Client</client_id>'\n              '<user_ip_address>%s</user_ip_address>'\n              '</token>')\nsession = requests.session()\nmy_ip = session.get('https://ipinfo.io/ip').text.strip()\nauth = HTTPBasicAuth(username, password)\nauth_resp = session.post(CMR_TOKEN_OPS,\n                         auth=auth,\n                         data=TOKEN_DATA % (str(username), str(password), my_ip),\n                         headers={'Content-Type': 'application/xml', 'Accept': 'application/json'},\n                         timeout=10)\nauth_token = auth_resp.json()['token']['id']\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint.\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for colections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the url for the root CMR endpoint.\nWe are going to search collections first, so we add collections to the url. I’m using a python format string here.\nurl = f'{CMR_OPS}/{\"collections\"}'\nIn this first example, I want to retrieve a list of collections that are hosted in the cloud. Each collection has a cloud-hosted parameter that is either True if that collection is in the cloud and False if it is not. I also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\nrequests returns a Response object.\nOften, we want to check that our request was successful. In a notebook or someother interactive environment, we can just type the name of the variable we have saved our requests Response to, in this case response.\n\nresponse\n\n<Response [200]>\n\n\nA cleaner and more understandable method is to check the status_code attribute. Both methods return a HTTP status code. You’ve probably seen a 404 error when you have tried to access a website that doesn’t exist.\n\nresponse.status_code\n\n200\n\n\nTry changing CMR_OPS to https://cmr.earthdata.nasa.gov/searches an run requests.get again. Don’t forget to rerun the cell that assigns url\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the Response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. This information is printed below.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 6246\nConnection: keep-alive\nDate: Tue, 19 Oct 2021 00:05:10 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: a09a605b-c7b7-4348-9da3-7fe24f23f990\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [\"above: atmospheric gas concentrations from airborne flasks, arctic-cap, 2017\",\"ORNL_CLOUD\",\"ABoVE_Atmospheric_Flask_Data_1717\",\"1\",2143812247,1]\nCMR-Hits: 919\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 23\nX-Request-Id: a09a605b-c7b7-4348-9da3-7fe24f23f990\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 802eebfb6e9ab6bda66a7156bc7add93.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: HIO50-C2\nX-Amz-Cf-Id: UTf8fGAnH4f672p_C5xgSkfiXmRm7Ju4O1fYc0QxwDGlBJkUe_gNkw==\n\n\nWe can see that the content returned is in json format in the UTF-8 character set. We can also see from CMR-Hits that 919 collections were found.\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but because it is case-insensitive, both\n\nresponse.headers['CMR-Hits']\n\n'919'\n\n\nand\n\nresponse.headers['cmr-hits']\n\n'919'\n\n\nwork.\nThis is a large number of data sets. I’m going to restrict the search to cloud-hosted datasets from ASF (Alaska SAR Facility) because I’m interested in SAR images of sea ice. You can modify the code below to explore all of the cloud-hosted datasets.\ndata_center = 'ASF'\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'data_center': data_center,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\nresponse.headers['cmr-hits']\n\n'49'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2021-10-19T00:09:01.052Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&data_center=ASF\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:39.000Z\",\"dataset_id\":\"SENTINEL-1A_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470488-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:22.000Z\",\"dataset_id\":\"SENTINEL-1B_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985661-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:56.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470533-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:47.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985645-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:49.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985660-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:58.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214471521-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:20.000Z\",\"dataset_id\":\"SENTINEL-1A_RAW\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_RAW\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_RAW\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A level zero product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470561-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:15.000Z\",\"dataset_id\":\"SENTINEL-1A_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1A_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470496-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:26.000Z\",\"dataset_id\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Single-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470682-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:02.000Z\",\"dataset_id\":\"SENTINEL-1B_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1B_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985617-ASF\",\"has_formats\":false,\"score\":0.5,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]}]}}'\n\n\nIt is more convenient to work with json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nStep through response.json(), then to response.json()['feed']['entry'][0]\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'ASF',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'ASF',\n 'dataset_id': 'SENTINEL-1A_SLC',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C1214470488-ASF',\n 'links': [{'href': 'https://vertex.daac.asf.alaska.edu/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['ASF', 'ESA/CS1CGS'],\n 'original_format': 'ECHO10',\n 'platforms': ['Sentinel-1A'],\n 'score': 0.5,\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'SENTINEL-1A_SLC',\n 'summary': 'Sentinel-1A slant-range product',\n 'time_start': '2014-04-03T00:00:00.000Z',\n 'title': 'SENTINEL-1A_SLC',\n 'updated': '2021-07-15T19:16:39.000Z',\n 'version_id': '1'}\n\n\nThe first response is not the result I am looking for. So I want to print the name of the dataset (dataset_id) and the concept id (id).\ncollections = response.json()['feed']['entry']\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\n\n\nBut there is a problem. We know from CMR-Hits that there are 49 datasets but only 10 are printed. This is because CMR restricts the number of results returned by a query. The default is 10 but it can be set to a maximum of 2000. Knowing that there were 49 ‘hits’, I’ll set page_size to 49.\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'data_center': data_center,\n                            'page_size': 49,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_HIGH_RES C1214470576-ASF\nASF SENTINEL-1A_OCN C1214472977-ASF\nASF SENTINEL-1B_RAW C1327985647-ASF\nASF SENTINEL-1B_OCN C1327985579-ASF\nASF SENTINEL-1A_METADATA_RAW C1214470532-ASF\nASF ALOS_AVNIR_OBS_ORI C1808440897-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_HIGH_RES C1327985741-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_MEDIUM_RES C1214472994-ASF\nASF SENTINEL-1B_METADATA_RAW C1327985650-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_HIGH_RES C1327985571-ASF\nASF SENTINEL-1A_METADATA_OCN C1266376001-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_MEDIUM_RES C1327985740-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES C1214472336-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_HIGH_RES C1214470732-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1214473170-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_MEDIUM_RES C1327985578-ASF\nASF SENTINEL-1B_METADATA_OCN C1327985646-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_HIGH_RES C1327985619-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1327985739-ASF\nASF STS-68_BROWSE_GRD C1661710593-ASF\nASF STS-68_BROWSE_SLC C1661710596-ASF\nASF STS-59_BROWSE_GRD C1661710578-ASF\nASF STS-59_BROWSE_SLC C1661710581-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_FULL_RES C1214471197-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_FULL_RES C1214471960-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_FULL_RES C1214472978-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_FULL_RES C1214473165-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_FULL_RES C1327985697-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_FULL_RES C1327985651-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_FULL_RES C1327985644-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_FULL_RES C1327985674-ASF\nASF STS-59_GRD C1661710583-ASF\nASF STS-59_METADATA_GRD C1661710586-ASF\nASF STS-59_METADATA_SLC C1661710588-ASF\nASF STS-59_SLC C1661710590-ASF\nASF STS-68_GRD C1661710597-ASF\nASF STS-68_METADATA_GRD C1661710600-ASF\nASF STS-68_METADATA_SLC C1661710603-ASF\nASF STS-68_SLC C1661710604-ASF"
  },
  {
    "objectID": "tutorials/data_discovery_with_cmr.html#granule-search",
    "href": "tutorials/data_discovery_with_cmr.html#granule-search",
    "title": "",
    "section": "Granule Search",
    "text": "In NASA speak, Granules are files. In this example, we will search for recent Sentinel-1 Ground Range Detected (GRD) Medium Resolution Synthetic Aperture Radar images over the east coast of Greenland. The data in these files are most useful for sea ice mapping.\nI’ll use the data range 2021-10-17 00:00 to 2021-10-18 23:59:59.\nI’ll use a simple bounding box to search. - SW: 76.08166,-67.1746 - NW: 88.19689,21.04862\nFrom the collections search, I know the concept ids for Sentinel-1A and Sentinel-1B GRD medium resolution are - C1214472336-ASF - C1327985578-ASF\nWe need to change the resource url to look for granules instead of collections\nurl = f'{CMR_OPS}/{\"granules\"}'\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1214472336-ASF',\n                            'temporal': '2020-10-17T00:00:00Z,2020-10-18T23:59:59Z',\n                            'bounding_box': '76.08166,-67.1746,88.19689,21.04862',\n                            'page_size': 200,\n                            'token': auth_token,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n6\n\n\ngranules = response.json()['feed']['entry']\n#for granule in granules:\n#    print(f'{granule[\"archive_center\"]} {granule[\"dataset_id\"]} {granule[\"id\"]}')\n\npprint(granules)\n\n[{'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954601581-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34836'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.163563 87.942726 -60.893669 89.293564 -59.279579 '\n                '96.119583 -57.619923 94.49958 -59.163563 87.942726']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E',\n  'time_end': '2020-10-17T13:20:39.000Z',\n  'time_start': '2020-10-17T13:20:09.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E-METADATA_GRD_MD',\n  'updated': '2020-10-19T17:13:39.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616816-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-66.104271 69.819366 -69.571243 74.741966 -67.42112 83.209152 '\n                '-64.210938 77.59269 -66.104271 69.819366']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B',\n  'time_end': '2020-10-17T14:57:20.000Z',\n  'time_start': '2020-10-17T14:56:16.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:35:34.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616638-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-62.765087 66.277176 -66.103951 69.81897 -64.211227 77.590164 '\n                '-61.060097 73.427185 -62.765087 66.277176']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE',\n  'time_end': '2020-10-17T14:58:20.000Z',\n  'time_start': '2020-10-17T14:57:20.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:33:31.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954805829-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-65.236397 83.18071 -68.712318 87.887711 -66.630493 96.161102 '\n                '-63.400326 90.779785 -65.236397 83.18071']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659',\n  'time_end': '2020-10-18T14:00:00.000Z',\n  'time_start': '2020-10-18T13:58:56.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659-METADATA_GRD_MD',\n  'updated': '2020-10-20T07:16:54.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954799806-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-61.876564 79.848251 -65.236069 83.180344 -63.400402 90.778 '\n                '-60.217258 86.840034 -61.876564 79.848251']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777',\n  'time_end': '2020-10-18T14:01:00.000Z',\n  'time_start': '2020-10-18T14:00:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:51:16.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954798927-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.113174 77.623962 -61.876228 79.847961 -60.217464 '\n                '86.837784 -57.570774 84.174744 -59.113174 77.623962']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D',\n  'time_end': '2020-10-18T14:01:48.000Z',\n  'time_start': '2020-10-18T14:01:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:46:01.000Z'}]"
  },
  {
    "objectID": "clinic/index.html#agenda",
    "href": "clinic/index.html#agenda",
    "title": "Pre-Hackathon Clinic",
    "section": "Agenda",
    "text": "The Clinic will occur in 2 halves, with a 5 minute break in-between:\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00-8:05\nWelcome\nTBD\n\n\n8:05-9:00\nJupyterHub, repos, environments\nLuis Lopez\n\n\n9:00-9:05\nBreak\n\n\n\n9:05-10:00\nNotebooks, python, syncing\nMakhan Virdi"
  }
]